{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "302f0d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import transformers\n",
    "import numpy as np\n",
    "import random\n",
    "import ast\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc93d7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "# This must be done before importing transformers\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "transformers_path = Path(\"/home/xiaoguzai/.local/lib/python3.9/site-packages/transformers\")\n",
    "\n",
    "input_dir = Path(\"/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/代码\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "#print('convert_file = ')\n",
    "#print(convert_file)\n",
    "#print('conversion_path = ')\n",
    "#print(conversion_path)\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path \n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "    filepath = deberta_v2_path/filename\n",
    "    \n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)\n",
    "    \n",
    "from transformers import AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "import transformers\n",
    "tokenizer = DebertaV2TokenizerFast.from_pretrained('/home/xiaoguzai/模型/deberta-v3-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4adc33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 360\n",
    "#下面prepare_input的时候将text和feature_text合在一起很巧妙\n",
    "\n",
    "def prepare_input(ids, text, feature_text):\n",
    "    #inputs1 = tokenizer.encode_plus(text,\\\n",
    "    #                               add_special_tokens=True,\\\n",
    "    #                               max_length = max_len,\\\n",
    "    #                               paddin)\n",
    "    inputs = {}\n",
    "    inputs1 = tokenizer.encode_plus(text,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   max_length = max_len,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs2 = tokenizer.encode_plus(feature_text,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   max_length = max_len,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    \n",
    "    if len(inputs1['input_ids'])+len(inputs2['input_ids'])-2 > max_len:\n",
    "        exceed_length = len(inputs1['input_ids'])+len(inputs2['input_ids'])-max_len-2\n",
    "        inputs['input_ids'] = inputs1['input_ids'][:len(inputs1['input_ids'])-exceed_length-1]+inputs2['input_ids'][1:]\n",
    "        inputs['attention_mask'] = [1]*max_len\n",
    "    else:\n",
    "        inputs['input_ids'] = inputs1['input_ids'][:-1]+inputs2['input_ids'][1:-1]\n",
    "        inputs['input_ids'] = inputs['input_ids']+[tokenizer.sep_token_id]\n",
    "        #inputs['input_ids'] = inputs['input_ids']+[tokenizer.sep_token_id]+[tokenizer.pad_token_id]*(max_len-len(inputs['input_ids'])-1)\n",
    "        inputs['attention_mask'] = [1]*(len(inputs['input_ids']))\n",
    "        inputs['attention_mask'] = inputs['attention_mask']+[0]*(max_len-len(inputs['input_ids']))\n",
    "        inputs['input_ids'] = inputs['input_ids']+[tokenizer.pad_token_id]*(max_len-len(inputs['input_ids']))\n",
    "        inputs['token_type_ids'] = [0]*max_len\n",
    "        #inputs['attention_mask'] = [1]*max_len\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v,dtype=torch.long)\n",
    "    return inputs,len(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bde0aadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00000_000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>17-year-old male, has come to the student heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00000_001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>17-year-old male, has come to the student heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00000_002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>17-year-old male, has come to the student heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00000_003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>17-year-old male, has come to the student heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00000_004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>17-year-old male, has come to the student heal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num  \\\n",
       "0  00000_000         0       0            0   \n",
       "1  00000_001         0       0            1   \n",
       "2  00000_002         0       0            2   \n",
       "3  00000_003         0       0            3   \n",
       "4  00000_004         0       0            4   \n",
       "\n",
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history  \n",
       "0  17-year-old male, has come to the student heal...  \n",
       "1  17-year-old male, has come to the student heal...  \n",
       "2  17-year-old male, has come to the student heal...  \n",
       "3  17-year-old male, has come to the student heal...  \n",
       "4  17-year-old male, has come to the student heal...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo_labeling = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/pseudo_labeling.csv')\n",
    "features = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/features.csv')\n",
    "patient_notes = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/patient_notes.csv')\n",
    "features.loc[27,'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "pseudo_labeling = pseudo_labeling.merge(features,on=['feature_num','case_num'],how='left')\n",
    "pseudo_labeling = pseudo_labeling.merge(patient_notes,on=['pn_num','case_num'],how='left')\n",
    "pseudo_labeling = pseudo_labeling.fillna('')\n",
    "pseudo_labeling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a602668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_labeling[\"pn_history\"] = pseudo_labeling[\"pn_history\"].apply(lambda x:x.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06fac1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1196b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_location_to_offset(text,location_list):\n",
    "    results = np.zeros(len(text))\n",
    "    #以char为级别计算，应该对整个text计算len\n",
    "    for idx, offset_mapping in enumerate(location_list):\n",
    "        try:\n",
    "            start = (int)(offset_mapping[0])\n",
    "            end = (int)(offset_mapping[1])\n",
    "            results[start:end] = 1\n",
    "        except:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def create_label(text):\n",
    "    encoded = tokenizer.encode_plus(text,\\\n",
    "                                add_special_tokens=True,\\\n",
    "                                max_length = max_len,\\\n",
    "                                padding = \"max_length\",\\\n",
    "                                return_offsets_mapping = True)\n",
    "    offset_mapping = encoded['offset_mapping']\n",
    "    return offset_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9347470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/5715 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|█████████████████████████████████████| 5715/5715 [00:03<00:00, 1512.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "test_ids = []\n",
    "test_text,test_input_ids,test_token_type_ids = [],[],[]\n",
    "test_attention_mask,test_offset,test_label = [],[],[]\n",
    "for  index,data  in  tqdm(pseudo_labeling.iterrows(),total=len(pseudo_labeling)):\n",
    "    #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "    ids = data['id']\n",
    "    text = data['pn_history']\n",
    "    feature_text = data['feature_text']\n",
    "    inputs,length = prepare_input(ids,text,feature_text)\n",
    "    current_offset = create_label(text)\n",
    "    test_ids.append(ids)\n",
    "    test_text.append(text)\n",
    "    test_input_ids.append(inputs['input_ids'].tolist())\n",
    "    test_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "    test_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "    test_offset.append(current_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8836b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,text,input_ids,offset,token_type_ids,attention_mask):\n",
    "        self.input_ids = input_ids\n",
    "        self.tensors = [text,\\\n",
    "                        torch.tensor(input_ids,dtype=torch.long),\n",
    "                        torch.tensor(offset),\\\n",
    "                        torch.tensor(token_type_ids,dtype=torch.long),\\\n",
    "                        torch.tensor(attention_mask,dtype=torch.long)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1abb1bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(1024,1)\n",
    "        \n",
    "    def forward(self,input_ids,token_type_ids,attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids,\\\n",
    "                           token_type_ids=token_type_ids,\\\n",
    "                           attention_mask=attention_mask)\n",
    "        outputs = outputs[0]\n",
    "        #outputs = self.dropout(outputs)\n",
    "        outputs = self.fc1(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c873d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_probs(total_text,offsets,predictions):\n",
    "    results = [np.zeros(len(t)) for t in total_text]\n",
    "    #!!!results 长短不一!!!\n",
    "    #以char为级别计算，应该对整个text计算len\n",
    "    torch.set_printoptions(threshold=np.inf)\n",
    "    for i, (offset, prediction) in enumerate(zip(offsets, predictions)):\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(offset, prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            #results[i][start:end] = ((float)(pred[0].item(),)\n",
    "            results[i][start:end] = pred[0].item()\n",
    "    return results\n",
    "\n",
    "def get_results(test_text,char_probs, th=0.5):\n",
    "    results = []\n",
    "    #for char_prob in char_probs:\n",
    "    for index in range(len(char_probs)):\n",
    "        char_prob = char_probs[index]\n",
    "        char_text = test_text[index]\n",
    "        #print('char_prob = ')\n",
    "        #print(char_prob)\n",
    "        #print('------------')\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        #result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = [[min(r),max(r)] for r in result]\n",
    "        \n",
    "        for index1 in range(len(result)):\n",
    "            if result[index1][0]-1 >= 0 and char_text[result[index1][0]-1] != ' ':\n",
    "                result[index1][0] = result[index1][0]-1\n",
    "                #preds[index][index1][0] = preds[index][index1][0]-1\n",
    "            #if preds[index][index1][1]+1 < len(current_text) and current_text[preds[index][index1][1]+1] != ' ':\n",
    "            #    preds[index][index1][1] = preds[index][index1][1]+1\n",
    "        \n",
    "        result = [str(r[0])+' '+str(r[1]) for r in result]\n",
    "        #result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1a7016f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaV2Model,DebertaModel\n",
    "import itertools\n",
    "deberta = DebertaV2Model.from_pretrained(\"/home/xiaoguzai/模型/deberta-v3-large\")\n",
    "#deberta = DebertaModel.from_pretrained(\"../input/deberta/base\")\n",
    "model = ClassificationModel(deberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46ed433c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                  | 0/1429 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                          | 1/1429 [00:00<05:29,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1429/1429 [02:18<00:00, 10.31it/s]\n",
      "  0%|                                                  | 0/1429 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                          | 1/1429 [00:00<04:38,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1429/1429 [02:19<00:00, 10.22it/s]\n",
      "  0%|                                          | 1/1429 [00:00<04:36,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1429/1429 [02:14<00:00, 11.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1429/1429 [02:14<00:00, 10.62it/s]\n",
      "  0%|                                          | 1/1429 [00:00<04:27,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1429/1429 [02:19<00:00, 10.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1429/1429 [02:19<00:00, 10.21it/s]\n",
      "  0%|                                                  | 0/1429 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                          | 1/1429 [00:00<04:38,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1429/1429 [02:19<00:00, 10.22it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TestDataset(test_text,\\\n",
    "                           test_input_ids,\\\n",
    "                           test_offset,\\\n",
    "                           test_token_type_ids,\\\n",
    "                           test_attention_mask)\n",
    "test_loader = DataLoader(test_dataset,batch_size=4,shuffle=False,\\\n",
    "                         num_workers=2,pin_memory=True)\n",
    "\n",
    "final_result = []\n",
    "r\"\"\"\n",
    "model_list = ['/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/deberta-v3模型文件/deberta_capatalize_noid_best_point=0.887656_fold=0.pth',\\\n",
    "              '/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/deberta-v3模型文件/deberta_capatalize_noid_best_point=0.883649_fold=1.pth',\\\n",
    "              '/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/deberta-v3模型文件/deberta_capatalize_noid_best_point=0.88135_fold=2.pth',\\\n",
    "              '/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/deberta-v3模型文件/deberta_capatalize_noid_best_point=0.881055_fold=3.pth',\\\n",
    "              '/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/deberta-v3模型文件/deberta_capatalize_noid_best_point=0.88443_fold=4.pth']\n",
    "              #'../input/nbmedebertamodelpart2/deberta_nocapitalize_groupkfold_best_point0.8784704033525406_fold3.pth']\n",
    "\"\"\"\n",
    "r\"\"\"\n",
    "model_list = ['/media/xiaoguzai/WD_BLACK/deberta-large-submit/archive(5)/deberta_Groupsplit_best_point_fold0.pth',\\\n",
    "              '/media/xiaoguzai/WD_BLACK/deberta-large-submit/archive(5)/deberta_Groupsplit_best_point_fold1.pth',\\\n",
    "              '/media/xiaoguzai/WD_BLACK/deberta-large-submit/archive(5)/deberta_Groupsplit_best_point_fold2.pth',\\\n",
    "              '/media/xiaoguzai/WD_BLACK/deberta-large-submit/archive(5)/deberta_Groupsplit_best_point_fold3.pth',\\\n",
    "              '/media/xiaoguzai/WD_BLACK/deberta-large-submit/archive(5)/deberta_Groupsplit_best_point_fold4.pth']\n",
    "\"\"\"\n",
    "model_list = ['/media/xiaoguzai/WD_BLACK/deberta-v3-large模型文件/deberta_capatalize_noid_best_point0.887656_fold0.pth',\\\n",
    "              '/media/xiaoguzai/WD_BLACK/deberta-v3-large模型文件/deberta_capatalize_noid_best_point0.883649_fold1.pth',\\\n",
    "              '/media/xiaoguzai/WD_BLACK/deberta-v3-large模型文件/deberta_capatalize_noid_best_point0.88135_fold2.pth',\\\n",
    "              '/media/xiaoguzai/WD_BLACK/deberta-v3-large模型文件/deberta_capatalize_noid_best_point0.881055_fold3.pth',\\\n",
    "              '/media/xiaoguzai/WD_BLACK/deberta-v3-large模型文件/deberta_capatalize_noid_best_point0.88443_fold4.pth']\n",
    "total_split = len(model_list)\n",
    "for current_split in range(total_split):\n",
    "    model = torch.load(model_list[current_split])\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    final_result = []\n",
    "    current_result = []\n",
    "    for batch_text,batch_input_ids,batch_offset,batch_token_type_ids,batch_attention_mask in tqdm(test_loader):\n",
    "        batch_input_ids = batch_input_ids.to(device)\n",
    "        batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "        batch_attention_mask = batch_attention_mask.to(device)\n",
    "        with torch.no_grad():\n",
    "            logit = model(input_ids=batch_input_ids,token_type_ids=batch_token_type_ids,\\\n",
    "                          attention_mask=batch_attention_mask)\n",
    "            logit = torch.sigmoid(logit)\n",
    "            preds = get_char_probs(batch_text,batch_offset,\\\n",
    "                                   logit.cpu())\n",
    "            for data in preds:\n",
    "                current_result.append(data)\n",
    "    results = get_results(test_text,current_result)\n",
    "    pseudo_labeling['location'] = results\n",
    "    pseudo_labeling[['id','case_num','pn_num','feature_num','location']].to_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/pseudo_labeling_deberta-v3_split='+str(current_split)+'_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb6005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
