{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ae4e2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|███████████████████████████████████| 42146/42146 [00:15<00:00, 2725.51it/s]\n",
      "100%|██████████████████████████████████████| 143/143 [00:00<00:00, 22142.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len = 360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/11440 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|████████████████████████████████████| 11440/11440 [00:12<00:00, 949.82it/s]\n",
      "100%|██████████████████████████████████████| 2860/2860 [00:03<00:00, 928.57it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/deberta-v2-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 1e-06\n",
      "第0个epoch的学习率：0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [22:04<00:00,  2.16it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:56<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "# This must be done before importing transformers\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "transformers_path = Path(\"/home/xiaoguzai/.local/lib/python3.9/site-packages/transformers\")\n",
    "\n",
    "input_dir = Path(\"/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/代码\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "#print('convert_file = ')\n",
    "#print(convert_file)\n",
    "#print('conversion_path = ')\n",
    "#print(conversion_path)\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path \n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "    filepath = deberta_v2_path/filename\n",
    "    \n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)\n",
    "    \n",
    "from transformers import AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "import transformers\n",
    "tokenizer = DebertaV2TokenizerFast.from_pretrained('/home/xiaoguzai/模型/deberta-v2-large')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "train = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/train.csv')\n",
    "\n",
    "train['annotation'] = train['annotation'].apply(ast.literal_eval)\n",
    "train['location'] = train['location'].apply(ast.literal_eval)\n",
    "features = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/features.csv')\n",
    "features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "\n",
    "patient_notes = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/patient_notes.csv')\n",
    "train = train.merge(features, on=['feature_num','case_num'],how='left')\n",
    "train.head()\n",
    "\n",
    "train = train.merge(patient_notes, on=['pn_num','case_num'],how='left')\n",
    "train.head()\n",
    "#这里数据读取有bug，一个history被读取了好多次\n",
    "train[\"pn_history\"] = train[\"pn_history\"].apply(lambda x:x.capitalize())\n",
    "\n",
    "\n",
    "import ast\n",
    "# incorrect annotation\n",
    "train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=42)\n",
    "\n",
    "total_split = 5\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "Fold = GroupKFold(n_splits=total_split)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index,val_index) in enumerate(Fold.split(train,train['location'],groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "#按照groups也就是train['pn_num']进行划分\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "train['annotation_length'] = train['annotation'].apply(len)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "pn_history_lengths = []\n",
    "for text in tqdm(patient_notes['pn_history'].fillna(\"\").values,total=len(patient_notes)):\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    pn_history_lengths.append(length)\n",
    "    \n",
    "features_lengths = []\n",
    "for text in tqdm(features['feature_text'].fillna(\"\").values, total=len(features)):\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    features_lengths.append(length)\n",
    "max_len = max(pn_history_lengths)+max(features_lengths)+3\n",
    "max_len = 360\n",
    "print('max_len = %d'%max_len)\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,inputs,labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.tensors = [torch.tensor(self.inputs),\\\n",
    "                       torch.tensor(self.labels)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "\n",
    "#下面prepare_input的时候将text和feature_text合在一起很巧妙\n",
    "def prepare_input(ids, text, feature_text):\n",
    "    #inputs1 = tokenizer.encode_plus(text,\\\n",
    "    #                               add_special_tokens=True,\\\n",
    "    #                               max_length = max_len,\\\n",
    "    #                               paddin)\n",
    "    inputs = {}\n",
    "    inputs1 = tokenizer.encode_plus(text,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   max_length = max_len,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs2 = tokenizer.encode_plus(feature_text,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   max_length = max_len,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    r\"\"\"\n",
    "    ids1,ids2 = ids.split('_')\n",
    "    inputs3 = tokenizer.encode_plus(ids1,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs4 = tokenizer.encode_plus(ids2,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs2['input_ids'] = inputs2['input_ids']+inputs3['input_ids'][1:]+inputs4['input_ids'][1:]\n",
    "    inputs2['attention_mask'] = inputs2['attention_mask']+inputs3['attention_mask'][1:]+inputs4['attention_mask'][1:]\n",
    "    inputs2['token_type_ids'] = inputs2['token_type_ids']+inputs3['token_type_ids'][1:]+inputs4['token_type_ids'][1:]\n",
    "    \"\"\"\n",
    "    if len(inputs1['input_ids'])+len(inputs2['input_ids'])-2 > max_len:\n",
    "        exceed_length = len(inputs1['input_ids'])+len(inputs2['input_ids'])-max_len-2\n",
    "        inputs['input_ids'] = inputs1['input_ids'][:len(inputs1['input_ids'])-exceed_length-1]+inputs2['input_ids'][1:]\n",
    "        inputs['attention_mask'] = [1]*max_len\n",
    "    else:\n",
    "        inputs['input_ids'] = inputs1['input_ids'][:-1]+inputs2['input_ids'][1:-1]\n",
    "        inputs['input_ids'] = inputs['input_ids']+[tokenizer.sep_token_id]\n",
    "        #inputs['input_ids'] = inputs['input_ids']+[tokenizer.sep_token_id]+[tokenizer.pad_token_id]*(max_len-len(inputs['input_ids'])-1)\n",
    "        inputs['attention_mask'] = [1]*(len(inputs['input_ids']))\n",
    "        inputs['attention_mask'] = inputs['attention_mask']+[0]*(max_len-len(inputs['input_ids']))\n",
    "        inputs['input_ids'] = inputs['input_ids']+[tokenizer.pad_token_id]*(max_len-len(inputs['input_ids']))\n",
    "        inputs['token_type_ids'] = [0]*max_len\n",
    "        #inputs['attention_mask'] = [1]*max_len\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v,dtype=torch.long)\n",
    "    return inputs,len(inputs['input_ids'])\n",
    "\n",
    "#打标记的时候还是只放入text的内容，不考虑feature_text的文本内容\n",
    "def create_label(text, annotation_length, location_list):\n",
    "    encoded = tokenizer.encode_plus(text,\\\n",
    "                                add_special_tokens=True,\\\n",
    "                                max_length = max_len,\\\n",
    "                                padding = \"max_length\",\\\n",
    "                                return_offsets_mapping = True)\n",
    "    offset_mapping = encoded['offset_mapping']\n",
    "    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "    label = np.zeros(len(offset_mapping))\n",
    "    label[ignore_idxes] = -1\n",
    "    if annotation_length != 0:\n",
    "        for location in location_list:\n",
    "            #location = 2 4,location = 8 10\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                #loc = ['2','4'],loc = ['8','10']\n",
    "                start_idx = -1\n",
    "                end_idx = -1\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                #start = 2,end = 4;start = 8,end = 10;\n",
    "                #!!!这里的start,end标记的为字符:Character spans indicating \n",
    "                #the location of each annotation within the note.\n",
    "                #注意前面的标注Character spans\n",
    "                for idx in range(len(offset_mapping)):\n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                        start_idx = idx - 1\n",
    "                        #start_idx = idx\n",
    "                        #print('111start_idx = %d 111'%start_idx)\n",
    "                        #字符比当前字符小的时候，指向前一位\n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                        end_idx = idx + 1\n",
    "                        #字符比当前字符大的时候，指向后一位\n",
    "                        #print('222start_idx = %d 222'%end_idx)\n",
    "                if start_idx == -1:\n",
    "                    start_idx = end_idx\n",
    "                    #print('333start_idx = %d 333'%start_idx)\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    #print('***start_idx = %d***'%start_idx)\n",
    "                    #print('***end_idx = %d***'%end_idx)\n",
    "                    label[start_idx:end_idx] = 1\n",
    "    return offset_mapping,label\n",
    "\n",
    "\n",
    "\n",
    "def change_location_to_offset(text,location_list):\n",
    "    results = np.zeros(len(text))\n",
    "    #以char为级别计算，应该对整个text计算len\n",
    "    for idx, offset_mapping in enumerate(location_list):\n",
    "        try:\n",
    "            start = (int)(offset_mapping[0])\n",
    "            end = (int)(offset_mapping[1])\n",
    "            results[start:end] = 1\n",
    "        except:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        #self.fc1 = nn.Linear(768,1)\n",
    "        #self.attention = AttentionLayer()\n",
    "        #self.fc1 = nn.Linear(1024,1)\n",
    "        self.fc1 = nn.Linear(1536,1)\n",
    "        \n",
    "    def forward(self,input_ids,token_type_ids,attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids,\\\n",
    "                           token_type_ids=token_type_ids,\\\n",
    "                           attention_mask=attention_mask)\n",
    "        outputs = outputs[0]\n",
    "        #outputs = self.dropout(outputs)\n",
    "        #outputs = self.attention(outputs)\n",
    "        outputs = self.fc1(outputs)\n",
    "        return outputs\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def my_collate(batch):\n",
    "    text_list,input_ids_list,offset_list = [],[],[]\n",
    "    token_type_ids_list,attention_mask_list,origin_label_list = [],[],[]\n",
    "    for data in batch:\n",
    "        text_list.append(data[0])\n",
    "        input_ids_list.append(data[1].tolist())\n",
    "        offset_list.append(data[2])\n",
    "        token_type_ids_list.append(data[3].tolist())\n",
    "        attention_mask_list.append(data[4].tolist())\n",
    "        #current_data_list = get_predictions(data[5])\n",
    "        current_data_list = []\n",
    "        for data1 in data[5]:\n",
    "            if ' ' in data1:\n",
    "                for data2 in data1.split(';'):\n",
    "                    data3 = data2.split(' ')\n",
    "                    current_data_list.append([(int)(data3[0]),(int)(data3[1])])\n",
    "        origin_label_list.append(current_data_list)\n",
    "    input_ids_list = torch.tensor(input_ids_list)\n",
    "    token_type_ids_list = torch.tensor(token_type_ids_list)\n",
    "    attention_mask_list = torch.tensor(attention_mask_list)\n",
    "    return text_list,input_ids_list,offset_list,\\\n",
    "           token_type_ids_list,attention_mask_list,origin_label_list\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,text,input_ids,offset,token_type_ids,attention_mask,label):\n",
    "        self.input_ids = input_ids\n",
    "        self.tensors = [text,\\\n",
    "                        torch.tensor(input_ids,dtype=torch.long),\\\n",
    "                        torch.tensor(offset),\\\n",
    "                       torch.tensor(token_type_ids,dtype=torch.long),\\\n",
    "                       torch.tensor(attention_mask,dtype=torch.long),\\\n",
    "                       torch.tensor(label)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "\n",
    "class ValidDataset(Dataset):\n",
    "    def __init__(self,text,input_ids,offset,token_type_ids,attention_mask,origin_label):\n",
    "        self.input_ids = input_ids\n",
    "        self.tensors = [text,\\\n",
    "                        torch.tensor(input_ids,dtype=torch.long),\\\n",
    "                        torch.tensor(offset),\\\n",
    "                        torch.tensor(token_type_ids,dtype=torch.long),\\\n",
    "                        torch.tensor(attention_mask,dtype=torch.long),\\\n",
    "                        origin_label]\n",
    "        #这里origin_label放入的为['']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "def compute_multilabel_loss(model,batch_token_ids,\\\n",
    "                            batch_token_type_ids,\\\n",
    "                            batch_attention_mask,\\\n",
    "                            batch_label):\n",
    "    with torch.cuda.amp.autocast(enabled=True):\n",
    "        logit = model(input_ids=batch_token_ids,\\\n",
    "                     attention_mask=batch_attention_mask,\\\n",
    "                     token_type_ids=batch_token_type_ids)\n",
    "    logit = logit.view(-1,1)\n",
    "    batch_label = batch_label.view(-1,1)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    loss = loss_fn(logit,batch_label)\n",
    "    torch.set_printoptions(threshold=np.inf)\n",
    "    loss = torch.masked_select(loss,batch_label!=-1)\n",
    "    loss = loss.mean()\n",
    "    #这里的loss不要勿写成logit\n",
    "    return loss\n",
    "\n",
    "def get_char_probs(total_text,offsets,predictions):\n",
    "    results = [np.zeros(len(t)) for t in total_text]\n",
    "    #!!!results 长短不一!!!\n",
    "    #以char为级别计算，应该对整个text计算len\n",
    "    torch.set_printoptions(threshold=np.inf)\n",
    "    for i, (offset, prediction) in enumerate(zip(offsets, predictions)):\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(offset, prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            #results[i][start:end] = ((float)(pred[0].item(),)\n",
    "            results[i][start:end] = pred[0].item()\n",
    "    return results\n",
    "\n",
    "from transformers import DebertaV2Model,DebertaModel\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import itertools\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def get_results(test_text,char_probs, th=0.5):\n",
    "    results = []\n",
    "    #for char_prob in char_probs:\n",
    "    for index in range(len(char_probs)):\n",
    "        char_prob = char_probs[index]\n",
    "        char_text = test_text[index]\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [[min(r),max(r)] for r in result]\n",
    "        \n",
    "        for index1 in range(len(result)):\n",
    "            if result[index1][0]-1 >= 0 and char_text[result[index1][0]-1] != ' ':\n",
    "                result[index1][0] = result[index1][0]-1\n",
    "        \n",
    "        result = [str(r[0])+' '+str(r[1]) for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=42)\n",
    "\n",
    "from transformers import DebertaV2Model,DebertaModel\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import itertools    \n",
    "from tqdm import tqdm\n",
    "\n",
    "bestpointlist = [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
    "#for current_fold in range(total_split):\n",
    "for current_fold in range(1):\n",
    "    train_data = train[train['fold'] != current_fold]\n",
    "    valid_data = train[train['fold'] == current_fold]\n",
    "    train_text,valid_text = [],[]\n",
    "    train_id,valid_id = [],[]\n",
    "    train_input_ids,train_token_type_ids,train_attention_mask = [],[],[]\n",
    "    train_offset,train_label = [],[]\n",
    "    train_length = []\n",
    "    valid_input_ids,valid_token_type_ids,valid_attention_mask = [],[],[]\n",
    "    valid_offset,valid_label = [],[]\n",
    "    valid_length = []\n",
    "    train_origin_label,valid_origin_label = [],[]\n",
    "\n",
    "    for  index,data  in  tqdm(train_data.iterrows(),total=len(train_data)):\n",
    "        #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "        ids = data['id']\n",
    "        text = data['pn_history']\n",
    "        feature_text = data['feature_text']\n",
    "        #print('text = ')\n",
    "        #print(text)\n",
    "        inputs,length = prepare_input(ids,text,feature_text)\n",
    "        #train_text.append(text+feature_text)\n",
    "        train_id.append(ids)\n",
    "        train_text.append(text)\n",
    "        train_input_ids.append(inputs['input_ids'].tolist())\n",
    "        train_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "        train_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "\n",
    "        annotation_length = data['annotation_length']\n",
    "\n",
    "        current_offset,current_label = create_label(text,annotation_length=data['annotation_length'],\\\n",
    "                                        location_list=data['location'])\n",
    "        true_label = change_location_to_offset(text,data['location'])\n",
    "        train_origin_label.append(true_label)\n",
    "        train_offset.append(current_offset)\n",
    "        train_label.append(current_label)\n",
    "        train_length.append(length)\n",
    "\n",
    "    for index,data in tqdm(valid_data.iterrows(),total=len(valid_data)):\n",
    "        ids = data['id']\n",
    "        text = data['pn_history']\n",
    "        feature_text = data['feature_text']\n",
    "        inputs,length = prepare_input(ids,text,feature_text)\n",
    "        #valid_text.append(text+feature_text)\n",
    "        valid_id.append(ids)\n",
    "        valid_text.append(text)\n",
    "        valid_input_ids.append(inputs['input_ids'].tolist())\n",
    "        valid_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "        valid_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "        annotation_length = data['annotation_length']\n",
    "        current_offset,current_label = create_label(text,annotation_length=data['annotation_length'],\\\n",
    "                                     location_list=data['location'])\n",
    "        #true_label = change_location_to_offset(text,data['location'])\n",
    "        #发生bug的地方，true_label的标记错误\n",
    "        valid_offset.append(current_offset)\n",
    "        valid_label.append(data['location'])\n",
    "        valid_length.append(length)\n",
    "    \n",
    "    train_dataset = TrainDataset(train_text,\\\n",
    "                                 train_input_ids,\\\n",
    "                                 train_offset,\\\n",
    "                                train_token_type_ids,\\\n",
    "                                train_attention_mask,\\\n",
    "                                train_label)\n",
    "    valid_dataset = ValidDataset(valid_text,\\\n",
    "                                 valid_input_ids,\\\n",
    "                                 valid_offset,\\\n",
    "                                valid_token_type_ids,\\\n",
    "                                valid_attention_mask,\\\n",
    "                                valid_label)\n",
    "\n",
    "    batch_size = 4\n",
    "    train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset,batch_size=batch_size,shuffle=False,collate_fn = my_collate)\n",
    "    #bcewithlogitloss有sigmoid函数,batch_size必须要大\n",
    "    bestpoint = 0.0\n",
    "    gradient_accumulation_steps = 4\n",
    "    #梯度累积的步数，每训练两次增加一步\n",
    "    predpath = None\n",
    "\n",
    "    deberta = DebertaV2Model.from_pretrained(\"/home/xiaoguzai/模型/deberta-v2-large\")\n",
    "    model = ClassificationModel(deberta)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=1e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5 and epoch < 8:\n",
    "            return 0.2\n",
    "        elif epoch >= 8:\n",
    "            return 0.1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    #lr为lr_lambda为学习率前面乘上的系数\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "    for epoch in range(1):\n",
    "        \n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        losses = AverageMeter()\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        step = 0\n",
    "        prebig = True\n",
    "        for batch_text,batch_ids,batch_offset,batch_token_type_ids,batch_attention_mask,batch_label in tqdm(train_loader):\n",
    "            batch_ids = batch_ids.to(device)\n",
    "            batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            batch_label = batch_label.to(device)\n",
    "            loss = compute_multilabel_loss(model,batch_ids,\\\n",
    "                                batch_token_type_ids,\\\n",
    "                                batch_attention_mask,\\\n",
    "                                batch_label)\n",
    "            r\"\"\"\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss/gradient_accumulation_steps\n",
    "            losses.update(loss.item(),batch_size)\n",
    "            scaler.scale(loss).backward()\n",
    "            \"\"\"\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(),50)\n",
    "            loss.backward()\n",
    "            #每一次进行相应的梯度计算\n",
    "            #\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            #!!!!!!!!!!!!!!!!!!!!!!!!!!!!反向完了才进行梯度裁剪！！！！！！！！！！！！！！！！！！！\n",
    "            #防止梯度爆炸，超过1000的部分不予计算\n",
    "            r\"\"\"\n",
    "            if (step+1)%gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \"\"\"\n",
    "            step = step+1\n",
    "        scheduler.step()\n",
    "        model.eval()\n",
    "        \n",
    "        \n",
    "        #model = torch.load(\"/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/deberta-v3-large模型文件/deberta_capatalize_noid_best_point=0.0_fold=0.pth\")\n",
    "        \n",
    "        pred_result = []\n",
    "        label_result = []\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        for batch_text,batch_ids,batch_offset,batch_token_type_ids,batch_attention_mask,batch_origin_label in tqdm(valid_loader):\n",
    "            batch_ids = batch_ids.to(device)\n",
    "            batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                logit = model(input_ids=batch_ids,token_type_ids=batch_token_type_ids,\\\n",
    "                              attention_mask=batch_attention_mask)\n",
    "                #logit = model(input_ids=batch_ids)\n",
    "                #logit = (4,512,1)\n",
    "                #!!!这点判断需要注意应该是以字符的形式进行判断!!!\n",
    "                #输入的id应该为text+symptom，但是判断正负的时候只应该判断text的内容\n",
    "                #torch.set_printoptions(threshold=np.inf)\n",
    "                logit = torch.sigmoid(logit)\n",
    "                #加上symptom得到的正常的logit\n",
    "                preds = get_char_probs(batch_text,batch_offset,\\\n",
    "                                       logit.cpu())\n",
    "                results = get_results(batch_text,preds,th=0.5)\n",
    "                preds = get_predictions(results)\n",
    "                truths = batch_origin_label\n",
    "                r\"\"\"\n",
    "                preds = \n",
    "                [[[696, 722]], [[668, 693]], [[203, 217]], [[70, 91]]]\n",
    "                truths = \n",
    "                [[[696, 724]], [[668, 693]], [[203, 217]], [[70, 91], [176, 183]]]\n",
    "                \"\"\"\n",
    "                for data in preds:\n",
    "                    pred_result.append(data)\n",
    "                for data in truths:\n",
    "                    label_result.append(data)\n",
    "                \n",
    "                \n",
    "        point = get_score(pred_result,label_result)\n",
    "        point = round(point, 6)\n",
    "        print('point = ')\n",
    "        print(point)\n",
    "        \n",
    "        if point >= bestpoint:\n",
    "            bestpoint = point \n",
    "            prebig = 0\n",
    "            newpath = '/media/xiaoguzai/WD_BLACK/new_deberta-v3/deberta_capatalize_noid_best_point='+str(bestpoint)+'_fold='+str(current_fold)+'.pth'\n",
    "            torch.save(model,newpath)\n",
    "            if predpath != None and os.path.exists(predpath):\n",
    "                os.remove(predpath)\n",
    "            predpath = newpath\n",
    "        else:\n",
    "            prebig = prebig+1\n",
    "\n",
    "        if prebig >= 3 and point < bestpoint:\n",
    "        #连续三个回合小于内容，跳出\n",
    "            break\n",
    "        bestpointlist[current_fold] = max(bestpointlist[current_fold],bestpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d6f226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf45259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[622, 631]],\n",
       " [[633, 652]],\n",
       " [],\n",
       " [[76, 84], [171, 180]],\n",
       " [[254, 270]],\n",
       " [],\n",
       " [[389, 396]],\n",
       " [[284, 303]],\n",
       " [],\n",
       " [[85, 99], [126, 138], [126, 131], [143, 151]],\n",
       " [[64, 75], [187, 209]],\n",
       " [[0, 5]],\n",
       " [[6, 7]],\n",
       " [[483, 500]],\n",
       " [[455, 480]],\n",
       " [[177, 191]],\n",
       " [[124, 135]],\n",
       " [[701, 716]],\n",
       " [],\n",
       " [[555, 563]],\n",
       " [],\n",
       " [],\n",
       " [[41, 55]],\n",
       " [],\n",
       " [[8, 14]],\n",
       " [[15, 16]],\n",
       " [[905, 938], [905, 929]],\n",
       " [[879, 900]],\n",
       " [[267, 282], [172, 187]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[413, 427]],\n",
       " [],\n",
       " [],\n",
       " [[79, 91]],\n",
       " [[103, 128]],\n",
       " [[22, 26]],\n",
       " [[45, 46]],\n",
       " [[666, 691]],\n",
       " [[636, 664]],\n",
       " [[384, 398]],\n",
       " [[120, 132]],\n",
       " [[359, 374]],\n",
       " [],\n",
       " [[571, 579]],\n",
       " [[376, 379]],\n",
       " [],\n",
       " [[133, 147]],\n",
       " [[154, 178], [199, 213]],\n",
       " [[88, 93]],\n",
       " [[94, 95]],\n",
       " [[621, 627], [632, 644]],\n",
       " [[594, 600], [606, 619]],\n",
       " [[282, 297]],\n",
       " [[51, 63], [155, 163]],\n",
       " [],\n",
       " [],\n",
       " [[473, 481], [528, 536], [832, 840]],\n",
       " [],\n",
       " [],\n",
       " [[64, 78], [64, 69], [83, 95]],\n",
       " [[37, 50]],\n",
       " [[13, 18]],\n",
       " [[19, 20]],\n",
       " [[511, 517], [522, 524]],\n",
       " [[484, 490], [495, 509]],\n",
       " [[240, 254]],\n",
       " [[27, 35]],\n",
       " [[220, 235]],\n",
       " [[277, 283], [310, 325]],\n",
       " [[441, 452]],\n",
       " [[214, 217]],\n",
       " [],\n",
       " [[36, 47]],\n",
       " [[48, 73]],\n",
       " [[5, 10]],\n",
       " [[11, 12]],\n",
       " [[689, 695], [716, 722]],\n",
       " [[732, 753]],\n",
       " [[288, 302]],\n",
       " [[93, 114]],\n",
       " [[327, 342]],\n",
       " [],\n",
       " [[418, 425], [800, 807]],\n",
       " [[283, 286]],\n",
       " [],\n",
       " [[41, 53]],\n",
       " [[63, 76]],\n",
       " [[19, 23]],\n",
       " [[23, 24]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[37, 58], [160, 169]],\n",
       " [],\n",
       " [],\n",
       " [[497, 504]],\n",
       " [[285, 288], [673, 676]],\n",
       " [],\n",
       " [[98, 117], [98, 103], [123, 129]],\n",
       " [[16, 25]],\n",
       " [[0, 6]],\n",
       " [[7, 11]],\n",
       " [[677, 686]],\n",
       " [[655, 675]],\n",
       " [],\n",
       " [[51, 63], [202, 209], [389, 397]],\n",
       " [[530, 541]],\n",
       " [],\n",
       " [[187, 194], [359, 367], [480, 487]],\n",
       " [[522, 525]],\n",
       " [],\n",
       " [[64, 76]],\n",
       " [[77, 100]],\n",
       " [[24, 29]],\n",
       " [[30, 31]],\n",
       " [],\n",
       " [[566, 581]],\n",
       " [],\n",
       " [[155, 163]],\n",
       " [],\n",
       " [[346, 352], [389, 406]],\n",
       " [[300, 307], [476, 484]],\n",
       " [[233, 236]],\n",
       " [[262, 268], [273, 281]],\n",
       " [[34, 48]],\n",
       " [[49, 61], [167, 180]],\n",
       " [[0, 12]],\n",
       " [],\n",
       " [[372, 409], [659, 669]],\n",
       " [[671, 694]],\n",
       " [],\n",
       " [[38, 46], [96, 104], [190, 197]],\n",
       " [[259, 270]],\n",
       " [[535, 541], [604, 616], [535, 541], [629, 650]],\n",
       " [[444, 452]],\n",
       " [[251, 254]],\n",
       " [],\n",
       " [[48, 77]],\n",
       " [[26, 37], [105, 129]],\n",
       " [[0, 5]],\n",
       " [[6, 7]],\n",
       " [[693, 702]],\n",
       " [[667, 690]],\n",
       " [[274, 295]],\n",
       " [[47, 55], [251, 258]],\n",
       " [],\n",
       " [],\n",
       " [[323, 331], [436, 444]],\n",
       " [[300, 303]],\n",
       " [],\n",
       " [[16, 30], [56, 70]],\n",
       " [[71, 93]],\n",
       " [[4, 9]],\n",
       " [[10, 11]],\n",
       " [],\n",
       " [[651, 677]],\n",
       " [[224, 238]],\n",
       " [[153, 165], [182, 201]],\n",
       " [],\n",
       " [],\n",
       " [[560, 572]],\n",
       " [[244, 247]],\n",
       " [[249, 272], [274, 297]],\n",
       " [[50, 64], [110, 123]],\n",
       " [[65, 73]],\n",
       " [[0, 5]],\n",
       " [[6, 7]],\n",
       " [[883, 900]],\n",
       " [[862, 881]],\n",
       " [[425, 460]],\n",
       " [[105, 113], [350, 358]],\n",
       " [[189, 200]],\n",
       " [[678, 680], [735, 753]],\n",
       " [[616, 624], [831, 839]],\n",
       " [[494, 497]],\n",
       " [],\n",
       " [[46, 60], [232, 261], [264, 287]],\n",
       " [[65, 88]],\n",
       " [[23, 28]],\n",
       " [[29, 30]],\n",
       " [[590, 596], [603, 605]],\n",
       " [[566, 588]],\n",
       " [[198, 212]],\n",
       " [[91, 108]],\n",
       " [],\n",
       " [],\n",
       " [[348, 354]],\n",
       " [[217, 224]],\n",
       " [],\n",
       " [[25, 39], [25, 30], [44, 50]],\n",
       " [[52, 81], [61, 81]],\n",
       " [[0, 5]],\n",
       " [],\n",
       " [[675, 689]],\n",
       " [[641, 669]],\n",
       " [[312, 326]],\n",
       " [],\n",
       " [[331, 346]],\n",
       " [],\n",
       " [[459, 466], [712, 719]],\n",
       " [[165, 168]],\n",
       " [],\n",
       " [[37, 55]],\n",
       " [[25, 33]],\n",
       " [[0, 5]],\n",
       " [],\n",
       " [],\n",
       " [[597, 625]],\n",
       " [[186, 200]],\n",
       " [[79, 102], [128, 140]],\n",
       " [],\n",
       " [[267, 273], [353, 357], [367, 373]],\n",
       " [[520, 533]],\n",
       " [],\n",
       " [],\n",
       " [[37, 51]],\n",
       " [[59, 77]],\n",
       " [[0, 9]],\n",
       " [[10, 11]],\n",
       " [[629, 653]],\n",
       " [[655, 681]],\n",
       " [[183, 197]],\n",
       " [[39, 47], [99, 107]],\n",
       " [],\n",
       " [[452, 458], [478, 501], [452, 476]],\n",
       " [[692, 699]],\n",
       " [[203, 206]],\n",
       " [[264, 270], [281, 294]],\n",
       " [[48, 60]],\n",
       " [[61, 82]],\n",
       " [[5, 10]],\n",
       " [[11, 12]],\n",
       " [[728, 734], [740, 757]],\n",
       " [[695, 726]],\n",
       " [[314, 322], [333, 335], [354, 359]],\n",
       " [[172, 195], [196, 215]],\n",
       " [[511, 526]],\n",
       " [],\n",
       " [[645, 650], [660, 667]],\n",
       " [[489, 508]],\n",
       " [],\n",
       " [[82, 99]],\n",
       " [[122, 136]],\n",
       " [[26, 37]],\n",
       " [[38, 42]],\n",
       " [[845, 847], [855, 861]],\n",
       " [[814, 824], [832, 838]],\n",
       " [[274, 282]],\n",
       " [[149, 158], [327, 335]],\n",
       " [[309, 325]],\n",
       " [],\n",
       " [[435, 443]],\n",
       " [[284, 303]],\n",
       " [[528, 536], [549, 555], [528, 536], [564, 577]],\n",
       " [[69, 83], [260, 272]],\n",
       " [[124, 147]],\n",
       " [[19, 24]],\n",
       " [[25, 29]],\n",
       " [[206, 218]],\n",
       " [],\n",
       " [[529, 563]],\n",
       " [],\n",
       " [[0, 6]],\n",
       " [],\n",
       " [[265, 306]],\n",
       " [[82, 114]],\n",
       " [[219, 229]],\n",
       " [],\n",
       " [[445, 460]],\n",
       " [[42, 55]],\n",
       " [[7, 8]],\n",
       " [],\n",
       " [],\n",
       " [[854, 887]],\n",
       " [[416, 424]],\n",
       " [[22, 27]],\n",
       " [[406, 424]],\n",
       " [[295, 329]],\n",
       " [[168, 171]],\n",
       " [[461, 467], [518, 525], [461, 467], [527, 536]],\n",
       " [[437, 455]],\n",
       " [[809, 836]],\n",
       " [[122, 145]],\n",
       " [[28, 29]],\n",
       " [],\n",
       " [],\n",
       " [[790, 823], [753, 768]],\n",
       " [[220, 248]],\n",
       " [[8, 15]],\n",
       " [[209, 228]],\n",
       " [[374, 416]],\n",
       " [[61, 65], [129, 132]],\n",
       " [],\n",
       " [[253, 269]],\n",
       " [[545, 560]],\n",
       " [[151, 165]],\n",
       " [[20, 26]],\n",
       " [],\n",
       " [],\n",
       " [[516, 553]],\n",
       " [[340, 357]],\n",
       " [[0, 4]],\n",
       " [[340, 348], [367, 375], [377, 386]],\n",
       " [[195, 230], [421, 468]],\n",
       " [[45, 53]],\n",
       " [[637, 643], [673, 682], [637, 643], [684, 691]],\n",
       " [[275, 286]],\n",
       " [[614, 629]],\n",
       " [[49, 53], [55, 61]],\n",
       " [[23, 24]],\n",
       " [],\n",
       " [[277, 291], [568, 578]],\n",
       " [[753, 785]],\n",
       " [[328, 336]],\n",
       " [[10, 16]],\n",
       " [[328, 336], [351, 359]],\n",
       " [],\n",
       " [[70, 84]],\n",
       " [[523, 541]],\n",
       " [[294, 305]],\n",
       " [],\n",
       " [[87, 106]],\n",
       " [[17, 23]],\n",
       " [[822, 824], [907, 924]],\n",
       " [[534, 547], [549, 571]],\n",
       " [[717, 751]],\n",
       " [[317, 329], [348, 356], [422, 441], [451, 459], [470, 486]],\n",
       " [[0, 4]],\n",
       " [[385, 393], [404, 412]],\n",
       " [[422, 446], [470, 486]],\n",
       " [[22, 30]],\n",
       " [[822, 824], [889, 905]],\n",
       " [],\n",
       " [[800, 811], [812, 815]],\n",
       " [[45, 59]],\n",
       " [[4, 5]],\n",
       " [],\n",
       " [],\n",
       " [[843, 864]],\n",
       " [[281, 309]],\n",
       " [[27, 38]],\n",
       " [[315, 320], [354, 367]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[435, 462]],\n",
       " [[758, 769]],\n",
       " [[73, 101]],\n",
       " [[39, 45]],\n",
       " [[368, 392]],\n",
       " [[333, 344]],\n",
       " [[668, 697]],\n",
       " [[277, 285]],\n",
       " [[13, 17]],\n",
       " [],\n",
       " [[195, 240]],\n",
       " [[37, 62]],\n",
       " [[413, 419], [458, 465], [413, 419], [469, 478]],\n",
       " [],\n",
       " [[644, 659]],\n",
       " [[67, 96]],\n",
       " [[18, 19]],\n",
       " [],\n",
       " [[517, 528]],\n",
       " [[657, 685]],\n",
       " [[475, 489], [499, 509]],\n",
       " [[2, 9]],\n",
       " [],\n",
       " [[342, 349], [363, 379], [381, 393], [403, 418]],\n",
       " [[151, 187]],\n",
       " [],\n",
       " [],\n",
       " [[585, 618]],\n",
       " [[96, 116]],\n",
       " [[14, 20]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[231, 239], [251, 276], [382, 390], [391, 406]],\n",
       " [[5, 10]],\n",
       " [[278, 293], [278, 280], [295, 301]],\n",
       " [[345, 377], [391, 406]],\n",
       " [],\n",
       " [[554, 562],\n",
       "  [585, 602],\n",
       "  [554, 562],\n",
       "  [585, 592],\n",
       "  [603, 607],\n",
       "  [554, 562],\n",
       "  [608, 615]],\n",
       " [[534, 552]],\n",
       " [[831, 846]],\n",
       " [[42, 62]],\n",
       " [[11, 17]],\n",
       " [],\n",
       " [],\n",
       " [[515, 527]],\n",
       " [[341, 349], [351, 365]],\n",
       " [[0, 5]],\n",
       " [],\n",
       " [],\n",
       " [[36, 71]],\n",
       " [],\n",
       " [[220, 238]],\n",
       " [[497, 514]],\n",
       " [[76, 86]],\n",
       " [[6, 7]],\n",
       " [],\n",
       " [[285, 296]],\n",
       " [],\n",
       " [[260, 268]],\n",
       " [[0, 7]],\n",
       " [],\n",
       " [[101, 137]],\n",
       " [[19, 22]],\n",
       " [],\n",
       " [[298, 316]],\n",
       " [[433, 452]],\n",
       " [[26, 48]],\n",
       " [[12, 13]],\n",
       " [],\n",
       " [[476, 497]],\n",
       " [[807, 826]],\n",
       " [[351, 359]],\n",
       " [[5, 10]],\n",
       " [[351, 359], [378, 386]],\n",
       " [[239, 286]],\n",
       " [],\n",
       " [],\n",
       " [[423, 434]],\n",
       " [[575, 612]],\n",
       " [[33, 59]],\n",
       " [[11, 12]],\n",
       " [],\n",
       " [],\n",
       " [[540, 560], [581, 593]],\n",
       " [[269, 277]],\n",
       " [[18, 22]],\n",
       " [[288, 308], [288, 295], [312, 318]],\n",
       " [[181, 208]],\n",
       " [[83, 101]],\n",
       " [[383, 397]],\n",
       " [[249, 267]],\n",
       " [[418, 433], [464, 488]],\n",
       " [[70, 82], [97, 101]],\n",
       " [[23, 24]],\n",
       " [],\n",
       " [],\n",
       " [[667, 702]],\n",
       " [[102, 124]],\n",
       " [[5, 9]],\n",
       " [],\n",
       " [[458, 500]],\n",
       " [[57, 65]],\n",
       " [],\n",
       " [[225, 237], [246, 283]],\n",
       " [[391, 401]],\n",
       " [[61, 93]],\n",
       " [[10, 11]],\n",
       " [],\n",
       " [],\n",
       " [[509, 544]],\n",
       " [[329, 353]],\n",
       " [[0, 5]],\n",
       " [],\n",
       " [[202, 242]],\n",
       " [],\n",
       " [[355, 361], [405, 428]],\n",
       " [],\n",
       " [[583, 598]],\n",
       " [[30, 70]],\n",
       " [[6, 7]],\n",
       " [],\n",
       " [],\n",
       " [[791, 822]],\n",
       " [],\n",
       " [[20, 25]],\n",
       " [],\n",
       " [[488, 545]],\n",
       " [[67, 89]],\n",
       " [],\n",
       " [],\n",
       " [[345, 362], [388, 399]],\n",
       " [[91, 111]],\n",
       " [[26, 27]],\n",
       " [],\n",
       " [[387, 398]],\n",
       " [],\n",
       " [[292, 316]],\n",
       " [[0, 5]],\n",
       " [[292, 300], [331, 344]],\n",
       " [[224, 252]],\n",
       " [[28, 46]],\n",
       " [[400, 406], [432, 457]],\n",
       " [[359, 377]],\n",
       " [[459, 474]],\n",
       " [[47, 58], [70, 85]],\n",
       " [[6, 7]],\n",
       " [],\n",
       " [],\n",
       " [[503, 541]],\n",
       " [[347, 355], [370, 389]],\n",
       " [[0, 11]],\n",
       " [[347, 369]],\n",
       " [[407, 454]],\n",
       " [[45, 60]],\n",
       " [],\n",
       " [],\n",
       " [[584, 599]],\n",
       " [[45, 49], [62, 85]],\n",
       " [[12, 18]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[360, 387], [393, 405], [418, 424]],\n",
       " [[33, 38]],\n",
       " [[407, 424]],\n",
       " [],\n",
       " [[92, 117]],\n",
       " [[442, 444], [480, 487], [442, 444], [489, 501], [442, 444], [503, 518]],\n",
       " [[260, 268]],\n",
       " [[772, 787]],\n",
       " [[182, 204]],\n",
       " [[39, 45]],\n",
       " [[584, 630]],\n",
       " [],\n",
       " [[537, 541]],\n",
       " [[729, 744]],\n",
       " [[174, 189]],\n",
       " [[79, 96]],\n",
       " [],\n",
       " [],\n",
       " [[25, 26]],\n",
       " [],\n",
       " [],\n",
       " [[202, 213]],\n",
       " [[136, 171]],\n",
       " [[118, 130]],\n",
       " [[265, 287]],\n",
       " [],\n",
       " [[20, 22]],\n",
       " [[134, 160]],\n",
       " [],\n",
       " [[372, 375]],\n",
       " [[305, 320], [325, 335], [922, 937]],\n",
       " [[524, 539]],\n",
       " [[72, 89]],\n",
       " [[645, 672]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[112, 132]],\n",
       " [[507, 518]],\n",
       " [[162, 186], [187, 209], [258, 296]],\n",
       " [[90, 110]],\n",
       " [[447, 459]],\n",
       " [],\n",
       " [[24, 30]],\n",
       " [[76, 102]],\n",
       " [],\n",
       " [],\n",
       " [[557, 584]],\n",
       " [[481, 496]],\n",
       " [[57, 74]],\n",
       " [],\n",
       " [],\n",
       " [[34, 40]],\n",
       " [],\n",
       " [],\n",
       " [[277, 288]],\n",
       " [[146, 222], [232, 263]],\n",
       " [],\n",
       " [[293, 301]],\n",
       " [],\n",
       " [[24, 28]],\n",
       " [[268, 286], [291, 303], [309, 316]],\n",
       " [[735, 749]],\n",
       " [[902, 905]],\n",
       " [[868, 883]],\n",
       " [[334, 349], [342, 349], [364, 387]],\n",
       " [[80, 87], [98, 108], [70, 87]],\n",
       " [],\n",
       " [],\n",
       " [[0, 2], [25, 31]],\n",
       " [],\n",
       " [],\n",
       " [[410, 421]],\n",
       " [[130, 163], [165, 188], [193, 214]],\n",
       " [[52, 69]],\n",
       " [[426, 434], [459, 468], [475, 481]],\n",
       " [],\n",
       " [[19, 24]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[466, 481], [499, 511]],\n",
       " [[559, 574]],\n",
       " [[49, 66]],\n",
       " [[607, 618], [623, 627], [633, 653]],\n",
       " [],\n",
       " [[26, 32]],\n",
       " [],\n",
       " [],\n",
       " [[328, 339]],\n",
       " [[95, 114], [141, 173], [189, 222], [223, 263]],\n",
       " [[67, 87]],\n",
       " [[311, 323]],\n",
       " [],\n",
       " [[21, 25]],\n",
       " [[394, 423],\n",
       "  [394, 415],\n",
       "  [424, 440],\n",
       "  [394, 410],\n",
       "  [442, 453],\n",
       "  [394, 410],\n",
       "  [454, 467]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[68, 85], [107, 124]],\n",
       " [],\n",
       " [],\n",
       " [[0, 2], [31, 37]],\n",
       " [],\n",
       " [[317, 337]],\n",
       " [[234, 245], [589, 600]],\n",
       " [[126, 157], [162, 178], [179, 207]],\n",
       " [[98, 106]],\n",
       " [[250, 258]],\n",
       " [],\n",
       " [[20, 25]],\n",
       " [[99, 117], [99, 109], [118, 124], [99, 109], [124, 135]],\n",
       " [],\n",
       " [],\n",
       " [[559, 569]],\n",
       " [[178, 185], [186, 193], [195, 224]],\n",
       " [[12, 38]],\n",
       " [],\n",
       " [],\n",
       " [[6, 7]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[39, 48]],\n",
       " [],\n",
       " [],\n",
       " [[0, 5]],\n",
       " [],\n",
       " [[544, 573]],\n",
       " [[512, 515]],\n",
       " [[782, 816]],\n",
       " [],\n",
       " [[36, 53]],\n",
       " [[633, 706]],\n",
       " [],\n",
       " [[30, 31]],\n",
       " [],\n",
       " [],\n",
       " [[183, 193]],\n",
       " [[81, 122]],\n",
       " [[54, 74]],\n",
       " [[153, 159]],\n",
       " [[161, 177]],\n",
       " [[24, 26]],\n",
       " [],\n",
       " [],\n",
       " [[52, 55]],\n",
       " [[460, 475]],\n",
       " [[545, 560]],\n",
       " [[18, 34]],\n",
       " [],\n",
       " [[244, 286]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[83, 104],\n",
       "  [109, 141],\n",
       "  [143, 174],\n",
       "  [177, 187],\n",
       "  [234, 238],\n",
       "  [177, 187],\n",
       "  [212, 229]],\n",
       " [[0, 14]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[78, 84],\n",
       "  [99, 118],\n",
       "  [78, 84],\n",
       "  [99, 104],\n",
       "  [133, 151],\n",
       "  [78, 84],\n",
       "  [99, 104],\n",
       "  [153, 171]],\n",
       " [],\n",
       " [],\n",
       " [[346, 356], [472, 487]],\n",
       " [[327, 345], [370, 384]],\n",
       " [[16, 32]],\n",
       " [],\n",
       " [],\n",
       " [[10, 11]],\n",
       " [],\n",
       " [],\n",
       " [[430, 441]],\n",
       " [[174, 181], [188, 211], [213, 221], [228, 240], [242, 272]],\n",
       " [[34, 40]],\n",
       " [[443, 451]],\n",
       " [],\n",
       " [[5, 9]],\n",
       " [[50, 72], [80, 87], [50, 64], [73, 87], [50, 64], [91, 103]],\n",
       " [[569, 572], [586, 596]],\n",
       " [],\n",
       " [[390, 405]],\n",
       " [],\n",
       " [[23, 40]],\n",
       " [],\n",
       " [],\n",
       " [[7, 8]],\n",
       " [],\n",
       " [],\n",
       " [[222, 233]],\n",
       " [[189, 203], [199, 216]],\n",
       " [[9, 22]],\n",
       " [[235, 243], [264, 276]],\n",
       " [],\n",
       " [[0, 6]],\n",
       " [[45, 64], [45, 54], [66, 76]],\n",
       " [],\n",
       " [[559, 562]],\n",
       " [[529, 539]],\n",
       " [[198, 213], [315, 330]],\n",
       " [[22, 39]],\n",
       " [],\n",
       " [],\n",
       " [[6, 7]],\n",
       " [],\n",
       " [[162, 174]],\n",
       " [[292, 303]],\n",
       " [[86, 94], [96, 121], [125, 160]],\n",
       " [],\n",
       " [[305, 313]],\n",
       " [],\n",
       " [[0, 5]],\n",
       " [[126, 193]],\n",
       " [],\n",
       " [],\n",
       " [[232, 262]],\n",
       " [],\n",
       " [[150, 193], [195, 212]],\n",
       " [],\n",
       " [],\n",
       " [[7, 11]],\n",
       " [],\n",
       " [[214, 230]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[19, 27], [83, 124]],\n",
       " [],\n",
       " [[0, 6]],\n",
       " [[681, 706]],\n",
       " [],\n",
       " [[835, 838]],\n",
       " [[799, 814]],\n",
       " [],\n",
       " [[74, 102]],\n",
       " [[286, 317]],\n",
       " [],\n",
       " [[0, 3], [27, 33]],\n",
       " [],\n",
       " [[209, 247]],\n",
       " [[395, 406]],\n",
       " [[131, 152], [157, 202]],\n",
       " [[103, 106], [117, 124]],\n",
       " [],\n",
       " [],\n",
       " [[21, 26]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[397, 412]],\n",
       " [[241, 256]],\n",
       " [[46, 63], [65, 92]],\n",
       " [],\n",
       " [],\n",
       " [[7, 13]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[134, 150]],\n",
       " [[28, 45], [93, 104]],\n",
       " [],\n",
       " [],\n",
       " [[0, 6]],\n",
       " [[55, 81],\n",
       "  [55, 65],\n",
       "  [74, 95],\n",
       "  [55, 65],\n",
       "  [74, 81],\n",
       "  [97, 111],\n",
       "  [55, 65],\n",
       "  [74, 81],\n",
       "  [113, 126]],\n",
       " [],\n",
       " [],\n",
       " [[642, 657]],\n",
       " [],\n",
       " [[13, 29], [164, 191], [164, 181], [193, 206]],\n",
       " [],\n",
       " [],\n",
       " [[7, 8]],\n",
       " [],\n",
       " [],\n",
       " [[275, 286]],\n",
       " [[208, 227]],\n",
       " [[30, 51], [140, 162]],\n",
       " [],\n",
       " [[291, 314], [316, 329], [331, 345]],\n",
       " [[0, 6]],\n",
       " [[103, 147], [148, 178]],\n",
       " [],\n",
       " [],\n",
       " [[559, 574]],\n",
       " [[249, 264]],\n",
       " [[84, 101]],\n",
       " [],\n",
       " [],\n",
       " [[32, 38]],\n",
       " [],\n",
       " [],\n",
       " [[213, 224]],\n",
       " [],\n",
       " [[59, 83]],\n",
       " [[229, 237]],\n",
       " [],\n",
       " [[20, 22]],\n",
       " [[376, 402]],\n",
       " [],\n",
       " [[509, 512]],\n",
       " [[477, 491]],\n",
       " [[454, 469]],\n",
       " [[21, 49], [58, 84], [376, 382], [407, 424]],\n",
       " [],\n",
       " [],\n",
       " [[10, 11]],\n",
       " [],\n",
       " [[338, 354]],\n",
       " [[154, 165]],\n",
       " [],\n",
       " [[12, 20]],\n",
       " [[167, 175]],\n",
       " [[192, 205], [206, 212]],\n",
       " [[0, 9]],\n",
       " [[569, 579], [580, 609], [569, 587], [614, 627], [569, 587], [628, 665]],\n",
       " [],\n",
       " [],\n",
       " [[914, 929]],\n",
       " [[378, 393]],\n",
       " [[43, 65], [94, 102], [113, 127], [103, 127]],\n",
       " [],\n",
       " [],\n",
       " [[27, 28], [0, 3]],\n",
       " [],\n",
       " [],\n",
       " [[409, 420]],\n",
       " [[136, 192], [231, 240], [246, 251], [260, 269], [275, 280], [290, 315]],\n",
       " [[67, 83], [67, 70], [76, 83]],\n",
       " [[436, 448]],\n",
       " [],\n",
       " [[15, 26]],\n",
       " [],\n",
       " [],\n",
       " [[455, 458]],\n",
       " [[414, 429]],\n",
       " [[228, 243]],\n",
       " [[33, 50]],\n",
       " [],\n",
       " [],\n",
       " [[23, 27]],\n",
       " [],\n",
       " [[714, 730]],\n",
       " [[202, 213]],\n",
       " [[63, 95], [96, 120]],\n",
       " [[51, 61]],\n",
       " [[215, 223]],\n",
       " [],\n",
       " [[0, 2]],\n",
       " [[57, 79]],\n",
       " [[561, 591]],\n",
       " [[776, 779]],\n",
       " [],\n",
       " [],\n",
       " [[36, 53], [151, 170]],\n",
       " [],\n",
       " [],\n",
       " [[6, 7]],\n",
       " [],\n",
       " [],\n",
       " [[349, 360]],\n",
       " [[172, 235], [262, 280], [237, 260]],\n",
       " [[18, 35]],\n",
       " [[318, 326], [365, 373]],\n",
       " [],\n",
       " [[0, 2]],\n",
       " [[922, 945]],\n",
       " [[69, 84]],\n",
       " [[503, 526]],\n",
       " [[664, 670]],\n",
       " [[61, 68]],\n",
       " [[579, 597]],\n",
       " [[202, 280]],\n",
       " [[798, 812]],\n",
       " [[25, 26]],\n",
       " [[85, 103]],\n",
       " [[393, 433]],\n",
       " [],\n",
       " [],\n",
       " [[292, 335]],\n",
       " [[134, 142]],\n",
       " [[13, 24]],\n",
       " [[604, 639]],\n",
       " [[232, 259]],\n",
       " [[310, 322]],\n",
       " [[386, 401]],\n",
       " [[197, 216]],\n",
       " [],\n",
       " [],\n",
       " [[730, 746]],\n",
       " [[6, 7]],\n",
       " [[70, 90]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[349, 378]],\n",
       " [[436, 442]],\n",
       " [[0, 5]],\n",
       " [[624, 657]],\n",
       " [[64, 82]],\n",
       " [[274, 294]],\n",
       " [[451, 458]],\n",
       " [[42, 49]],\n",
       " [[255, 263], [573, 581]],\n",
       " [],\n",
       " [[538, 560]],\n",
       " [[0, 2], [20, 24]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[131, 139], [163, 181]],\n",
       " [[248, 254]],\n",
       " [[14, 19]],\n",
       " [[582, 606]],\n",
       " [[12, 45]],\n",
       " [[389, 400]],\n",
       " [[544, 550]],\n",
       " [[104, 111]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[6, 7]],\n",
       " [[46, 62]],\n",
       " [],\n",
       " [[395, 414]],\n",
       " [[67, 81], [83, 102]],\n",
       " [[140, 186]],\n",
       " [[252, 261]],\n",
       " [[0, 5]],\n",
       " [[896, 920]],\n",
       " [[69, 84]],\n",
       " [[557, 576]],\n",
       " [[459, 465], [708, 714]],\n",
       " [],\n",
       " [],\n",
       " [[133, 146]],\n",
       " [[843, 862]],\n",
       " [[0, 3], [29, 38]],\n",
       " [[50, 65]],\n",
       " [],\n",
       " [[578, 592]],\n",
       " [[95, 107]],\n",
       " [[272, 302], [192, 202], [236, 254]],\n",
       " [[467, 473]],\n",
       " [],\n",
       " [[658, 681]],\n",
       " [[43, 58]],\n",
       " [[465, 476]],\n",
       " [[636, 642]],\n",
       " [[110, 122]],\n",
       " [],\n",
       " [[65, 77], [160, 179], [193, 208]],\n",
       " [[726, 744]],\n",
       " [[12, 16]],\n",
       " [[26, 39]],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [[256, 262]],\n",
       " [[0, 11]],\n",
       " [[903, 930]],\n",
       " [[22, 37]],\n",
       " [[259, 271]],\n",
       " [[637, 643]],\n",
       " [[181, 188]],\n",
       " [[310, 330], [826, 834]],\n",
       " [[74, 88], [93, 111]],\n",
       " [[768, 784]],\n",
       " [[6, 7]],\n",
       " [[60, 72]],\n",
       " [[133, 165]],\n",
       " [[503, 509], [536, 547]],\n",
       " [[167, 179]],\n",
       " [[449, 501]],\n",
       " [[53, 59]],\n",
       " [[0, 5]],\n",
       " [[489, 520]],\n",
       " [[105, 132]],\n",
       " [[175, 194]],\n",
       " [[324, 339], [528, 537]],\n",
       " [[89, 104]],\n",
       " [],\n",
       " [[25, 34], [50, 85]],\n",
       " [],\n",
       " [[4, 5]],\n",
       " [[6, 21]],\n",
       " [],\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96b0ec72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0364],\n",
       "         [0.0205],\n",
       "         [0.0165],\n",
       "         [0.0214],\n",
       "         [0.0184],\n",
       "         [0.0187],\n",
       "         [0.0192],\n",
       "         [0.0206],\n",
       "         [0.0197],\n",
       "         [0.0198],\n",
       "         [0.0186],\n",
       "         [0.0180],\n",
       "         [0.0193],\n",
       "         [0.0186],\n",
       "         [0.0188],\n",
       "         [0.0187],\n",
       "         [0.0184],\n",
       "         [0.0191],\n",
       "         [0.0171],\n",
       "         [0.0193],\n",
       "         [0.0176],\n",
       "         [0.0172],\n",
       "         [0.0176],\n",
       "         [0.0177],\n",
       "         [0.0174],\n",
       "         [0.0175],\n",
       "         [0.0154],\n",
       "         [0.0175],\n",
       "         [0.0172],\n",
       "         [0.0170],\n",
       "         [0.0186],\n",
       "         [0.0164],\n",
       "         [0.0177],\n",
       "         [0.0167],\n",
       "         [0.0156],\n",
       "         [0.0166],\n",
       "         [0.0165],\n",
       "         [0.0169],\n",
       "         [0.0180],\n",
       "         [0.0162],\n",
       "         [0.0153],\n",
       "         [0.0159],\n",
       "         [0.0167],\n",
       "         [0.0174],\n",
       "         [0.0137],\n",
       "         [0.0156],\n",
       "         [0.0154],\n",
       "         [0.0154],\n",
       "         [0.0165],\n",
       "         [0.0151],\n",
       "         [0.0138],\n",
       "         [0.0156],\n",
       "         [0.0152],\n",
       "         [0.0147],\n",
       "         [0.0140],\n",
       "         [0.0151],\n",
       "         [0.0149],\n",
       "         [0.0161],\n",
       "         [0.0151],\n",
       "         [0.0124],\n",
       "         [0.0137],\n",
       "         [0.0138],\n",
       "         [0.0139],\n",
       "         [0.0146],\n",
       "         [0.0135],\n",
       "         [0.0136],\n",
       "         [0.0150],\n",
       "         [0.0137],\n",
       "         [0.0124],\n",
       "         [0.0129],\n",
       "         [0.0141],\n",
       "         [0.0119],\n",
       "         [0.0122],\n",
       "         [0.0146],\n",
       "         [0.0130],\n",
       "         [0.0130],\n",
       "         [0.0127],\n",
       "         [0.0109],\n",
       "         [0.0119],\n",
       "         [0.0126],\n",
       "         [0.0123],\n",
       "         [0.0107],\n",
       "         [0.0108],\n",
       "         [0.0122],\n",
       "         [0.0117],\n",
       "         [0.0102],\n",
       "         [0.0112],\n",
       "         [0.0121],\n",
       "         [0.0103],\n",
       "         [0.0112],\n",
       "         [0.0101],\n",
       "         [0.0117],\n",
       "         [0.0099],\n",
       "         [0.0118],\n",
       "         [0.0099],\n",
       "         [0.0104],\n",
       "         [0.0110],\n",
       "         [0.0101],\n",
       "         [0.0094],\n",
       "         [0.0098],\n",
       "         [0.0111],\n",
       "         [0.0085],\n",
       "         [0.0097],\n",
       "         [0.0105],\n",
       "         [0.0100],\n",
       "         [0.0096],\n",
       "         [0.0099],\n",
       "         [0.0108],\n",
       "         [0.0094],\n",
       "         [0.0094],\n",
       "         [0.0096],\n",
       "         [0.0078],\n",
       "         [0.0097],\n",
       "         [0.0081],\n",
       "         [0.0097],\n",
       "         [0.0079],\n",
       "         [0.0083],\n",
       "         [0.0082],\n",
       "         [0.0086],\n",
       "         [0.0083],\n",
       "         [0.0071],\n",
       "         [0.0083],\n",
       "         [0.0075],\n",
       "         [0.0086],\n",
       "         [0.0078],\n",
       "         [0.0076],\n",
       "         [0.0076],\n",
       "         [0.0073],\n",
       "         [0.0072],\n",
       "         [0.0062],\n",
       "         [0.0068],\n",
       "         [0.0069],\n",
       "         [0.0067],\n",
       "         [0.0071],\n",
       "         [0.0064],\n",
       "         [0.0066],\n",
       "         [0.0061],\n",
       "         [0.0068],\n",
       "         [0.0060],\n",
       "         [0.0066],\n",
       "         [0.0058],\n",
       "         [0.0058],\n",
       "         [0.0058],\n",
       "         [0.0058],\n",
       "         [0.0056],\n",
       "         [0.0059],\n",
       "         [0.0047],\n",
       "         [0.0057],\n",
       "         [0.0050],\n",
       "         [0.0054],\n",
       "         [0.0054],\n",
       "         [0.0053],\n",
       "         [0.0050],\n",
       "         [0.0046],\n",
       "         [0.0047],\n",
       "         [0.0048],\n",
       "         [0.0046],\n",
       "         [0.0052],\n",
       "         [0.0042],\n",
       "         [0.0048],\n",
       "         [0.0045],\n",
       "         [0.0043],\n",
       "         [0.0052],\n",
       "         [0.0046],\n",
       "         [0.0045],\n",
       "         [0.0046],\n",
       "         [0.0043],\n",
       "         [0.0038],\n",
       "         [0.0039],\n",
       "         [0.0037],\n",
       "         [0.0037],\n",
       "         [0.0038],\n",
       "         [0.0041],\n",
       "         [0.0037],\n",
       "         [0.0036],\n",
       "         [0.0032],\n",
       "         [0.0032],\n",
       "         [0.0033],\n",
       "         [0.0030],\n",
       "         [0.0031],\n",
       "         [0.0030],\n",
       "         [0.0029],\n",
       "         [0.0030],\n",
       "         [0.0033],\n",
       "         [0.0040],\n",
       "         [0.0240],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881]],\n",
       "\n",
       "        [[0.0364],\n",
       "         [0.0203],\n",
       "         [0.0163],\n",
       "         [0.0212],\n",
       "         [0.0183],\n",
       "         [0.0185],\n",
       "         [0.0191],\n",
       "         [0.0205],\n",
       "         [0.0196],\n",
       "         [0.0197],\n",
       "         [0.0185],\n",
       "         [0.0179],\n",
       "         [0.0192],\n",
       "         [0.0185],\n",
       "         [0.0187],\n",
       "         [0.0186],\n",
       "         [0.0183],\n",
       "         [0.0190],\n",
       "         [0.0169],\n",
       "         [0.0191],\n",
       "         [0.0175],\n",
       "         [0.0171],\n",
       "         [0.0175],\n",
       "         [0.0176],\n",
       "         [0.0173],\n",
       "         [0.0174],\n",
       "         [0.0154],\n",
       "         [0.0174],\n",
       "         [0.0171],\n",
       "         [0.0169],\n",
       "         [0.0185],\n",
       "         [0.0163],\n",
       "         [0.0176],\n",
       "         [0.0166],\n",
       "         [0.0155],\n",
       "         [0.0165],\n",
       "         [0.0165],\n",
       "         [0.0169],\n",
       "         [0.0180],\n",
       "         [0.0161],\n",
       "         [0.0152],\n",
       "         [0.0159],\n",
       "         [0.0166],\n",
       "         [0.0174],\n",
       "         [0.0137],\n",
       "         [0.0155],\n",
       "         [0.0154],\n",
       "         [0.0153],\n",
       "         [0.0167],\n",
       "         [0.0150],\n",
       "         [0.0138],\n",
       "         [0.0155],\n",
       "         [0.0152],\n",
       "         [0.0147],\n",
       "         [0.0139],\n",
       "         [0.0151],\n",
       "         [0.0149],\n",
       "         [0.0160],\n",
       "         [0.0151],\n",
       "         [0.0124],\n",
       "         [0.0137],\n",
       "         [0.0138],\n",
       "         [0.0139],\n",
       "         [0.0145],\n",
       "         [0.0136],\n",
       "         [0.0136],\n",
       "         [0.0150],\n",
       "         [0.0137],\n",
       "         [0.0124],\n",
       "         [0.0129],\n",
       "         [0.0142],\n",
       "         [0.0120],\n",
       "         [0.0123],\n",
       "         [0.0147],\n",
       "         [0.0131],\n",
       "         [0.0130],\n",
       "         [0.0128],\n",
       "         [0.0109],\n",
       "         [0.0120],\n",
       "         [0.0126],\n",
       "         [0.0124],\n",
       "         [0.0107],\n",
       "         [0.0109],\n",
       "         [0.0122],\n",
       "         [0.0118],\n",
       "         [0.0103],\n",
       "         [0.0113],\n",
       "         [0.0121],\n",
       "         [0.0103],\n",
       "         [0.0113],\n",
       "         [0.0102],\n",
       "         [0.0118],\n",
       "         [0.0100],\n",
       "         [0.0119],\n",
       "         [0.0100],\n",
       "         [0.0104],\n",
       "         [0.0111],\n",
       "         [0.0102],\n",
       "         [0.0095],\n",
       "         [0.0099],\n",
       "         [0.0112],\n",
       "         [0.0086],\n",
       "         [0.0098],\n",
       "         [0.0106],\n",
       "         [0.0101],\n",
       "         [0.0097],\n",
       "         [0.0100],\n",
       "         [0.0110],\n",
       "         [0.0095],\n",
       "         [0.0096],\n",
       "         [0.0097],\n",
       "         [0.0080],\n",
       "         [0.0099],\n",
       "         [0.0082],\n",
       "         [0.0098],\n",
       "         [0.0080],\n",
       "         [0.0084],\n",
       "         [0.0083],\n",
       "         [0.0087],\n",
       "         [0.0085],\n",
       "         [0.0072],\n",
       "         [0.0085],\n",
       "         [0.0077],\n",
       "         [0.0087],\n",
       "         [0.0080],\n",
       "         [0.0078],\n",
       "         [0.0078],\n",
       "         [0.0075],\n",
       "         [0.0074],\n",
       "         [0.0063],\n",
       "         [0.0070],\n",
       "         [0.0071],\n",
       "         [0.0069],\n",
       "         [0.0073],\n",
       "         [0.0066],\n",
       "         [0.0067],\n",
       "         [0.0063],\n",
       "         [0.0070],\n",
       "         [0.0062],\n",
       "         [0.0068],\n",
       "         [0.0061],\n",
       "         [0.0059],\n",
       "         [0.0060],\n",
       "         [0.0059],\n",
       "         [0.0058],\n",
       "         [0.0061],\n",
       "         [0.0048],\n",
       "         [0.0059],\n",
       "         [0.0052],\n",
       "         [0.0056],\n",
       "         [0.0056],\n",
       "         [0.0055],\n",
       "         [0.0051],\n",
       "         [0.0048],\n",
       "         [0.0049],\n",
       "         [0.0049],\n",
       "         [0.0048],\n",
       "         [0.0054],\n",
       "         [0.0044],\n",
       "         [0.0050],\n",
       "         [0.0046],\n",
       "         [0.0045],\n",
       "         [0.0054],\n",
       "         [0.0047],\n",
       "         [0.0046],\n",
       "         [0.0048],\n",
       "         [0.0044],\n",
       "         [0.0039],\n",
       "         [0.0040],\n",
       "         [0.0038],\n",
       "         [0.0039],\n",
       "         [0.0039],\n",
       "         [0.0042],\n",
       "         [0.0038],\n",
       "         [0.0037],\n",
       "         [0.0033],\n",
       "         [0.0033],\n",
       "         [0.0034],\n",
       "         [0.0031],\n",
       "         [0.0032],\n",
       "         [0.0030],\n",
       "         [0.0029],\n",
       "         [0.0029],\n",
       "         [0.0032],\n",
       "         [0.0035],\n",
       "         [0.0041],\n",
       "         [0.0240],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881]],\n",
       "\n",
       "        [[0.0371],\n",
       "         [0.0208],\n",
       "         [0.0167],\n",
       "         [0.0217],\n",
       "         [0.0188],\n",
       "         [0.0190],\n",
       "         [0.0195],\n",
       "         [0.0210],\n",
       "         [0.0200],\n",
       "         [0.0202],\n",
       "         [0.0190],\n",
       "         [0.0184],\n",
       "         [0.0197],\n",
       "         [0.0190],\n",
       "         [0.0192],\n",
       "         [0.0191],\n",
       "         [0.0187],\n",
       "         [0.0195],\n",
       "         [0.0174],\n",
       "         [0.0196],\n",
       "         [0.0180],\n",
       "         [0.0176],\n",
       "         [0.0180],\n",
       "         [0.0181],\n",
       "         [0.0177],\n",
       "         [0.0179],\n",
       "         [0.0158],\n",
       "         [0.0179],\n",
       "         [0.0176],\n",
       "         [0.0174],\n",
       "         [0.0190],\n",
       "         [0.0167],\n",
       "         [0.0181],\n",
       "         [0.0170],\n",
       "         [0.0159],\n",
       "         [0.0169],\n",
       "         [0.0169],\n",
       "         [0.0173],\n",
       "         [0.0184],\n",
       "         [0.0165],\n",
       "         [0.0156],\n",
       "         [0.0163],\n",
       "         [0.0171],\n",
       "         [0.0178],\n",
       "         [0.0141],\n",
       "         [0.0160],\n",
       "         [0.0158],\n",
       "         [0.0158],\n",
       "         [0.0172],\n",
       "         [0.0154],\n",
       "         [0.0142],\n",
       "         [0.0160],\n",
       "         [0.0156],\n",
       "         [0.0151],\n",
       "         [0.0143],\n",
       "         [0.0155],\n",
       "         [0.0154],\n",
       "         [0.0165],\n",
       "         [0.0155],\n",
       "         [0.0128],\n",
       "         [0.0141],\n",
       "         [0.0142],\n",
       "         [0.0143],\n",
       "         [0.0150],\n",
       "         [0.0140],\n",
       "         [0.0140],\n",
       "         [0.0155],\n",
       "         [0.0141],\n",
       "         [0.0128],\n",
       "         [0.0134],\n",
       "         [0.0146],\n",
       "         [0.0124],\n",
       "         [0.0127],\n",
       "         [0.0152],\n",
       "         [0.0136],\n",
       "         [0.0135],\n",
       "         [0.0132],\n",
       "         [0.0113],\n",
       "         [0.0124],\n",
       "         [0.0130],\n",
       "         [0.0128],\n",
       "         [0.0111],\n",
       "         [0.0114],\n",
       "         [0.0127],\n",
       "         [0.0122],\n",
       "         [0.0107],\n",
       "         [0.0117],\n",
       "         [0.0126],\n",
       "         [0.0108],\n",
       "         [0.0117],\n",
       "         [0.0106],\n",
       "         [0.0123],\n",
       "         [0.0104],\n",
       "         [0.0124],\n",
       "         [0.0104],\n",
       "         [0.0109],\n",
       "         [0.0116],\n",
       "         [0.0106],\n",
       "         [0.0099],\n",
       "         [0.0104],\n",
       "         [0.0117],\n",
       "         [0.0090],\n",
       "         [0.0103],\n",
       "         [0.0111],\n",
       "         [0.0106],\n",
       "         [0.0102],\n",
       "         [0.0105],\n",
       "         [0.0115],\n",
       "         [0.0100],\n",
       "         [0.0101],\n",
       "         [0.0103],\n",
       "         [0.0084],\n",
       "         [0.0105],\n",
       "         [0.0087],\n",
       "         [0.0104],\n",
       "         [0.0085],\n",
       "         [0.0090],\n",
       "         [0.0089],\n",
       "         [0.0092],\n",
       "         [0.0090],\n",
       "         [0.0077],\n",
       "         [0.0091],\n",
       "         [0.0082],\n",
       "         [0.0094],\n",
       "         [0.0086],\n",
       "         [0.0083],\n",
       "         [0.0083],\n",
       "         [0.0081],\n",
       "         [0.0080],\n",
       "         [0.0069],\n",
       "         [0.0076],\n",
       "         [0.0077],\n",
       "         [0.0075],\n",
       "         [0.0080],\n",
       "         [0.0072],\n",
       "         [0.0074],\n",
       "         [0.0069],\n",
       "         [0.0077],\n",
       "         [0.0068],\n",
       "         [0.0075],\n",
       "         [0.0067],\n",
       "         [0.0065],\n",
       "         [0.0066],\n",
       "         [0.0065],\n",
       "         [0.0064],\n",
       "         [0.0067],\n",
       "         [0.0053],\n",
       "         [0.0066],\n",
       "         [0.0057],\n",
       "         [0.0062],\n",
       "         [0.0062],\n",
       "         [0.0060],\n",
       "         [0.0056],\n",
       "         [0.0053],\n",
       "         [0.0053],\n",
       "         [0.0054],\n",
       "         [0.0052],\n",
       "         [0.0058],\n",
       "         [0.0047],\n",
       "         [0.0054],\n",
       "         [0.0050],\n",
       "         [0.0048],\n",
       "         [0.0058],\n",
       "         [0.0050],\n",
       "         [0.0049],\n",
       "         [0.0051],\n",
       "         [0.0047],\n",
       "         [0.0042],\n",
       "         [0.0044],\n",
       "         [0.0041],\n",
       "         [0.0042],\n",
       "         [0.0044],\n",
       "         [0.0047],\n",
       "         [0.0043],\n",
       "         [0.0042],\n",
       "         [0.0037],\n",
       "         [0.0037],\n",
       "         [0.0039],\n",
       "         [0.0036],\n",
       "         [0.0037],\n",
       "         [0.0034],\n",
       "         [0.0033],\n",
       "         [0.0033],\n",
       "         [0.0033],\n",
       "         [0.0032],\n",
       "         [0.0032],\n",
       "         [0.0035],\n",
       "         [0.0030],\n",
       "         [0.0037],\n",
       "         [0.0031],\n",
       "         [0.0037],\n",
       "         [0.0039],\n",
       "         [0.0244],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881]],\n",
       "\n",
       "        [[0.0371],\n",
       "         [0.0206],\n",
       "         [0.0166],\n",
       "         [0.0215],\n",
       "         [0.0186],\n",
       "         [0.0188],\n",
       "         [0.0194],\n",
       "         [0.0208],\n",
       "         [0.0199],\n",
       "         [0.0200],\n",
       "         [0.0188],\n",
       "         [0.0182],\n",
       "         [0.0195],\n",
       "         [0.0188],\n",
       "         [0.0190],\n",
       "         [0.0189],\n",
       "         [0.0185],\n",
       "         [0.0193],\n",
       "         [0.0172],\n",
       "         [0.0194],\n",
       "         [0.0177],\n",
       "         [0.0175],\n",
       "         [0.0177],\n",
       "         [0.0178],\n",
       "         [0.0175],\n",
       "         [0.0177],\n",
       "         [0.0157],\n",
       "         [0.0177],\n",
       "         [0.0174],\n",
       "         [0.0172],\n",
       "         [0.0188],\n",
       "         [0.0166],\n",
       "         [0.0179],\n",
       "         [0.0168],\n",
       "         [0.0157],\n",
       "         [0.0167],\n",
       "         [0.0167],\n",
       "         [0.0171],\n",
       "         [0.0182],\n",
       "         [0.0163],\n",
       "         [0.0155],\n",
       "         [0.0161],\n",
       "         [0.0169],\n",
       "         [0.0176],\n",
       "         [0.0139],\n",
       "         [0.0158],\n",
       "         [0.0156],\n",
       "         [0.0156],\n",
       "         [0.0170],\n",
       "         [0.0152],\n",
       "         [0.0140],\n",
       "         [0.0157],\n",
       "         [0.0154],\n",
       "         [0.0149],\n",
       "         [0.0142],\n",
       "         [0.0153],\n",
       "         [0.0151],\n",
       "         [0.0162],\n",
       "         [0.0153],\n",
       "         [0.0126],\n",
       "         [0.0139],\n",
       "         [0.0140],\n",
       "         [0.0141],\n",
       "         [0.0148],\n",
       "         [0.0138],\n",
       "         [0.0137],\n",
       "         [0.0152],\n",
       "         [0.0139],\n",
       "         [0.0126],\n",
       "         [0.0131],\n",
       "         [0.0144],\n",
       "         [0.0122],\n",
       "         [0.0124],\n",
       "         [0.0149],\n",
       "         [0.0132],\n",
       "         [0.0132],\n",
       "         [0.0129],\n",
       "         [0.0111],\n",
       "         [0.0121],\n",
       "         [0.0128],\n",
       "         [0.0126],\n",
       "         [0.0109],\n",
       "         [0.0111],\n",
       "         [0.0124],\n",
       "         [0.0119],\n",
       "         [0.0105],\n",
       "         [0.0114],\n",
       "         [0.0123],\n",
       "         [0.0105],\n",
       "         [0.0115],\n",
       "         [0.0103],\n",
       "         [0.0120],\n",
       "         [0.0102],\n",
       "         [0.0121],\n",
       "         [0.0101],\n",
       "         [0.0106],\n",
       "         [0.0112],\n",
       "         [0.0103],\n",
       "         [0.0096],\n",
       "         [0.0101],\n",
       "         [0.0114],\n",
       "         [0.0087],\n",
       "         [0.0100],\n",
       "         [0.0107],\n",
       "         [0.0103],\n",
       "         [0.0099],\n",
       "         [0.0101],\n",
       "         [0.0112],\n",
       "         [0.0097],\n",
       "         [0.0097],\n",
       "         [0.0099],\n",
       "         [0.0081],\n",
       "         [0.0101],\n",
       "         [0.0084],\n",
       "         [0.0100],\n",
       "         [0.0082],\n",
       "         [0.0086],\n",
       "         [0.0085],\n",
       "         [0.0089],\n",
       "         [0.0087],\n",
       "         [0.0074],\n",
       "         [0.0087],\n",
       "         [0.0079],\n",
       "         [0.0089],\n",
       "         [0.0082],\n",
       "         [0.0080],\n",
       "         [0.0080],\n",
       "         [0.0077],\n",
       "         [0.0076],\n",
       "         [0.0065],\n",
       "         [0.0072],\n",
       "         [0.0072],\n",
       "         [0.0071],\n",
       "         [0.0075],\n",
       "         [0.0068],\n",
       "         [0.0070],\n",
       "         [0.0065],\n",
       "         [0.0072],\n",
       "         [0.0064],\n",
       "         [0.0070],\n",
       "         [0.0062],\n",
       "         [0.0061],\n",
       "         [0.0062],\n",
       "         [0.0061],\n",
       "         [0.0059],\n",
       "         [0.0062],\n",
       "         [0.0049],\n",
       "         [0.0061],\n",
       "         [0.0053],\n",
       "         [0.0057],\n",
       "         [0.0058],\n",
       "         [0.0056],\n",
       "         [0.0052],\n",
       "         [0.0049],\n",
       "         [0.0050],\n",
       "         [0.0050],\n",
       "         [0.0048],\n",
       "         [0.0054],\n",
       "         [0.0044],\n",
       "         [0.0051],\n",
       "         [0.0047],\n",
       "         [0.0045],\n",
       "         [0.0054],\n",
       "         [0.0047],\n",
       "         [0.0047],\n",
       "         [0.0048],\n",
       "         [0.0045],\n",
       "         [0.0040],\n",
       "         [0.0041],\n",
       "         [0.0038],\n",
       "         [0.0039],\n",
       "         [0.0041],\n",
       "         [0.0044],\n",
       "         [0.0039],\n",
       "         [0.0038],\n",
       "         [0.0034],\n",
       "         [0.0033],\n",
       "         [0.0035],\n",
       "         [0.0032],\n",
       "         [0.0033],\n",
       "         [0.0031],\n",
       "         [0.0030],\n",
       "         [0.0029],\n",
       "         [0.0031],\n",
       "         [0.0039],\n",
       "         [0.0035],\n",
       "         [0.0043],\n",
       "         [0.0242],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881],\n",
       "         [0.8881]]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde40dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
