{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717a05ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 42146/42146 [00:13<00:00, 3195.59it/s]\n",
      "100%|██████████████████████████████████████| 143/143 [00:00<00:00, 22131.49it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************current_fold = 0************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/11440 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|███████████████████████████████████| 11440/11440 [00:11<00:00, 1025.80it/s]\n",
      "100%|█████████████████████████████████████| 5705/5705 [00:05<00:00, 1005.80it/s]\n",
      "100%|█████████████████████████████████████| 2860/2860 [00:02<00:00, 1035.28it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 1e-06\n",
      "epoch = 0\n",
      "第0个epoch的学习率：0.00000200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [15:03<00:00,  4.75it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:12<00:00,  9.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.88872953\n",
      "epoch = 1\n",
      "第1个epoch的学习率：0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:25<00:00,  4.35it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:16<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.88702423\n",
      "epoch = 2\n",
      "第2个epoch的学习率：0.00000067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:24<00:00,  4.36it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:17<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.88636422\n",
      "epoch = 3\n",
      "第3个epoch的学习率：0.00000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:24<00:00,  4.35it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:17<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.88607399\n",
      "************current_fold = 1************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 11440/11440 [00:11<00:00, 964.83it/s]\n",
      "100%|██████████████████████████████████████| 5705/5705 [00:05<00:00, 977.74it/s]\n",
      "100%|█████████████████████████████████████| 2860/2860 [00:02<00:00, 1029.26it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 1e-06\n",
      "epoch = 0\n",
      "第0个epoch的学习率：0.00000200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:09<00:00,  4.42it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:16<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.88021824\n",
      "epoch = 1\n",
      "第1个epoch的学习率：0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:06<00:00,  4.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:17<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8797998\n",
      "epoch = 2\n",
      "第2个epoch的学习率：0.00000067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:05<00:00,  4.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:16<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.88324167\n",
      "epoch = 3\n",
      "第3个epoch的学习率：0.00000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:03<00:00,  4.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:16<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.88110856\n",
      "************current_fold = 2************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 11440/11440 [00:12<00:00, 928.28it/s]\n",
      "100%|██████████████████████████████████████| 5705/5705 [00:06<00:00, 923.99it/s]\n",
      "100%|█████████████████████████████████████| 2860/2860 [00:02<00:00, 1011.83it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 1e-06\n",
      "epoch = 0\n",
      "第0个epoch的学习率：0.00000200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:07<00:00,  4.43it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:17<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8764292\n",
      "epoch = 1\n",
      "第1个epoch的学习率：0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:03<00:00,  4.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:17<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.87766669\n",
      "epoch = 2\n",
      "第2个epoch的学习率：0.00000067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:02<00:00,  4.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:17<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.87726466\n",
      "epoch = 3\n",
      "第3个epoch的学习率：0.00000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:01<00:00,  4.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:17<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.87750528\n",
      "************current_fold = 3************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 11440/11440 [00:12<00:00, 908.63it/s]\n",
      "100%|██████████████████████████████████████| 5705/5705 [00:05<00:00, 975.80it/s]\n",
      "100%|█████████████████████████████████████| 2860/2860 [00:02<00:00, 1023.69it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 1e-06\n",
      "epoch = 0\n",
      "第0个epoch的学习率：0.00000200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:07<00:00,  4.43it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:16<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.87981817\n",
      "epoch = 1\n",
      "第1个epoch的学习率：0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:06<00:00,  4.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:16<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.87915084\n",
      "epoch = 2\n",
      "第2个epoch的学习率：0.00000067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:05<00:00,  4.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:16<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.87912843\n",
      "epoch = 3\n",
      "第3个epoch的学习率：0.00000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:03<00:00,  4.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:17<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.88049932\n",
      "************current_fold = 4************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 11440/11440 [00:12<00:00, 924.94it/s]\n",
      "100%|██████████████████████████████████████| 5705/5705 [00:06<00:00, 903.32it/s]\n",
      "100%|██████████████████████████████████████| 2860/2860 [00:02<00:00, 987.05it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 1e-06\n",
      "epoch = 0\n",
      "第0个epoch的学习率：0.00000200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:08<00:00,  4.42it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:16<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.88304716\n",
      "epoch = 1\n",
      "第1个epoch的学习率：0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:04<00:00,  4.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:16<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.88506564\n",
      "epoch = 2\n",
      "第2个epoch的学习率：0.00000067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:04<00:00,  4.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:16<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.88674583\n",
      "epoch = 3\n",
      "第3个epoch的学习率：0.00000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4287/4287 [16:05<00:00,  4.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [01:16<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.88498146\n"
     ]
    }
   ],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "# This must be done before importing transformers\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "transformers_path = Path(\"/home/xiaoguzai/.local/lib/python3.9/site-packages/transformers\")\n",
    "\n",
    "input_dir = Path(\"/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/代码\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "#print('convert_file = ')\n",
    "#print(convert_file)\n",
    "#print('conversion_path = ')\n",
    "#print(conversion_path)\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path \n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "    filepath = deberta_v2_path/filename\n",
    "    \n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)\n",
    "    \n",
    "from transformers import AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "import transformers\n",
    "tokenizer = DebertaV2TokenizerFast.from_pretrained('/home/xiaoguzai/模型/deberta-v3-large')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "train = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/train.csv')\n",
    "\n",
    "train['annotation'] = train['annotation'].apply(ast.literal_eval)\n",
    "train['location'] = train['location'].apply(ast.literal_eval)\n",
    "features = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/features.csv')\n",
    "features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "\n",
    "patient_notes = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/patient_notes.csv')\n",
    "train = train.merge(features, on=['feature_num','case_num'],how='left')\n",
    "train.head()\n",
    "\n",
    "train = train.merge(patient_notes, on=['pn_num','case_num'],how='left')\n",
    "train.head()\n",
    "\n",
    "import re\n",
    "def process_feature_text(text):\n",
    "    text = re.sub('I-year', '1-year', text)\n",
    "    text = re.sub('-OR-', \" or \", text)\n",
    "    text = re.sub('-', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_spaces(txt):\n",
    "    txt = re.sub('\\n', ' ', txt)\n",
    "    txt = re.sub('\\t', ' ', txt)\n",
    "    txt = re.sub('\\r', ' ', txt)\n",
    "#    txt = re.sub(r'\\s+', ' ', txt)\n",
    "    return txt\n",
    "train[\"feature_text\"] = train[\"feature_text\"].apply(process_feature_text)\n",
    "train[\"pn_history\"] = train[\"pn_history\"].apply(clean_spaces)\n",
    "train[\"feature_text\"] = train[\"feature_text\"].apply(clean_spaces)\n",
    "#!!!不加capitalize()得分0.852(maxlen=310),加capitalize()得分0.861\n",
    "#测试maxlen=466+capitalize=0.670,maxlen=350+capitalize()=0.8693\n",
    "train[\"pn_history\"] = train[\"pn_history\"].apply(lambda x:x.capitalize())\n",
    "\n",
    "import ast\n",
    "# incorrect annotation\n",
    "train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=42)\n",
    "\n",
    "total_fold = 5\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "Fold = GroupKFold(n_splits=5)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index,val_index) in enumerate(Fold.split(train,train['location'],groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "#按照groups也就是train['pn_num']进行划分\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "train['annotation_length'] = train['annotation'].apply(len)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "pn_history_lengths = []\n",
    "for text in tqdm(patient_notes['pn_history'].fillna(\"\").values,total=len(patient_notes)):\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    pn_history_lengths.append(length)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "features_lengths = []\n",
    "result_lengths = []\n",
    "i = 0\n",
    "for text in tqdm(features['feature_text'].fillna(\"\").values, total=len(features)):\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    features_lengths.append(length)\n",
    "    result_lengths.append(pn_history_lengths[i]+length+3)\n",
    "    i = i+1\n",
    "max_len = max(pn_history_lengths)+max(features_lengths)+3\n",
    "a = 100\n",
    "bins = int((max(result_lengths)-min(result_lengths))/a)\n",
    "plt.hist(result_lengths,bins,density=1,stacked=True)\n",
    "plt.grid(True,linestyle='--',alpha=0.5)\n",
    "plt.show()\n",
    "max_len = 360\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,inputs,labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.tensors = [torch.tensor(self.inputs),\\\n",
    "                       torch.tensor(self.labels)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "#下面prepare_input的时候将text和feature_text合在一起很巧妙\n",
    "\n",
    "def prepare_input(ids, text, feature_text):\n",
    "    #inputs1 = tokenizer.encode_plus(text,\\\n",
    "    #                               add_special_tokens=True,\\\n",
    "    #                               max_length = max_len,\\\n",
    "    #                               paddin)\n",
    "    inputs = {}\n",
    "    inputs1 = tokenizer.encode_plus(text,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   max_length = max_len,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs2 = tokenizer.encode_plus(feature_text,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   max_length = max_len,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    ids1,ids2 = ids.split('_')\n",
    "    inputs3 = tokenizer.encode_plus(ids1,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs4 = tokenizer.encode_plus(ids2,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs2['input_ids'] = inputs2['input_ids']+inputs3['input_ids'][1:]+inputs4['input_ids'][1:]\n",
    "    inputs2['attention_mask'] = inputs2['attention_mask']+inputs3['attention_mask'][1:]+inputs4['attention_mask'][1:]\n",
    "    #inputs2['token_type_ids'] = inputs2['token_type_ids']+inputs3['token_type_ids'][1:]+inputs4['token_type_ids'][1:]\n",
    "    \n",
    "    if len(inputs1['input_ids'])+len(inputs2['input_ids'])-2 > max_len:\n",
    "        exceed_length = len(inputs1['input_ids'])+len(inputs2['input_ids'])-max_len-2\n",
    "        inputs['input_ids'] = inputs1['input_ids'][:len(inputs1['input_ids'])-exceed_length-1]+inputs2['input_ids'][1:]\n",
    "        inputs['attention_mask'] = [1]*max_len\n",
    "    else:\n",
    "        inputs['input_ids'] = inputs1['input_ids'][:-1]+inputs2['input_ids'][1:-1]\n",
    "        inputs['input_ids'] = inputs['input_ids']+[tokenizer.sep_token_id]\n",
    "        #inputs['input_ids'] = inputs['input_ids']+[tokenizer.sep_token_id]+[tokenizer.pad_token_id]*(max_len-len(inputs['input_ids'])-1)\n",
    "        inputs['attention_mask'] = [1]*(len(inputs['input_ids']))\n",
    "        inputs['attention_mask'] = inputs['attention_mask']+[0]*(max_len-len(inputs['input_ids']))\n",
    "        inputs['input_ids'] = inputs['input_ids']+[tokenizer.pad_token_id]*(max_len-len(inputs['input_ids']))\n",
    "        #inputs['token_type_ids'] = [0]*max_len\n",
    "        #inputs['attention_mask'] = [1]*max_len\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v,dtype=torch.long)\n",
    "    return inputs,len(inputs['input_ids'])\n",
    "\n",
    "#打标记的时候还是只放入text的内容，不考虑feature_text的文本内容\n",
    "def create_label(text,location_list):\n",
    "    encoded = tokenizer.encode_plus(text,\\\n",
    "                                add_special_tokens=True,\\\n",
    "                                max_length = max_len,\\\n",
    "                                padding = \"max_length\",\\\n",
    "                                return_offsets_mapping = True,\\\n",
    "                                truncation=True)\n",
    "    offset_mapping = encoded['offset_mapping']\n",
    "    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "    label = np.zeros(len(offset_mapping))\n",
    "    label[ignore_idxes] = -1\n",
    "    if annotation_length != 0:\n",
    "        for location in location_list:\n",
    "            #location = 2 4,location = 8 10\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                #loc = ['2','4'],loc = ['8','10']\n",
    "                start_idx = -1\n",
    "                end_idx = -1\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                #start = 2,end = 4;start = 8,end = 10;\n",
    "                #!!!这里的start,end标记的为字符:Character spans indicating \n",
    "                #the location of each annotation within the note.\n",
    "                #注意前面的标注Character spans\n",
    "                for idx in range(len(offset_mapping)):\n",
    "                    #if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                        #start_idx还能往前去\n",
    "                        start_idx = idx - 1\n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                        #end_idx还能往后去\n",
    "                        end_idx = idx+1\n",
    "                        #end_idx = idx\n",
    "                if start_idx == -1:\n",
    "                    start_idx = end_idx\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    label[start_idx:end_idx] = 1\n",
    "    return offset_mapping,label\n",
    "\n",
    "def change_location_to_offset(text,location_list):\n",
    "    results = np.zeros(len(text))\n",
    "    #以char为级别计算，应该对整个text计算len\n",
    "    for idx, offset_mapping in enumerate(location_list):\n",
    "        try:\n",
    "            start = (int)(offset_mapping[0])\n",
    "            end = (int)(offset_mapping[1])\n",
    "            results[start:end] = 1\n",
    "        except:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.fc1 = nn.Linear(1024,1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids,\\\n",
    "                           attention_mask=attention_mask)\n",
    "        outputs = outputs[0]\n",
    "        #outputs = self.dropout(outputs)\n",
    "        outputs = self.fc1(outputs)\n",
    "        return outputs\n",
    "    \n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def my_collate(batch):\n",
    "    text_list,input_ids_list,offset_list = [],[],[]\n",
    "    attention_mask_list,origin_label_list = [],[]\n",
    "    for data in batch:\n",
    "        text_list.append(data[0])\n",
    "        input_ids_list.append(data[1].tolist())\n",
    "        offset_list.append(data[2])\n",
    "        attention_mask_list.append(data[3].tolist())\n",
    "        current_data_list = []\n",
    "        for data1 in data[4]:\n",
    "            if ' ' in data1:\n",
    "                for data2 in data1.split(';'):\n",
    "                    data3 = data2.split(' ')\n",
    "                    current_data_list.append([(int)(data3[0]),(int)(data3[1])])\n",
    "        origin_label_list.append(current_data_list)\n",
    "    input_ids_list = torch.tensor(input_ids_list)\n",
    "    attention_mask_list = torch.tensor(attention_mask_list)\n",
    "    return text_list,input_ids_list,offset_list,\\\n",
    "           attention_mask_list,origin_label_list\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,text,input_ids,offset,attention_mask,label):\n",
    "        self.input_ids = input_ids\n",
    "        self.tensors = [text,\\\n",
    "                        torch.tensor(input_ids,dtype=torch.long),\\\n",
    "                        torch.tensor(offset),\\\n",
    "                       torch.tensor(attention_mask,dtype=torch.long),\\\n",
    "                       torch.tensor(label)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "\n",
    "class ValidDataset(Dataset):\n",
    "    def __init__(self,text,input_ids,offset,attention_mask,origin_label):\n",
    "        self.input_ids = input_ids\n",
    "        self.tensors = [text,\\\n",
    "                        torch.tensor(input_ids,dtype=torch.long),\\\n",
    "                        torch.tensor(offset),\\\n",
    "                        torch.tensor(attention_mask,dtype=torch.long),\\\n",
    "                        origin_label]\n",
    "        #这里origin_label放入的为['']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "def compute_multilabel_loss(model,batch_token_ids,\\\n",
    "                            batch_attention_mask,\\\n",
    "                            batch_label):\n",
    "    with torch.cuda.amp.autocast(enabled=True):\n",
    "        logit = model(input_ids=batch_token_ids,\\\n",
    "                     attention_mask=batch_attention_mask)\n",
    "    logit = logit.view(-1,1)\n",
    "    batch_label = batch_label.view(-1,1)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    loss = loss_fn(logit,batch_label)\n",
    "    loss = torch.masked_select(loss,batch_label!=-1)\n",
    "    loss = loss.mean()\n",
    "    #这里的loss不要勿写成logit\n",
    "    return loss\n",
    "\n",
    "def get_char_probs(total_text,offsets,predictions,th=0.5):\n",
    "    results = [np.zeros(len(t)) for t in total_text]\n",
    "    #!!!results 长短不一!!!\n",
    "    #以char为级别计算，应该对整个text计算len\n",
    "    prepred = 0.0\n",
    "    torch.set_printoptions(threshold=np.inf)\n",
    "    for i, (offset, prediction) in enumerate(zip(offsets, predictions)):\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(offset, prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            if pred[0] > th and prepred < th and results[i][start] == ' ':\n",
    "            #当前标1,前面标0,并且当前打头的是空格\n",
    "                start = start+1\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred[0].item()\n",
    "            #prepred = pred[0].item()\n",
    "    return results\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0]\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)+1}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def post_process_spaces(target, text, th=0.5):\n",
    "    target = np.copy(target)\n",
    "\n",
    "    if text[0] == \" \":\n",
    "        target[0] = 0\n",
    "    if text[-1] == \" \":\n",
    "        target[-1] = 0\n",
    "\n",
    "    for i in range(1, len(text) - 1):\n",
    "        if text[i] == \" \":\n",
    "            if target[i] >= th and target[i - 1] < th:  # space before\n",
    "                target[i] = 0\n",
    "\n",
    "            if target[i] >= th and target[i + 1] < th:  # space after\n",
    "                target[i] = 0\n",
    "\n",
    "            if target[i - 1] >= th and target[i + 1] >= th:\n",
    "                target[i] = 1\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=42)\n",
    "\n",
    "\n",
    "from transformers import DebertaV2Model,DebertaModel\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from transformers import RobertaModel\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import itertools    \n",
    "from tqdm import tqdm\n",
    "\n",
    "bestpointlist = [0.0,0.0,0.0,0.0,0.0]\n",
    "bestmodellist = ['/media/xiaoguzai/WD_BLACK/deberta-v3-large模型文件/deberta_capatalize_noid_best_point0.887656_fold0.pth',\\\n",
    "              '/media/xiaoguzai/WD_BLACK/deberta-v3-large模型文件/deberta_capatalize_noid_best_point0.883649_fold1.pth',\\\n",
    "              '/media/xiaoguzai/WD_BLACK/deberta-v3-large模型文件/deberta_capatalize_noid_best_point0.88135_fold2.pth',\\\n",
    "              '/media/xiaoguzai/WD_BLACK/deberta-v3-large模型文件/deberta_capatalize_noid_best_point0.881055_fold3.pth',\\\n",
    "              '/media/xiaoguzai/WD_BLACK/deberta-v3-large模型文件/deberta_capatalize_noid_best_point0.88443_fold4.pth']\n",
    "#for current_fold in range(total_fold):\n",
    "for current_fold in range(total_fold):\n",
    "    print('************current_fold = %d************'%current_fold)\n",
    "    train_data = train[train['fold'] != current_fold]\n",
    "    valid_data = train[train['fold'] == current_fold]\n",
    "    train_text,valid_text = [],[]\n",
    "    train_input_ids,train_token_type_ids,train_attention_mask = [],[],[]\n",
    "    train_offset,train_label = [],[]\n",
    "    train_length = []\n",
    "    valid_input_ids,valid_token_type_ids,valid_attention_mask = [],[],[]\n",
    "    valid_offset,valid_label = [],[]\n",
    "    valid_length = []\n",
    "    train_origin_label,valid_origin_label = [],[]\n",
    "    pseudo_id,pseudo_text = [],[]\n",
    "    pseudo_label = []\n",
    "    pseudo_input_ids,pseudo_token_type_ids,pseudo_attention_mask = [],[],[]\n",
    "    pseudo_offset,pseudo_label = [],[]\n",
    "    pseudo_length = []\n",
    "    \n",
    "    pseudo_labeling_train = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/pseudo_labeling_deberta-v3_split='+str(current_fold)+'_train.csv')\n",
    "    pseudo_labeling_train['location'] = pseudo_labeling_train['location'].apply(ast.literal_eval)\n",
    "    pseudo_labeling_train = pseudo_labeling_train.merge(features, on=['feature_num','case_num'],how='left')\n",
    "    pseudo_labeling_train = pseudo_labeling_train.merge(patient_notes, on=['pn_num','case_num'],how='left')\n",
    "    #这里数据读取有bug，一个history被读取了好多次\n",
    "    pseudo_labeling_train = pseudo_labeling_train.dropna()\n",
    "    pseudo_labeling_train[\"pn_history\"] = pseudo_labeling_train[\"pn_history\"].apply(lambda x:x.capitalize())\n",
    "    #pseudo_labeling_train['annotation_length'] = pseudo_labeling_train['annotation'].apply(len)\n",
    "\n",
    "    for  index,data  in  tqdm(train_data.iterrows(),total=len(train_data)):\n",
    "        #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "        ids = data['id']\n",
    "        text = data['pn_history']\n",
    "        feature_text = data['feature_text']\n",
    "        inputs,length = prepare_input(ids,text,feature_text)\n",
    "        #train_text.append(text+feature_text)\n",
    "        train_text.append(text)\n",
    "        train_input_ids.append(inputs['input_ids'].tolist())\n",
    "        train_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "\n",
    "        annotation_length = data['annotation_length']\n",
    "\n",
    "        current_offset,current_label = create_label(text,\\\n",
    "                                        location_list=data['location'])\n",
    "        true_label = change_location_to_offset(text,data['location'])\n",
    "        train_origin_label.append(true_label)\n",
    "        train_offset.append(current_offset)\n",
    "        train_label.append(current_label)\n",
    "        train_length.append(length)\n",
    "\n",
    "    for  index,data  in  tqdm(pseudo_labeling_train.iterrows(),total=len(pseudo_labeling_train)):\n",
    "        #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "        ids = data['id']\n",
    "        text = data['pn_history']\n",
    "        feature_text = data['feature_text']\n",
    "        inputs,length = prepare_input(ids,text,feature_text)\n",
    "        pseudo_id.append(ids)\n",
    "        pseudo_text.append(text)\n",
    "        pseudo_input_ids.append(inputs['input_ids'].tolist())\n",
    "        #pseudo_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "        pseudo_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "        current_offset,current_label = create_label(text,\\\n",
    "                                        location_list=data['location'])\n",
    "        true_label = change_location_to_offset(text,data['location'])\n",
    "        pseudo_offset.append(current_offset)\n",
    "        pseudo_label.append(current_label)\n",
    "        \n",
    "    train_text = train_text+pseudo_text\n",
    "    train_offset = train_offset+pseudo_offset\n",
    "    train_input_ids = train_input_ids+pseudo_input_ids\n",
    "    #train_token_type_ids = train_token_type_ids+pseudo_token_type_ids\n",
    "    train_attention_mask = train_attention_mask+pseudo_attention_mask\n",
    "    train_label = train_label+pseudo_label\n",
    "        \n",
    "    for index,data in tqdm(valid_data.iterrows(),total=len(valid_data)):\n",
    "        ids = data['id']\n",
    "        text = data['pn_history']\n",
    "        feature_text = data['feature_text']\n",
    "        inputs,length = prepare_input(ids,text,feature_text)\n",
    "        #valid_text.append(text+feature_text)\n",
    "        valid_text.append(text)\n",
    "        valid_input_ids.append(inputs['input_ids'].tolist())\n",
    "        valid_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "        annotation_length = data['annotation_length']\n",
    "        current_offset,current_label = create_label(text,\\\n",
    "                                     location_list=data['location'])\n",
    "        #true_label = change_location_to_offset(text,data['location'])\n",
    "        #发生bug的地方，true_label的标记错误\n",
    "        valid_offset.append(current_offset)\n",
    "        valid_label.append(data['location'])\n",
    "        valid_length.append(length)\n",
    "\n",
    "    train_dataset = TrainDataset(train_text,\\\n",
    "                                 train_input_ids,\\\n",
    "                                 train_offset,\\\n",
    "                                train_attention_mask,\\\n",
    "                                train_label)\n",
    "    valid_dataset = ValidDataset(valid_text,\\\n",
    "                                 valid_input_ids,\\\n",
    "                                 valid_offset,\\\n",
    "                                valid_attention_mask,\\\n",
    "                                valid_label)\n",
    "\n",
    "    batch_size = 4\n",
    "    train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset,batch_size=batch_size,shuffle=False,collate_fn = my_collate)\n",
    "    #bcewithlogitloss有sigmoid函数,batch_size必须要大\n",
    "    bestpoint = 0.0\n",
    "    gradient_accumulation_steps = 4\n",
    "    #梯度累积的步数，每训练两次增加一步\n",
    "\n",
    "    deberta = DebertaV2Model.from_pretrained(\"/home/xiaoguzai/模型/deberta-v3-large\")\n",
    "    model = ClassificationModel(deberta)\n",
    "    model = torch.load(bestmodellist[current_fold])\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=1e-6)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    for epoch in range(4):\n",
    "        \n",
    "        print('epoch = %d'%epoch)\n",
    "        print(\"第%d个epoch的学习率：%.8f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "    \n",
    "        losses = AverageMeter()\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "        step = 0\n",
    "        prebig = True\n",
    "        for batch_text,batch_ids,batch_offset,batch_attention_mask,batch_label in tqdm(train_loader):\n",
    "            batch_ids = batch_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            batch_label = batch_label.to(device)\n",
    "            loss = compute_multilabel_loss(model,batch_ids,\\\n",
    "                                batch_attention_mask,\\\n",
    "                                batch_label)\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss/gradient_accumulation_steps\n",
    "            losses.update(loss.item(),batch_size)\n",
    "            scaler.scale(loss).backward()\n",
    "            #loss.backward()\n",
    "            #每一次进行相应的梯度计算\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
    "            #防止梯度爆炸，超过1000的部分不予计算\n",
    "            if (step+1)%gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            step = step+1\n",
    "        scheduler.step()\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        pred_result = []\n",
    "        label_result = []\n",
    "        for batch_text,batch_ids,batch_offset,batch_attention_mask,batch_origin_label in tqdm(valid_loader):\n",
    "            batch_ids = batch_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                logit = model(input_ids=batch_ids,\\\n",
    "                              attention_mask=batch_attention_mask)\n",
    "                #logit = model(input_ids=batch_ids)\n",
    "                #logit = (4,512,1)\n",
    "                #!!!这点判断需要注意应该是以字符的形式进行判断!!!\n",
    "                #输入的id应该为text+symptom，但是判断正负的时候只应该判断text的内容\n",
    "                #torch.set_printoptions(threshold=np.inf)\n",
    "                logit = torch.sigmoid(logit)\n",
    "                #加上symptom得到的正常的logit\n",
    "                preds = get_char_probs(batch_text,batch_offset,\\\n",
    "                                       logit.cpu())\n",
    "                \n",
    "                for index in range(len(preds)):\n",
    "                    pred_data = preds[index]\n",
    "                    text_data = batch_text[index]\n",
    "                    pred_data = post_process_spaces(pred_data,text_data)\n",
    "                    #current_result.append(pred_data)\n",
    "                    preds[index] = pred_data\n",
    "                \n",
    "                results = get_results(preds,th=0.5)\n",
    "                preds = get_predictions(results)\n",
    "                truths = batch_origin_label\n",
    "                r\"\"\"\n",
    "                preds = \n",
    "                [[[696, 722]], [[668, 693]], [[203, 217]], [[70, 91]]]\n",
    "                truths = \n",
    "                [[[696, 724]], [[668, 693]], [[203, 217]], [[70, 91], [176, 183]]]\n",
    "                \"\"\"\n",
    "                \n",
    "                for data in preds:\n",
    "                    pred_result.append(data)\n",
    "                for data in truths:\n",
    "                    label_result.append(data)\n",
    "        \n",
    "        point = get_score(pred_result,label_result)\n",
    "        point = round(point, 8)\n",
    "        print('point = ')\n",
    "        print(point)\n",
    "        if point > bestpoint:\n",
    "            bestpoint = point \n",
    "            prebig = 0\n",
    "            #torch.save(model,'roberta_Groupsplit_best_point='+str(bestpoint)+'_fold='+str(current_fold)+'.pth')\n",
    "            torch.save(model,'/media/xiaoguzai/WD_BLACK/deberta-large-v3-submit/deberta_Groupsplit_best_point='+str(bestpoint)+'_fold='+str(current_fold)+'.pth')\n",
    "        else:\n",
    "            prebig = prebig+1\n",
    "\n",
    "        if prebig == 4:\n",
    "            break\n",
    "        bestpointlist[current_fold] = max(bestpointlist[current_fold],bestpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb68df02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624917d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|███████████████████████████████████| 42146/42146 [00:13<00:00, 3158.57it/s]\n",
      "100%|██████████████████████████████████████| 143/143 [00:00<00:00, 26449.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len = 354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/11440 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|███████████████████████████████████| 11440/11440 [00:11<00:00, 1013.10it/s]\n",
      "100%|█████████████████████████████████████| 2860/2860 [00:02<00:00, 1057.32it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 1e-05\n",
      "第0个epoch的学习率：0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2860 [00:00<?, ?it/s]/tmp/ipykernel_26424/2118087243.py:692: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.32it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.840585\n",
      "第1个epoch的学习率：0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:11<00:00, 11.37it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.867622\n",
      "第2个epoch的学习率：0.000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:10<00:00, 11.40it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.87501\n",
      "第3个epoch的学习率：0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:11<00:00, 11.37it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.874445\n",
      "第4个epoch的学习率：0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:11<00:00, 11.36it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.878297\n",
      "第5个epoch的学习率：0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:11<00:00, 11.36it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.875887\n",
      "第6个epoch的学习率：0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.35it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.877113\n",
      "第7个epoch的学习率：0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:11<00:00, 11.36it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.878072\n",
      "第8个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:11<00:00, 11.35it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.877224\n",
      "第9个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.875763\n",
      "第10个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.34it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.875973\n",
      "第11个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.876863\n",
      "第12个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.876832\n",
      "第13个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.34it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.877779\n",
      "第14个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████▉                     | 1314/2860 [01:55<02:16, 11.35it/s]/tmp/ipykernel_26424/2118087243.py:692: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.34it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.878683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 11440/11440 [00:12<00:00, 932.30it/s]\n",
      "100%|█████████████████████████████████████| 2860/2860 [00:02<00:00, 1035.63it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 1e-05\n",
      "第0个epoch的学习率：0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2860 [00:00<?, ?it/s]/tmp/ipykernel_26424/2118087243.py:692: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [04:13<00:00, 11.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.808472\n",
      "第1个epoch的学习率：0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.32it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.843513\n",
      "第2个epoch的学习率：0.000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.858316\n",
      "第3个epoch的学习率：0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.32it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.862814\n",
      "第4个epoch的学习率：0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.31it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.864533\n",
      "第5个epoch的学习率：0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.32it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.863754\n",
      "第6个epoch的学习率：0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.865462\n",
      "第7个epoch的学习率：0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.32it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.863556\n",
      "第8个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.31it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.865192\n",
      "第9个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.866353\n",
      "第10个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.31it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.867077\n",
      "第11个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:13<00:00, 11.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.866161\n",
      "第12个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████▏               | 1702/2860 [02:30<01:41, 11.37it/s]/tmp/ipykernel_26424/2118087243.py:692: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [04:13<00:00, 11.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.867469\n",
      "第13个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.31it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.866224\n",
      "第14个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.32it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.869819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 11440/11440 [00:12<00:00, 944.33it/s]\n",
      "100%|█████████████████████████████████████| 2860/2860 [00:02<00:00, 1033.37it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 1e-05\n",
      "第0个epoch的学习率：0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2860 [00:00<?, ?it/s]/tmp/ipykernel_26424/2118087243.py:692: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [04:13<00:00, 11.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.80548\n",
      "第1个epoch的学习率：0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.31it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.846063\n",
      "第2个epoch的学习率：0.000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.32it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.86188\n",
      "第3个epoch的学习率：0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.31it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.863206\n",
      "第4个epoch的学习率：0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.32it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.865675\n",
      "第5个epoch的学习率：0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.31it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.868546\n",
      "第6个epoch的学习率：0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:13<00:00, 11.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.869687\n",
      "第7个epoch的学习率：0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:13<00:00, 11.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.871187\n",
      "第8个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:13<00:00, 11.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.871262\n",
      "第9个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:13<00:00, 11.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.871993\n",
      "第10个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:13<00:00, 11.29it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.870764\n",
      "第11个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:13<00:00, 11.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.870876\n",
      "第12个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████████▌           | 2020/2860 [02:58<01:14, 11.20it/s]/tmp/ipykernel_26424/2118087243.py:692: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [04:13<00:00, 11.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.872238\n",
      "第13个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:13<00:00, 11.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.870577\n",
      "第14个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.31it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.869266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 11440/11440 [00:12<00:00, 935.72it/s]\n",
      "100%|█████████████████████████████████████| 2860/2860 [00:02<00:00, 1062.83it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 1e-05\n",
      "第0个epoch的学习率：0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2860 [00:00<?, ?it/s]/tmp/ipykernel_26424/2118087243.py:692: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.814678\n",
      "第1个epoch的学习率：0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.32it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.845607\n",
      "第2个epoch的学习率：0.000007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.863821\n",
      "第3个epoch的学习率：0.000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.32it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.86649\n",
      "第4个epoch的学习率：0.000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.866341\n",
      "第5个epoch的学习率：0.000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.34it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.869258\n",
      "第6个epoch的学习率：0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.32it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.869974\n",
      "第7个epoch的学习率：0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.32it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.868592\n",
      "第8个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.870929\n",
      "第9个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.32it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.869404\n",
      "第10个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.870835\n",
      "第11个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████▋                                | 548/2860 [00:48<03:24, 11.28it/s]/tmp/ipykernel_26424/2118087243.py:692: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.871815\n",
      "第12个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.871907\n",
      "第13个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.873316\n",
      "第14个epoch的学习率：0.000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████▍                                | 536/2860 [00:47<03:25, 11.31it/s]/tmp/ipykernel_26424/2118087243.py:692: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [04:12<00:00, 11.35it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:27<00:00, 25.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.873408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 11440/11440 [00:12<00:00, 936.00it/s]\n",
      "100%|█████████████████████████████████████| 2860/2860 [00:02<00:00, 1028.80it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化的学习率： 1e-05\n",
      "第0个epoch的学习率：0.000020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2860 [00:00<?, ?it/s]/tmp/ipykernel_26424/2118087243.py:692: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "  9%|███▌                                    | 255/2860 [00:22<03:46, 11.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26424/2118087243.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mbatch_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_attention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0mbatch_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             loss = compute_multilabel_loss(model,batch_ids,\\\n\u001b[0m\u001b[1;32m    683\u001b[0m                                 \u001b[0mbatch_token_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                                 \u001b[0mbatch_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_26424/2118087243.py\u001b[0m in \u001b[0;36mcompute_multilabel_loss\u001b[0;34m(model, batch_token_ids, batch_token_type_ids, batch_attention_mask, batch_label)\u001b[0m\n\u001b[1;32m    427\u001b[0m                             batch_label):\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         logit = model(input_ids=batch_token_ids,\\\n\u001b[0m\u001b[1;32m    430\u001b[0m                      \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                      token_type_ids=batch_token_type_ids)\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_26424/2118087243.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         outputs = self.model(input_ids=input_ids,\\\n\u001b[0m\u001b[1;32m    349\u001b[0m                            \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                            attention_mask=attention_mask)\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1059\u001b[0m         )\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 )\n\u001b[1;32m    492\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 output_states = layer_module(\n\u001b[0m\u001b[1;32m    494\u001b[0m                     \u001b[0mnext_kv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     ):\n\u001b[0;32m--> 337\u001b[0;31m         attention_output = self.attention(\n\u001b[0m\u001b[1;32m    338\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     ):\n\u001b[0;32m--> 268\u001b[0;31m         self_output = self.self(\n\u001b[0m\u001b[1;32m    269\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelative_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0mrel_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m             rel_att = self.disentangled_attention_bias(\n\u001b[0m\u001b[1;32m    696\u001b[0m                 \u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py\u001b[0m in \u001b[0;36mdisentangled_attention_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0matt_span\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_ebd_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m         \u001b[0mrelative_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelative_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0mrel_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrel_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_ebd_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0matt_span\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_ebd_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0matt_span\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "# This must be done before importing transformers\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "transformers_path = Path(\"/home/xiaoguzai/.local/lib/python3.9/site-packages/transformers\")\n",
    "\n",
    "input_dir = Path(\"/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/代码\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "#print('convert_file = ')\n",
    "#print(convert_file)\n",
    "#print('conversion_path = ')\n",
    "#print(conversion_path)\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path \n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "    filepath = deberta_v2_path/filename\n",
    "    \n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)\n",
    "    \n",
    "from transformers import AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "import transformers\n",
    "tokenizer = DebertaV2TokenizerFast.from_pretrained('/home/xiaoguzai/模型/deberta-v3-base')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "train = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/train.csv')\n",
    "\n",
    "train['annotation'] = train['annotation'].apply(ast.literal_eval)\n",
    "train['location'] = train['location'].apply(ast.literal_eval)\n",
    "features = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/features.csv')\n",
    "features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "\n",
    "patient_notes = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/patient_notes.csv')\n",
    "train = train.merge(features, on=['feature_num','case_num'],how='left')\n",
    "train.head()\n",
    "\n",
    "train = train.merge(patient_notes, on=['pn_num','case_num'],how='left')\n",
    "train.head()\n",
    "#这里数据读取有bug，一个history被读取了好多次\n",
    "train[\"pn_history\"] = train[\"pn_history\"].apply(lambda x:x.capitalize())\n",
    "\n",
    "\n",
    "import ast\n",
    "# incorrect annotation\n",
    "train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=31415)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "Fold = GroupKFold(n_splits=5)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index,val_index) in enumerate(Fold.split(train,train['location'],groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "#按照groups也就是train['pn_num']进行划分\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "train['annotation_length'] = train['annotation'].apply(len)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "pn_history_lengths = []\n",
    "for text in tqdm(patient_notes['pn_history'].fillna(\"\").values,total=len(patient_notes)):\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    pn_history_lengths.append(length)\n",
    "    \n",
    "features_lengths = []\n",
    "for text in tqdm(features['feature_text'].fillna(\"\").values, total=len(features)):\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    features_lengths.append(length)\n",
    "max_len = max(pn_history_lengths)+max(features_lengths)+3\n",
    "print('max_len = %d'%max_len)\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,inputs,labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.tensors = [torch.tensor(self.inputs),\\\n",
    "                       torch.tensor(self.labels)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "\n",
    "max_len = 360\n",
    "#下面prepare_input的时候将text和feature_text合在一起很巧妙\n",
    "def prepare_input(ids, text, feature_text):\n",
    "    #inputs1 = tokenizer.encode_plus(text,\\\n",
    "    #                               add_special_tokens=True,\\\n",
    "    #                               max_length = max_len,\\\n",
    "    #                               paddin)\n",
    "    inputs = {}\n",
    "    inputs1 = tokenizer.encode_plus(text,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   max_length = max_len,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs2 = tokenizer.encode_plus(feature_text,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   max_length = max_len,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    r\"\"\"\n",
    "    ids1,ids2 = ids.split('_')\n",
    "    inputs3 = tokenizer.encode_plus(ids1,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs4 = tokenizer.encode_plus(ids2,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs2['input_ids'] = inputs2['input_ids']+inputs3['input_ids'][1:]+inputs4['input_ids'][1:]\n",
    "    inputs2['attention_mask'] = inputs2['attention_mask']+inputs3['attention_mask'][1:]+inputs4['attention_mask'][1:]\n",
    "    inputs2['token_type_ids'] = inputs2['token_type_ids']+inputs3['token_type_ids'][1:]+inputs4['token_type_ids'][1:]\n",
    "    \"\"\"\n",
    "    if len(inputs1['input_ids'])+len(inputs2['input_ids'])-2 > max_len:\n",
    "        exceed_length = len(inputs1['input_ids'])+len(inputs2['input_ids'])-max_len-2\n",
    "        inputs['input_ids'] = inputs1['input_ids'][:len(inputs1['input_ids'])-exceed_length-1]+inputs2['input_ids'][1:]\n",
    "        inputs['attention_mask'] = [1]*max_len\n",
    "    else:\n",
    "        inputs['input_ids'] = inputs1['input_ids'][:-1]+inputs2['input_ids'][1:-1]\n",
    "        inputs['input_ids'] = inputs['input_ids']+[tokenizer.sep_token_id]\n",
    "        #inputs['input_ids'] = inputs['input_ids']+[tokenizer.sep_token_id]+[tokenizer.pad_token_id]*(max_len-len(inputs['input_ids'])-1)\n",
    "        inputs['attention_mask'] = [1]*(len(inputs['input_ids']))\n",
    "        inputs['attention_mask'] = inputs['attention_mask']+[0]*(max_len-len(inputs['input_ids']))\n",
    "        inputs['input_ids'] = inputs['input_ids']+[tokenizer.pad_token_id]*(max_len-len(inputs['input_ids']))\n",
    "        inputs['token_type_ids'] = [0]*max_len\n",
    "        #inputs['attention_mask'] = [1]*max_len\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v,dtype=torch.long)\n",
    "    return inputs,len(inputs['input_ids'])\n",
    "\n",
    "#打标记的时候还是只放入text的内容，不考虑feature_text的文本内容\n",
    "def create_label(text, annotation_length, location_list):\n",
    "    encoded = tokenizer.encode_plus(text,\\\n",
    "                                add_special_tokens=True,\\\n",
    "                                max_length = max_len,\\\n",
    "                                padding = \"max_length\",\\\n",
    "                                return_offsets_mapping = True)\n",
    "    offset_mapping = encoded['offset_mapping']\n",
    "    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "    label = np.zeros(len(offset_mapping))\n",
    "    label[ignore_idxes] = -1\n",
    "    if annotation_length != 0:\n",
    "        for location in location_list:\n",
    "            #location = 2 4,location = 8 10\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                #loc = ['2','4'],loc = ['8','10']\n",
    "                start_idx = -1\n",
    "                end_idx = -1\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                #start = 2,end = 4;start = 8,end = 10;\n",
    "                #!!!这里的start,end标记的为字符:Character spans indicating \n",
    "                #the location of each annotation within the note.\n",
    "                #注意前面的标注Character spans\n",
    "                for idx in range(len(offset_mapping)):\n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                        start_idx = idx - 1\n",
    "                        #print('111start_idx = %d 111'%start_idx)\n",
    "                        #字符比当前字符小的时候，指向前一位\n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                        end_idx = idx + 1\n",
    "                        #字符比当前字符大的时候，指向后一位\n",
    "                        #print('222start_idx = %d 222'%end_idx)\n",
    "                if start_idx == -1:\n",
    "                    start_idx = end_idx\n",
    "                    #print('333start_idx = %d 333'%start_idx)\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    #print('***start_idx = %d***'%start_idx)\n",
    "                    #print('***end_idx = %d***'%end_idx)\n",
    "                    label[start_idx:end_idx] = 1\n",
    "    return offset_mapping,label\n",
    "\n",
    "\n",
    "\n",
    "def change_location_to_offset(text,location_list):\n",
    "    results = np.zeros(len(text))\n",
    "    #以char为级别计算，应该对整个text计算len\n",
    "    for idx, offset_mapping in enumerate(location_list):\n",
    "        try:\n",
    "            start = (int)(offset_mapping[0])\n",
    "            end = (int)(offset_mapping[1])\n",
    "            results[start:end] = 1\n",
    "        except:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        #self.fc1 = nn.Linear(768,1)\n",
    "        #self.fc1 = nn.Linear(1024,1)\n",
    "        self.fc1 = nn.Linear(768,1)\n",
    "        \n",
    "    def forward(self,input_ids,token_type_ids,attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids,\\\n",
    "                           token_type_ids=token_type_ids,\\\n",
    "                           attention_mask=attention_mask)\n",
    "        outputs = outputs[0]\n",
    "        #outputs = self.dropout(outputs)\n",
    "        outputs = self.fc1(outputs)\n",
    "        return outputs\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def my_collate(batch):\n",
    "    text_list,input_ids_list,offset_list = [],[],[]\n",
    "    token_type_ids_list,attention_mask_list,origin_label_list = [],[],[]\n",
    "    for data in batch:\n",
    "        text_list.append(data[0])\n",
    "        input_ids_list.append(data[1].tolist())\n",
    "        offset_list.append(data[2])\n",
    "        token_type_ids_list.append(data[3].tolist())\n",
    "        attention_mask_list.append(data[4].tolist())\n",
    "        #current_data_list = get_predictions(data[5])\n",
    "        current_data_list = []\n",
    "        for data1 in data[5]:\n",
    "            if ' ' in data1:\n",
    "                for data2 in data1.split(';'):\n",
    "                    data3 = data2.split(' ')\n",
    "                    current_data_list.append([(int)(data3[0]),(int)(data3[1])])\n",
    "        origin_label_list.append(current_data_list)\n",
    "    input_ids_list = torch.tensor(input_ids_list)\n",
    "    token_type_ids_list = torch.tensor(token_type_ids_list)\n",
    "    attention_mask_list = torch.tensor(attention_mask_list)\n",
    "    return text_list,input_ids_list,offset_list,\\\n",
    "           token_type_ids_list,attention_mask_list,origin_label_list\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,text,input_ids,offset,token_type_ids,attention_mask,label):\n",
    "        self.input_ids = input_ids\n",
    "        self.tensors = [text,\\\n",
    "                        torch.tensor(input_ids,dtype=torch.long),\\\n",
    "                        torch.tensor(offset),\\\n",
    "                       torch.tensor(token_type_ids,dtype=torch.long),\\\n",
    "                       torch.tensor(attention_mask,dtype=torch.long),\\\n",
    "                       torch.tensor(label)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "\n",
    "class ValidDataset(Dataset):\n",
    "    def __init__(self,text,input_ids,offset,token_type_ids,attention_mask,origin_label):\n",
    "        self.input_ids = input_ids\n",
    "        self.tensors = [text,\\\n",
    "                        torch.tensor(input_ids,dtype=torch.long),\\\n",
    "                        torch.tensor(offset),\\\n",
    "                        torch.tensor(token_type_ids,dtype=torch.long),\\\n",
    "                        torch.tensor(attention_mask,dtype=torch.long),\\\n",
    "                        origin_label]\n",
    "        #这里origin_label放入的为['']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "def compute_multilabel_loss(model,batch_token_ids,\\\n",
    "                            batch_token_type_ids,\\\n",
    "                            batch_attention_mask,\\\n",
    "                            batch_label):\n",
    "    with torch.cuda.amp.autocast(enabled=True):\n",
    "        logit = model(input_ids=batch_token_ids,\\\n",
    "                     attention_mask=batch_attention_mask,\\\n",
    "                     token_type_ids=batch_token_type_ids)\n",
    "    logit = logit.view(-1,1)\n",
    "    batch_label = batch_label.view(-1,1)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    loss = loss_fn(logit,batch_label)\n",
    "    torch.set_printoptions(threshold=np.inf)\n",
    "    loss = torch.masked_select(loss,batch_label!=-1)\n",
    "    loss = loss.mean()\n",
    "    #这里的loss不要勿写成logit\n",
    "    return loss\n",
    "\n",
    "def get_char_probs(total_text,offsets,predictions):\n",
    "    results = [np.zeros(len(t)) for t in total_text]\n",
    "    #!!!results 长短不一!!!\n",
    "    #以char为级别计算，应该对整个text计算len\n",
    "    torch.set_printoptions(threshold=np.inf)\n",
    "    for i, (offset, prediction) in enumerate(zip(offsets, predictions)):\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(offset, prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            #results[i][start:end] = ((float)(pred[0].item(),)\n",
    "            results[i][start:end] = pred[0].item()\n",
    "    return results\n",
    "\n",
    "from transformers import DebertaV2Model,DebertaModel\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import itertools\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def get_results(test_text,char_probs, th=0.5):\n",
    "    results = []\n",
    "    #for char_prob in char_probs:\n",
    "    for index in range(len(char_probs)):\n",
    "        char_prob = char_probs[index]\n",
    "        char_text = test_text[index]\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [[min(r),max(r)] for r in result]\n",
    "        \n",
    "        for index1 in range(len(result)):\n",
    "            if result[index1][0]-1 >= 0 and char_text[result[index1][0]-1] != ' ':\n",
    "                result[index1][0] = result[index1][0]-1\n",
    "        \n",
    "        result = [str(r[0])+' '+str(r[1]) for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "from transformers import DebertaV2Model,DebertaModel\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import itertools    \n",
    "from tqdm import tqdm\n",
    "\n",
    "bestpointlist = [0.0,0.0,0.0,0.0,0.0]\n",
    "for current_fold in range(5):\n",
    "    train_data = train[train['fold'] != current_fold]\n",
    "    valid_data = train[train['fold'] == current_fold]\n",
    "    train_text,valid_text = [],[]\n",
    "    train_id,valid_id = [],[]\n",
    "    train_input_ids,train_token_type_ids,train_attention_mask = [],[],[]\n",
    "    train_offset,train_label = [],[]\n",
    "    train_length = []\n",
    "    valid_input_ids,valid_token_type_ids,valid_attention_mask = [],[],[]\n",
    "    valid_offset,valid_label = [],[]\n",
    "    valid_length = []\n",
    "    train_origin_label,valid_origin_label = [],[]\n",
    "\n",
    "    for  index,data  in  tqdm(train_data.iterrows(),total=len(train_data)):\n",
    "        #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "        ids = data['id']\n",
    "        text = data['pn_history']\n",
    "        feature_text = data['feature_text']\n",
    "        #print('text = ')\n",
    "        #print(text)\n",
    "        inputs,length = prepare_input(ids,text,feature_text)\n",
    "        #train_text.append(text+feature_text)\n",
    "        train_id.append(ids)\n",
    "        train_text.append(text)\n",
    "        train_input_ids.append(inputs['input_ids'].tolist())\n",
    "        train_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "        train_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "\n",
    "        annotation_length = data['annotation_length']\n",
    "\n",
    "        current_offset,current_label = create_label(text,annotation_length=data['annotation_length'],\\\n",
    "                                        location_list=data['location'])\n",
    "        true_label = change_location_to_offset(text,data['location'])\n",
    "        train_origin_label.append(true_label)\n",
    "        train_offset.append(current_offset)\n",
    "        train_label.append(current_label)\n",
    "        train_length.append(length)\n",
    "\n",
    "    for index,data in tqdm(valid_data.iterrows(),total=len(valid_data)):\n",
    "        ids = data['id']\n",
    "        text = data['pn_history']\n",
    "        feature_text = data['feature_text']\n",
    "        inputs,length = prepare_input(ids,text,feature_text)\n",
    "        #valid_text.append(text+feature_text)\n",
    "        valid_id.append(ids)\n",
    "        valid_text.append(text)\n",
    "        valid_input_ids.append(inputs['input_ids'].tolist())\n",
    "        valid_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "        valid_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "        annotation_length = data['annotation_length']\n",
    "        current_offset,current_label = create_label(text,annotation_length=data['annotation_length'],\\\n",
    "                                     location_list=data['location'])\n",
    "        #true_label = change_location_to_offset(text,data['location'])\n",
    "        #发生bug的地方，true_label的标记错误\n",
    "        valid_offset.append(current_offset)\n",
    "        valid_label.append(data['location'])\n",
    "        valid_length.append(length)\n",
    "\n",
    "    train_dataset = TrainDataset(train_text,\\\n",
    "                                 train_input_ids,\\\n",
    "                                 train_offset,\\\n",
    "                                train_token_type_ids,\\\n",
    "                                train_attention_mask,\\\n",
    "                                train_label)\n",
    "    valid_dataset = ValidDataset(valid_text,\\\n",
    "                                 valid_input_ids,\\\n",
    "                                 valid_offset,\\\n",
    "                                valid_token_type_ids,\\\n",
    "                                valid_attention_mask,\\\n",
    "                                valid_label)\n",
    "\n",
    "    batch_size = 4\n",
    "    train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset,batch_size=batch_size,shuffle=False,collate_fn = my_collate)\n",
    "    #bcewithlogitloss有sigmoid函数,batch_size必须要大\n",
    "    bestpoint = 0.0\n",
    "    gradient_accumulation_steps = 4\n",
    "    #梯度累积的步数，每训练两次增加一步\n",
    "    predpath = None\n",
    "\n",
    "    deberta = DebertaV2Model.from_pretrained(\"/home/xiaoguzai/模型/deberta-v3-base\")\n",
    "    model = ClassificationModel(deberta)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=1e-5)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5 and epoch < 8:\n",
    "            return 0.2\n",
    "        elif epoch >= 8:\n",
    "            return 0.1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    #lr为lr_lambda为学习率前面乘上的系数\n",
    "    print(\"初始化的学习率：\", optimizer.defaults['lr'])\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "    for epoch in range(15):\n",
    "        \n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        #model = torch.load(\"/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/best_point=0.8127543174426041.pth\")\n",
    "        losses = AverageMeter()\n",
    "        #scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "        print(\"第%d个epoch的学习率：%f\" % (epoch, optimizer.param_groups[0]['lr']))\n",
    "        step = 0\n",
    "        prebig = True\n",
    "        for batch_text,batch_ids,batch_offset,batch_token_type_ids,batch_attention_mask,batch_label in tqdm(train_loader):\n",
    "            batch_ids = batch_ids.to(device)\n",
    "            batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            batch_label = batch_label.to(device)\n",
    "            loss = compute_multilabel_loss(model,batch_ids,\\\n",
    "                                batch_token_type_ids,\\\n",
    "                                batch_attention_mask,\\\n",
    "                                batch_label)\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss/gradient_accumulation_steps\n",
    "            losses.update(loss.item(),batch_size)\n",
    "            scaler.scale(loss).backward()\n",
    "            #loss.backward()\n",
    "            #每一次进行相应的梯度计算\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
    "            #防止梯度爆炸，超过1000的部分不予计算\n",
    "            if (step+1)%gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            step = step+1\n",
    "        scheduler.step()\n",
    "        model.eval()\n",
    "        \n",
    "        pred_result = []\n",
    "        label_result = []\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        for batch_text,batch_ids,batch_offset,batch_token_type_ids,batch_attention_mask,batch_origin_label in tqdm(valid_loader):\n",
    "            batch_ids = batch_ids.to(device)\n",
    "            batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                logit = model(input_ids=batch_ids,token_type_ids=batch_token_type_ids,\\\n",
    "                              attention_mask=batch_attention_mask)\n",
    "                #logit = model(input_ids=batch_ids)\n",
    "                #logit = (4,512,1)\n",
    "                #!!!这点判断需要注意应该是以字符的形式进行判断!!!\n",
    "                #输入的id应该为text+symptom，但是判断正负的时候只应该判断text的内容\n",
    "                #torch.set_printoptions(threshold=np.inf)\n",
    "                logit = torch.sigmoid(logit)\n",
    "                #加上symptom得到的正常的logit\n",
    "                preds = get_char_probs(batch_text,batch_offset,\\\n",
    "                                       logit.cpu())\n",
    "                results = get_results(batch_text,preds,th=0.5)\n",
    "                preds = get_predictions(results)\n",
    "                truths = batch_origin_label\n",
    "                r\"\"\"\n",
    "                preds = \n",
    "                [[[696, 722]], [[668, 693]], [[203, 217]], [[70, 91]]]\n",
    "                truths = \n",
    "                [[[696, 724]], [[668, 693]], [[203, 217]], [[70, 91], [176, 183]]]\n",
    "                \"\"\"\n",
    "                for data in preds:\n",
    "                    pred_result.append(data)\n",
    "                for data in truths:\n",
    "                    label_result.append(data)\n",
    "\n",
    "\n",
    "        point = get_score(pred_result,label_result)\n",
    "        point = round(point, 6)\n",
    "        print('point = ')\n",
    "        print(point)\n",
    "        \n",
    "        if point > bestpoint:\n",
    "            bestpoint = point \n",
    "            prebig = 0\n",
    "            newpath = '/media/xiaoguzai/WD_BLACK/deberta-v3-base/deberta_base_best_point='+str(bestpoint)+'_fold='+str(current_fold)+'.pth'\n",
    "            torch.save(model,newpath)\n",
    "            if predpath != None and os.path.exists(predpath):\n",
    "                os.remove(predpath)\n",
    "            predpath = newpath\n",
    "        else:\n",
    "            prebig = prebig+1\n",
    "\n",
    "        if prebig >= 3 and point < bestpoint:\n",
    "        #连续三个回合小于内容，跳出\n",
    "            break\n",
    "        bestpointlist[current_fold] = max(bestpointlist[current_fold],bestpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
