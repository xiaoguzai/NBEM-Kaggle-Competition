{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717a05ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 42146/42146 [00:11<00:00, 3722.74it/s]\n",
      "100%|██████████████████████████████████████| 143/143 [00:00<00:00, 23388.92it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************current_fold = 0************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/11440 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|███████████████████████████████████| 11440/11440 [00:10<00:00, 1061.31it/s]\n",
      "100%|█████████████████████████████████████| 2860/2860 [00:02<00:00, 1053.71it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/roberta-english-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2860 [00:00<?, ?it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:57<00:00,  6.84it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8030794610943086\n",
      "epoch = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:32<00:00,  7.28it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8453710339435794\n",
      "epoch = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:32<00:00,  7.29it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8560525976537321\n",
      "epoch = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:32<00:00,  7.29it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8613307618129218\n",
      "epoch = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:11<00:00,  6.62it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:48<00:00, 14.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8600870045042155\n",
      "epoch = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████▍                              | 678/2860 [01:32<04:55,  7.38it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8642098019726279\n",
      "epoch = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|█████████████████████████▉             | 1903/2860 [04:20<02:06,  7.59it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:32<00:00,  7.29it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8562539217736874\n",
      "epoch = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|████████████████████████████████████▌  | 2678/2860 [06:07<00:24,  7.43it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8542216358839051\n",
      "epoch = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.867418170536222\n",
      "epoch = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.870809107906667\n",
      "epoch = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.864688543918255\n",
      "epoch = 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8654290515702133\n",
      "epoch = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████▉           | 2050/2860 [04:40<01:49,  7.42it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.31it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8613333333333333\n",
      "epoch = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.31it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8691989684175059\n",
      "epoch = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:30<00:00,  7.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.870991196863071\n",
      "epoch = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|████████████████████████               | 1762/2860 [04:00<02:26,  7.50it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:30<00:00,  7.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.861260088518615\n",
      "epoch = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:30<00:00,  7.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8684446493992263\n",
      "epoch = 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:29<00:00,  7.34it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8629284307288247\n",
      "epoch = 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:29<00:00,  7.34it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8660429757989172\n",
      "epoch = 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:28<00:00,  7.35it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8722164766256038\n",
      "epoch = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████▏       | 2289/2860 [05:10<01:18,  7.27it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:28<00:00,  7.36it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8686951033503394\n",
      "epoch = 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:28<00:00,  7.37it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8710756255865908\n",
      "epoch = 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:27<00:00,  7.39it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8673821892882572\n",
      "epoch = 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████████████████████████▏            | 1919/2860 [04:19<02:01,  7.76it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:27<00:00,  7.38it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8702306024914471\n",
      "epoch = 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:27<00:00,  7.39it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8647243527049983\n",
      "epoch = 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:26<00:00,  7.41it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8704649605420381\n",
      "epoch = 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:25<00:00,  7.42it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8691645951820086\n",
      "epoch = 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|██████████████▎                        | 1050/2860 [02:21<03:57,  7.62it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:26<00:00,  7.41it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8644748221869394\n",
      "epoch = 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:26<00:00,  7.41it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8700348882284532\n",
      "epoch = 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8700780796283152\n",
      "epoch = 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8739236669149255\n",
      "epoch = 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8730203765798298\n",
      "epoch = 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8693595342066959\n",
      "epoch = 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8722617725818038\n",
      "epoch = 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8709531852348474\n",
      "epoch = 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████▎                                | 527/2860 [01:10<05:07,  7.58it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8667201615199441\n",
      "epoch = 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████▎                     | 1274/2860 [02:51<03:27,  7.65it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:34<00:00,  7.25it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8723133300413234\n",
      "epoch = 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████████████████▉                      | 1241/2860 [02:46<03:42,  7.29it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8764105944855936\n",
      "epoch = 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████████████████████████▎           | 2007/2860 [04:28<01:49,  7.80it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:22<00:00,  7.47it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:45<00:00, 15.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8744380009122305\n",
      "epoch = 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▋                                     | 192/2860 [00:25<06:10,  7.20it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8734715644323681\n",
      "epoch = 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:25<00:00,  7.41it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:48<00:00, 14.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8754002576957914\n",
      "epoch = 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▉                                      | 138/2860 [00:19<06:24,  7.08it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.31it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:48<00:00, 14.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8774939588314926\n",
      "epoch = 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|█████████████████████████▋             | 1884/2860 [04:26<02:19,  7.01it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:46<00:00,  7.03it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:47<00:00, 15.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8753303157949818\n",
      "epoch = 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:39<00:00,  7.15it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:47<00:00, 15.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8751550808978704\n",
      "epoch = 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:39<00:00,  7.16it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:47<00:00, 15.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8764530744336569\n",
      "epoch = 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:43<00:00,  7.09it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:47<00:00, 15.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8763862382008563\n",
      "epoch = 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:42<00:00,  7.11it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8740363983151872\n",
      "epoch = 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:40<00:00,  7.14it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:47<00:00, 15.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8730782613178005\n",
      "epoch = 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:01<00:00,  6.79it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:48<00:00, 14.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.875298601955435\n",
      "epoch = 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████▌                                | 542/2860 [01:19<05:34,  6.93it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:02<00:00,  6.78it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:48<00:00, 14.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8707319566550809\n",
      "************current_fold = 1************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 11440/11440 [00:11<00:00, 994.52it/s]\n",
      "100%|██████████████████████████████████████| 2860/2860 [00:02<00:00, 991.16it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/roberta-english-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2860 [00:00<?, ?it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:00<00:00,  6.80it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:47<00:00, 15.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8332109920322447\n",
      "epoch = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:12<00:00,  6.61it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.848178382667402\n",
      "epoch = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▍                                      | 104/2860 [00:15<07:12,  6.38it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:13<00:00,  6.59it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8620015938713077\n",
      "epoch = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████▋                         | 1007/2860 [02:32<04:30,  6.86it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:13<00:00,  6.60it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8634203803952192\n",
      "epoch = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▌                                     | 183/2860 [00:27<06:30,  6.85it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:13<00:00,  6.60it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8602733013937283\n",
      "epoch = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████▏                              | 660/2860 [01:40<05:47,  6.34it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:13<00:00,  6.60it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8698535000131037\n",
      "epoch = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:13<00:00,  6.60it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8715189375049214\n",
      "epoch = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:13<00:00,  6.60it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8660401840128966\n",
      "epoch = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:13<00:00,  6.60it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8704622718924622\n",
      "epoch = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:13<00:00,  6.60it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8725140302230241\n",
      "epoch = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████▌                               | 610/2860 [01:32<05:29,  6.84it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:12<00:00,  6.61it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8599423927579467\n",
      "epoch = 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████████████████████████████▋      | 2401/2860 [06:03<01:10,  6.52it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:13<00:00,  6.61it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8637788874695158\n",
      "epoch = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:12<00:00,  6.61it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8679255305022175\n",
      "epoch = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:12<00:00,  6.61it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8682487832395346\n",
      "epoch = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:12<00:00,  6.62it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8710641767286563\n",
      "epoch = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|████████████████████████████████████▉  | 2708/2860 [06:49<00:23,  6.34it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:11<00:00,  6.62it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8703262955854127\n",
      "epoch = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████▉              | 1827/2860 [04:35<02:29,  6.90it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:11<00:00,  6.62it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.862676788685524\n",
      "epoch = 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|███████████████████████████████████▌   | 2611/2860 [06:34<00:35,  6.92it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:11<00:00,  6.63it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8620316622691293\n",
      "epoch = 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:11<00:00,  6.63it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8751345114156436\n",
      "epoch = 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:10<00:00,  6.65it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8728357901777675\n",
      "epoch = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:09<00:00,  6.65it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8667150108774474\n",
      "epoch = 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|██████████████████▎                    | 1347/2860 [03:22<03:38,  6.92it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:09<00:00,  6.65it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8620991764499736\n",
      "epoch = 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████                             | 787/2860 [01:58<04:57,  6.96it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:09<00:00,  6.66it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8680043643718232\n",
      "epoch = 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:08<00:00,  6.67it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8748559153306088\n",
      "epoch = 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:07<00:00,  6.69it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8733383431370008\n",
      "epoch = 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▊                                | 560/2860 [01:23<05:55,  6.47it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:07<00:00,  6.69it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8721003051258478\n",
      "epoch = 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:07<00:00,  6.69it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8747362765525284\n",
      "epoch = 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:06<00:00,  6.70it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8759027887606095\n",
      "epoch = 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:06<00:00,  6.71it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8738070825774904\n",
      "epoch = 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:06<00:00,  6.71it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.868319647080709\n",
      "epoch = 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:05<00:00,  6.72it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8757555227885598\n",
      "epoch = 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:05<00:00,  6.72it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8744951144407267\n",
      "epoch = 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:05<00:00,  6.72it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8742759976619374\n",
      "epoch = 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:05<00:00,  6.72it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8732740660222316\n",
      "epoch = 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████▎         | 2145/2860 [05:18<01:46,  6.69it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:05<00:00,  6.72it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8710210404027365\n",
      "epoch = 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:05<00:00,  6.72it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8741006246211422\n",
      "epoch = 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:05<00:00,  6.73it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8718831065699614\n",
      "epoch = 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:04<00:00,  6.73it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.873633145404484\n",
      "epoch = 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:04<00:00,  6.74it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8754789272030652\n",
      "epoch = 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:04<00:00,  6.73it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8756450894611375\n",
      "epoch = 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|████████████████████▍                  | 1497/2860 [03:41<03:26,  6.61it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:04<00:00,  6.74it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8753312893000053\n",
      "epoch = 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:04<00:00,  6.73it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8741078248044457\n",
      "epoch = 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:03<00:00,  6.75it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8779441013294254\n",
      "epoch = 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████▏                                  | 369/2860 [00:54<06:15,  6.63it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:04<00:00,  6.73it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8787988609888686\n",
      "epoch = 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|████████████████████▉                  | 1538/2860 [03:47<03:12,  6.88it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:03<00:00,  6.75it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8751394265468145\n",
      "epoch = 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████▋                         | 1003/2860 [02:28<04:25,  6.99it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:03<00:00,  6.75it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8713619722390498\n",
      "epoch = 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:03<00:00,  6.75it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8730364415295923\n",
      "epoch = 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:03<00:00,  6.76it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8772165219679032\n",
      "epoch = 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:03<00:00,  6.76it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.87597051468374\n",
      "epoch = 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|███████████████████████████████▌       | 2313/2860 [05:42<01:23,  6.56it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:03<00:00,  6.75it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.870072145504425\n",
      "************current_fold = 2************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 11440/11440 [00:11<00:00, 962.44it/s]\n",
      "100%|█████████████████████████████████████| 2860/2860 [00:02<00:00, 1052.97it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/roberta-english-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2860 [00:00<?, ?it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:14<00:00,  6.58it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.826334818345543\n",
      "epoch = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:14<00:00,  6.58it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8451848620351184\n",
      "epoch = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:14<00:00,  6.58it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8524845129827336\n",
      "epoch = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:14<00:00,  6.58it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8583653033357175\n",
      "epoch = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:14<00:00,  6.58it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8629065997280618\n",
      "epoch = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▋                                      | 120/2860 [00:18<07:10,  6.37it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:14<00:00,  6.59it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8620484882910331\n",
      "epoch = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████████████████                       | 1175/2860 [02:58<04:07,  6.81it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:14<00:00,  6.58it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8458259232137253\n",
      "epoch = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:14<00:00,  6.58it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8589339210536735\n",
      "epoch = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:14<00:00,  6.59it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8646567981606345\n",
      "epoch = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████▏                            | 796/2860 [02:00<05:24,  6.35it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:13<00:00,  6.59it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8600181538340108\n",
      "epoch = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:13<00:00,  6.59it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8619597867561497\n",
      "epoch = 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▋                                  | 405/2860 [01:01<06:18,  6.49it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:13<00:00,  6.60it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8645182151325717\n",
      "epoch = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|███████████████████████                | 1695/2860 [04:16<02:50,  6.84it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:13<00:00,  6.60it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8586738976047518\n",
      "epoch = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████▌                 | 1577/2860 [03:58<03:17,  6.50it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:12<00:00,  6.61it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8635796715199097\n",
      "epoch = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████████████████████████████████▍ | 2744/2860 [06:54<00:18,  6.33it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:12<00:00,  6.61it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8618302072030176\n",
      "epoch = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:12<00:00,  6.62it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8662557740179178\n",
      "epoch = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:11<00:00,  6.63it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8610345386735851\n",
      "epoch = 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▌                                 | 469/2860 [01:10<06:00,  6.63it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:11<00:00,  6.64it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8611107502435856\n",
      "epoch = 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:12<00:00,  6.62it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8614330014689081\n",
      "epoch = 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:08<00:00,  6.67it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8684635120480335\n",
      "epoch = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:08<00:00,  6.67it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8663803516306535\n",
      "epoch = 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████▊                        | 1085/2860 [02:42<04:29,  6.58it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:08<00:00,  6.68it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8658649647295936\n",
      "epoch = 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:08<00:00,  6.68it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8651937913178335\n",
      "epoch = 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:07<00:00,  6.70it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8704328494834576\n",
      "epoch = 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:07<00:00,  6.68it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8654032659694371\n",
      "epoch = 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:08<00:00,  6.67it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8702044866940012\n",
      "epoch = 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:08<00:00,  6.67it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8681618973962362\n",
      "epoch = 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:07<00:00,  6.69it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8732301526616921\n",
      "epoch = 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:29<00:00,  7.35it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8639579054752413\n",
      "epoch = 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████        | 2275/2860 [05:07<01:16,  7.69it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:25<00:00,  7.41it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8650220050874148\n",
      "epoch = 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:26<00:00,  7.41it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8693825790599984\n",
      "epoch = 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████████████████████████████████▎    | 2514/2860 [05:38<00:46,  7.42it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:25<00:00,  7.42it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8688709399328903\n",
      "epoch = 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████▏         | 2143/2860 [04:48<01:33,  7.70it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:25<00:00,  7.42it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8661262522891306\n",
      "epoch = 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:25<00:00,  7.42it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8724778046811945\n",
      "epoch = 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8717894542237247\n",
      "epoch = 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8711512978408293\n",
      "epoch = 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8679235415631619\n",
      "epoch = 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8656855439642325\n",
      "epoch = 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8683321184812139\n",
      "epoch = 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.866464260475829\n",
      "epoch = 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8701939302681794\n",
      "epoch = 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████▊                       | 1160/2860 [02:35<03:55,  7.23it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8656511885310705\n",
      "epoch = 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████▉                        | 1094/2860 [02:27<03:53,  7.56it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8674980924566529\n",
      "epoch = 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8711823207307989\n",
      "epoch = 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.47it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8694921824525131\n",
      "epoch = 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████▊                     | 1303/2860 [02:54<03:23,  7.65it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8649224135636528\n",
      "epoch = 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8709377901578459\n",
      "epoch = 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:22<00:00,  7.48it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8701002674677525\n",
      "epoch = 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:22<00:00,  7.49it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8697273424483642\n",
      "epoch = 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:22<00:00,  7.48it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8651834699435578\n",
      "************current_fold = 3************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 11440/11440 [00:11<00:00, 1024.16it/s]\n",
      "100%|█████████████████████████████████████| 2860/2860 [00:02<00:00, 1006.59it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/roberta-english-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2860 [00:00<?, ?it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:33<00:00,  7.27it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.7835000073424674\n",
      "epoch = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|███████████▋                            | 838/2860 [01:55<04:31,  7.44it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:32<00:00,  7.29it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8411580116973345\n",
      "epoch = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:32<00:00,  7.28it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.863462724969479\n",
      "epoch = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:32<00:00,  7.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8661926981114909\n",
      "epoch = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███████████████▍                       | 1129/2860 [02:34<04:05,  7.06it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8664407988587732\n",
      "epoch = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|████████████████████▊                  | 1526/2860 [03:45<03:06,  7.15it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:04<00:00,  6.73it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8716711466596735\n",
      "epoch = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8654518778573955\n",
      "epoch = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████▉                     | 1311/2860 [02:59<03:23,  7.61it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8592468834340059\n",
      "epoch = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|██████████████████████████████████▉    | 2558/2860 [05:49<00:40,  7.46it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.31it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8619671835324749\n",
      "epoch = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████                            | 859/2860 [01:57<04:23,  7.61it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.31it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8665082247383037\n",
      "epoch = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.31it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8741820997612442\n",
      "epoch = 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|██████████████████████▋                | 1667/2860 [03:47<02:37,  7.58it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:30<00:00,  7.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8709433371595534\n",
      "epoch = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:30<00:00,  7.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8802728969556542\n",
      "epoch = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|████████████████████████▎              | 1781/2860 [04:02<02:30,  7.18it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:30<00:00,  7.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.869500059156818\n",
      "epoch = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████▍                              | 675/2860 [01:32<04:47,  7.61it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:30<00:00,  7.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8714226094874357\n",
      "epoch = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|████████████████████████████████████▏  | 2656/2860 [06:02<00:28,  7.11it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:29<00:00,  7.34it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.876463175399166\n",
      "epoch = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:40<00:00,  7.13it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:47<00:00, 15.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8694215296226151\n",
      "epoch = 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████████████▌             | 1871/2860 [04:41<02:20,  7.01it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:10<00:00,  6.65it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8697826454998894\n",
      "epoch = 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|███████████████████▉                   | 1463/2860 [03:40<03:19,  7.00it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:10<00:00,  6.65it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8651395512469706\n",
      "epoch = 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▌                                 | 465/2860 [01:10<06:07,  6.52it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:09<00:00,  6.65it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.873244284409378\n",
      "epoch = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▎                                   | 305/2860 [00:45<06:23,  6.66it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:09<00:00,  6.67it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8788157410812798\n",
      "epoch = 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|████████▎                               | 594/2860 [01:28<05:36,  6.74it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:08<00:00,  6.67it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8736490367044327\n",
      "epoch = 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:52<00:00,  6.94it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:47<00:00, 15.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8744668527197093\n",
      "epoch = 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:07<00:00,  6.69it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8748006276177757\n",
      "epoch = 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████████████████▏                      | 1188/2860 [02:57<04:18,  6.47it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [07:07<00:00,  6.69it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:48<00:00, 14.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8650255985933701\n",
      "epoch = 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:07<00:00,  6.70it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8675136116152451\n",
      "epoch = 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [07:06<00:00,  6.70it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:49<00:00, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8753323788095725\n",
      "epoch = 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|█████████████████████████████████▏     | 2431/2860 [06:01<00:55,  7.75it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:59<00:00,  6.82it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8592154242626706\n",
      "epoch = 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████████████████▋                      | 1222/2860 [02:44<03:36,  7.56it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:26<00:00,  7.41it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8700377948651113\n",
      "epoch = 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▍                                 | 464/2860 [01:02<05:36,  7.12it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:25<00:00,  7.41it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8734208765692848\n",
      "epoch = 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████▊              | 1821/2860 [04:04<02:22,  7.29it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.43it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8731847968545215\n",
      "epoch = 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|█████████████████████████████▉         | 2198/2860 [04:55<01:27,  7.55it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.43it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8653260207190736\n",
      "epoch = 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|█████████████████████████▊             | 1890/2860 [04:14<02:07,  7.60it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:25<00:00,  7.43it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8636194245621248\n",
      "epoch = 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████                                    | 287/2860 [00:38<05:35,  7.66it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:25<00:00,  7.41it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8696578144276347\n",
      "epoch = 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▋                                        | 51/2860 [00:06<06:13,  7.52it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.43it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8721268266211869\n",
      "epoch = 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8743721222268732\n",
      "epoch = 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8731576366123744\n",
      "epoch = 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|██████████████████████████████████████▎| 2811/2860 [06:16<00:06,  7.80it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:22<00:00,  7.47it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.874368315900811\n",
      "epoch = 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████▎                 | 1565/2860 [03:29<02:55,  7.39it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8696439575918403\n",
      "epoch = 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|████████████████████▏                  | 1476/2860 [03:18<03:13,  7.15it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8734687446660452\n",
      "epoch = 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|██████████████████████████████████████▋| 2836/2860 [06:20<00:03,  7.08it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8757796999208953\n",
      "epoch = 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|████████████████████████▎              | 1784/2860 [03:58<02:28,  7.26it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8717518860016763\n",
      "epoch = 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|█████████████████████████████████▏     | 2438/2860 [05:27<00:55,  7.62it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8723434714142558\n",
      "epoch = 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8681218617477952\n",
      "epoch = 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████▍      | 2381/2860 [05:19<01:05,  7.29it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8602162003652212\n",
      "epoch = 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8715537232671992\n",
      "epoch = 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8745030496885561\n",
      "epoch = 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|█████████████████████████████████▉     | 2490/2860 [05:33<00:49,  7.55it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8679307669807842\n",
      "epoch = 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█████▏                                  | 368/2860 [00:49<05:43,  7.26it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8707621584913545\n",
      "epoch = 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████▌                                | 542/2860 [01:12<05:05,  7.60it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8761681641479803\n",
      "************current_fold = 4************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 11440/11440 [00:11<00:00, 996.06it/s]\n",
      "100%|█████████████████████████████████████| 2860/2860 [00:02<00:00, 1087.15it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/roberta-english-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2860 [00:00<?, ?it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:33<00:00,  7.28it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8165974893060685\n",
      "epoch = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:33<00:00,  7.28it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8504141585635643\n",
      "epoch = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████▍                       | 1135/2860 [02:35<03:48,  7.56it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:32<00:00,  7.28it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8600810862989896\n",
      "epoch = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|██████████████▉                        | 1094/2860 [02:30<03:58,  7.41it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:32<00:00,  7.29it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8665396570741001\n",
      "epoch = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|████████████▎                           | 884/2860 [02:01<04:39,  7.06it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:32<00:00,  7.29it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8672234576201066\n",
      "epoch = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:32<00:00,  7.29it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8723879240972926\n",
      "epoch = 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:32<00:00,  7.29it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8658661117071468\n",
      "epoch = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:32<00:00,  7.28it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8689660462262221\n",
      "epoch = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:32<00:00,  7.29it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8609762053341292\n",
      "epoch = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▍                                  | 388/2860 [00:53<05:50,  7.05it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:32<00:00,  7.29it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8710667396757885\n",
      "epoch = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████▉                             | 786/2860 [01:47<04:54,  7.04it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8715503088108826\n",
      "epoch = 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████                                | 574/2860 [01:18<05:08,  7.42it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.30it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8744539266483795\n",
      "epoch = 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████▎                              | 664/2860 [01:30<05:11,  7.06it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.31it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8740359897172236\n",
      "epoch = 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|██████████████████████▍                | 1644/2860 [03:44<02:52,  7.05it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:31<00:00,  7.31it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8748696951809196\n",
      "epoch = 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:30<00:00,  7.32it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8796597448086064\n",
      "epoch = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:30<00:00,  7.33it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8816816974732469\n",
      "epoch = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▍                                  | 391/2860 [00:53<05:24,  7.61it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:41<00:00,  7.13it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8757527653891704\n",
      "epoch = 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████████████████████████▍            | 1941/2860 [04:24<02:07,  7.24it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:29<00:00,  7.34it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8715102417514804\n",
      "epoch = 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|██████▎                                 | 452/2860 [01:01<05:43,  7.01it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:29<00:00,  7.34it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8784310727119811\n",
      "epoch = 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|█████████████████████████████████▊     | 2479/2860 [05:36<00:49,  7.74it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:28<00:00,  7.36it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.880370263839134\n",
      "epoch = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:28<00:00,  7.37it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8814959717896579\n",
      "epoch = 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████▊              | 1817/2860 [04:06<02:23,  7.27it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:27<00:00,  7.38it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8771314904122907\n",
      "epoch = 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:28<00:00,  7.37it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8846138746024891\n",
      "epoch = 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:27<00:00,  7.38it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8809576245935407\n",
      "epoch = 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:26<00:00,  7.40it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8830110853312709\n",
      "epoch = 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████▍                             | 749/2860 [01:41<04:51,  7.25it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:27<00:00,  7.38it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8727090584503154\n",
      "epoch = 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████▏                             | 724/2860 [01:38<05:01,  7.08it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:27<00:00,  7.38it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.877371386230949\n",
      "epoch = 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:26<00:00,  7.40it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.882244851083378\n",
      "epoch = 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:25<00:00,  7.42it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8826828637890928\n",
      "epoch = 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:25<00:00,  7.43it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8822038853875641\n",
      "epoch = 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:25<00:00,  7.42it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8815158605346938\n",
      "epoch = 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:25<00:00,  7.42it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8841586839085646\n",
      "epoch = 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.43it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.879067733388213\n",
      "epoch = 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████████████████▎                      | 1199/2860 [02:41<03:36,  7.68it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:25<00:00,  7.42it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8731894922104165\n",
      "epoch = 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:25<00:00,  7.41it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8840727606704594\n",
      "epoch = 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8852365816471561\n",
      "epoch = 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███▏                                    | 231/2860 [00:30<05:41,  7.70it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:38<00:00,  7.17it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8858085808580858\n",
      "epoch = 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████████████████████████▎            | 1934/2860 [04:19<02:02,  7.55it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8813431280608524\n",
      "epoch = 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8829491664951635\n",
      "epoch = 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8841888323565402\n",
      "epoch = 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8818942148335625\n",
      "epoch = 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8810422282120395\n",
      "epoch = 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8842899640700056\n",
      "epoch = 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████▍                       | 1131/2860 [02:31<03:41,  7.80it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8729658858512226\n",
      "epoch = 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|██████████████████████████████████▎    | 2517/2860 [05:38<00:46,  7.38it/s]/tmp/ipykernel_101321/1058046846.py:686: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
      "100%|███████████████████████████████████████| 2860/2860 [06:24<00:00,  7.44it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.880990789405214\n",
      "epoch = 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.45it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8813023469270681\n",
      "epoch = 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.882631287149138\n",
      "epoch = 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8840504219189499\n",
      "epoch = 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.47it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8821978021978022\n",
      "epoch = 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2860/2860 [06:23<00:00,  7.46it/s]\n",
      "100%|█████████████████████████████████████████| 715/715 [00:46<00:00, 15.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8853265981985232\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,DebertaV2Tokenizer\n",
    "from transformers import RobertaModel,RobertaTokenizerFast\n",
    "tokenizer = AutoTokenizer.from_pretrained('/home/xiaoguzai/模型/roberta-english-large',\\\n",
    "                                         trim_offsets=False)\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "train = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/train.csv')\n",
    "\n",
    "train['annotation'] = train['annotation'].apply(ast.literal_eval)\n",
    "train['location'] = train['location'].apply(ast.literal_eval)\n",
    "features = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/features.csv')\n",
    "features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "\n",
    "patient_notes = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/patient_notes.csv')\n",
    "train = train.merge(features, on=['feature_num','case_num'],how='left')\n",
    "train.head()\n",
    "\n",
    "train = train.merge(patient_notes, on=['pn_num','case_num'],how='left')\n",
    "train.head()\n",
    "\n",
    "import re\n",
    "def process_feature_text(text):\n",
    "    text = re.sub('I-year', '1-year', text)\n",
    "    text = re.sub('-OR-', \" or \", text)\n",
    "    text = re.sub('-', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_spaces(txt):\n",
    "    txt = re.sub('\\n', ' ', txt)\n",
    "    txt = re.sub('\\t', ' ', txt)\n",
    "    txt = re.sub('\\r', ' ', txt)\n",
    "#    txt = re.sub(r'\\s+', ' ', txt)\n",
    "    return txt\n",
    "train[\"feature_text\"] = train[\"feature_text\"].apply(process_feature_text)\n",
    "train[\"pn_history\"] = train[\"pn_history\"].apply(clean_spaces)\n",
    "train[\"feature_text\"] = train[\"feature_text\"].apply(clean_spaces)\n",
    "#!!!不加capitalize()得分0.852(maxlen=310),加capitalize()得分0.861\n",
    "#测试maxlen=466+capitalize=0.670,maxlen=350+capitalize()=0.8693\n",
    "train[\"pn_history\"] = train[\"pn_history\"].apply(lambda x:x.capitalize())\n",
    "\n",
    "import ast\n",
    "# incorrect annotation\n",
    "train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=42)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "skf = StratifiedKFold(n_splits=5,random_state=42,shuffle=True)\n",
    "#splits = list(skf.split(train,train[]))\n",
    "for n, (train_index,val_index) in enumerate(skf.split(X=train,y=train['case_num'],\\\n",
    "                                                     groups=train['pn_num'])):\n",
    "    train.loc[val_index,'fold'] = int(n)\n",
    "#按照groups也就是train['pn_num']以及train['location']进行划分\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "train['annotation_length'] = train['annotation'].apply(len)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "pn_history_lengths = []\n",
    "for text in tqdm(patient_notes['pn_history'].fillna(\"\").values,total=len(patient_notes)):\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    pn_history_lengths.append(length)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "features_lengths = []\n",
    "result_lengths = []\n",
    "i = 0\n",
    "for text in tqdm(features['feature_text'].fillna(\"\").values, total=len(features)):\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    features_lengths.append(length)\n",
    "    result_lengths.append(pn_history_lengths[i]+length+3)\n",
    "    i = i+1\n",
    "max_len = max(pn_history_lengths)+max(features_lengths)+3\n",
    "a = 100\n",
    "bins = int((max(result_lengths)-min(result_lengths))/a)\n",
    "plt.hist(result_lengths,bins,density=1,stacked=True)\n",
    "plt.grid(True,linestyle='--',alpha=0.5)\n",
    "plt.show()\n",
    "max_len = 354\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,inputs,labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.tensors = [torch.tensor(self.inputs),\\\n",
    "                       torch.tensor(self.labels)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "#下面prepare_input的时候将text和feature_text合在一起很巧妙\n",
    "\n",
    "def prepare_input(ids, text, feature_text):\n",
    "    #inputs1 = tokenizer.encode_plus(text,\\\n",
    "    #                               add_special_tokens=True,\\\n",
    "    #                               max_length = max_len,\\\n",
    "    #                               paddin)\n",
    "    inputs = {}\n",
    "    inputs1 = tokenizer.encode_plus(text,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   max_length = max_len,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs2 = tokenizer.encode_plus(feature_text,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   max_length = max_len,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    ids1,ids2 = ids.split('_')\n",
    "    inputs3 = tokenizer.encode_plus(ids1,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs4 = tokenizer.encode_plus(ids2,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs2['input_ids'] = inputs2['input_ids']+inputs3['input_ids'][1:]+inputs4['input_ids'][1:]\n",
    "    inputs2['attention_mask'] = inputs2['attention_mask']+inputs3['attention_mask'][1:]+inputs4['attention_mask'][1:]\n",
    "    #inputs2['token_type_ids'] = inputs2['token_type_ids']+inputs3['token_type_ids'][1:]+inputs4['token_type_ids'][1:]\n",
    "    \n",
    "    if len(inputs1['input_ids'])+len(inputs2['input_ids'])-2 > max_len:\n",
    "        exceed_length = len(inputs1['input_ids'])+len(inputs2['input_ids'])-max_len-2\n",
    "        inputs['input_ids'] = inputs1['input_ids'][:len(inputs1['input_ids'])-exceed_length-1]+inputs2['input_ids'][1:]\n",
    "        inputs['attention_mask'] = [1]*max_len\n",
    "    else:\n",
    "        inputs['input_ids'] = inputs1['input_ids'][:-1]+inputs2['input_ids'][1:-1]\n",
    "        inputs['input_ids'] = inputs['input_ids']+[tokenizer.sep_token_id]\n",
    "        #inputs['input_ids'] = inputs['input_ids']+[tokenizer.sep_token_id]+[tokenizer.pad_token_id]*(max_len-len(inputs['input_ids'])-1)\n",
    "        inputs['attention_mask'] = [1]*(len(inputs['input_ids']))\n",
    "        inputs['attention_mask'] = inputs['attention_mask']+[0]*(max_len-len(inputs['input_ids']))\n",
    "        inputs['input_ids'] = inputs['input_ids']+[tokenizer.pad_token_id]*(max_len-len(inputs['input_ids']))\n",
    "        #inputs['token_type_ids'] = [0]*max_len\n",
    "        #inputs['attention_mask'] = [1]*max_len\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v,dtype=torch.long)\n",
    "    return inputs,len(inputs['input_ids'])\n",
    "\n",
    "#打标记的时候还是只放入text的内容，不考虑feature_text的文本内容\n",
    "def create_label(text, annotation_length, location_list):\n",
    "    encoded = tokenizer.encode_plus(text,\\\n",
    "                                add_special_tokens=True,\\\n",
    "                                max_length = max_len,\\\n",
    "                                padding = \"max_length\",\\\n",
    "                                return_offsets_mapping = True,\\\n",
    "                                truncation=True)\n",
    "    offset_mapping = encoded['offset_mapping']\n",
    "    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "    label = np.zeros(len(offset_mapping))\n",
    "    label[ignore_idxes] = -1\n",
    "    if annotation_length != 0:\n",
    "        for location in location_list:\n",
    "            #location = 2 4,location = 8 10\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                #loc = ['2','4'],loc = ['8','10']\n",
    "                start_idx = -1\n",
    "                end_idx = -1\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                #start = 2,end = 4;start = 8,end = 10;\n",
    "                #!!!这里的start,end标记的为字符:Character spans indicating \n",
    "                #the location of each annotation within the note.\n",
    "                #注意前面的标注Character spans\n",
    "                for idx in range(len(offset_mapping)):\n",
    "                    #if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                        #start_idx还能往前去\n",
    "                        start_idx = idx - 1\n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                        #end_idx还能往后去\n",
    "                        end_idx = idx+1\n",
    "                        #end_idx = idx\n",
    "                if start_idx == -1:\n",
    "                    start_idx = end_idx\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    label[start_idx:end_idx] = 1\n",
    "    return offset_mapping,label\n",
    "\n",
    "def change_location_to_offset(text,location_list):\n",
    "    results = np.zeros(len(text))\n",
    "    #以char为级别计算，应该对整个text计算len\n",
    "    for idx, offset_mapping in enumerate(location_list):\n",
    "        try:\n",
    "            start = (int)(offset_mapping[0])\n",
    "            end = (int)(offset_mapping[1])\n",
    "            results[start:end] = 1\n",
    "        except:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        self.fc1 = nn.Linear(1024,1)\n",
    "        \n",
    "    def forward(self,input_ids,attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids,\\\n",
    "                           attention_mask=attention_mask)\n",
    "        outputs = outputs[0]\n",
    "        #outputs = self.dropout(outputs)\n",
    "        outputs = self.fc1(outputs)\n",
    "        return outputs\n",
    "    \n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def my_collate(batch):\n",
    "    text_list,input_ids_list,offset_list = [],[],[]\n",
    "    attention_mask_list,origin_label_list = [],[]\n",
    "    for data in batch:\n",
    "        text_list.append(data[0])\n",
    "        input_ids_list.append(data[1].tolist())\n",
    "        offset_list.append(data[2])\n",
    "        attention_mask_list.append(data[3].tolist())\n",
    "        current_data_list = []\n",
    "        for data1 in data[4]:\n",
    "            if ' ' in data1:\n",
    "                for data2 in data1.split(';'):\n",
    "                    data3 = data2.split(' ')\n",
    "                    current_data_list.append([(int)(data3[0]),(int)(data3[1])])\n",
    "        origin_label_list.append(current_data_list)\n",
    "    input_ids_list = torch.tensor(input_ids_list)\n",
    "    attention_mask_list = torch.tensor(attention_mask_list)\n",
    "    return text_list,input_ids_list,offset_list,\\\n",
    "           attention_mask_list,origin_label_list\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,text,input_ids,offset,attention_mask,label):\n",
    "        self.input_ids = input_ids\n",
    "        self.tensors = [text,\\\n",
    "                        torch.tensor(input_ids,dtype=torch.long),\\\n",
    "                        torch.tensor(offset),\\\n",
    "                       torch.tensor(attention_mask,dtype=torch.long),\\\n",
    "                       torch.tensor(label)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "\n",
    "class ValidDataset(Dataset):\n",
    "    def __init__(self,text,input_ids,offset,attention_mask,origin_label):\n",
    "        self.input_ids = input_ids\n",
    "        self.tensors = [text,\\\n",
    "                        torch.tensor(input_ids,dtype=torch.long),\\\n",
    "                        torch.tensor(offset),\\\n",
    "                        torch.tensor(attention_mask,dtype=torch.long),\\\n",
    "                        origin_label]\n",
    "        #这里origin_label放入的为['']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "def compute_multilabel_loss(model,batch_token_ids,\\\n",
    "                            batch_attention_mask,\\\n",
    "                            batch_label):\n",
    "    with torch.cuda.amp.autocast(enabled=True):\n",
    "        logit = model(input_ids=batch_token_ids,\\\n",
    "                     attention_mask=batch_attention_mask)\n",
    "    logit = logit.view(-1,1)\n",
    "    batch_label = batch_label.view(-1,1)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    loss = loss_fn(logit,batch_label)\n",
    "    loss = torch.masked_select(loss,batch_label!=-1)\n",
    "    loss = loss.mean()\n",
    "    #这里的loss不要勿写成logit\n",
    "    return loss\n",
    "\n",
    "def get_char_probs(total_text,offsets,predictions,th=0.5):\n",
    "    results = [np.zeros(len(t)) for t in total_text]\n",
    "    #!!!results 长短不一!!!\n",
    "    #以char为级别计算，应该对整个text计算len\n",
    "    prepred = 0.0\n",
    "    torch.set_printoptions(threshold=np.inf)\n",
    "    for i, (offset, prediction) in enumerate(zip(offsets, predictions)):\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(offset, prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            if pred[0] > th and prepred < th and results[i][start] == ' ':\n",
    "            #当前标1,前面标0,并且当前打头的是空格\n",
    "                start = start+1\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred[0].item()\n",
    "            #prepred = pred[0].item()\n",
    "    return results\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0]\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)+1}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def post_process_spaces(target, text, th=0.5):\n",
    "    target = np.copy(target)\n",
    "\n",
    "    if text[0] == \" \":\n",
    "        target[0] = 0\n",
    "    if text[-1] == \" \":\n",
    "        target[-1] = 0\n",
    "\n",
    "    for i in range(1, len(text) - 1):\n",
    "        if text[i] == \" \":\n",
    "            if target[i] >= th and target[i - 1] < th:  # space before\n",
    "                target[i] = 0\n",
    "\n",
    "            if target[i] >= th and target[i + 1] < th:  # space after\n",
    "                target[i] = 0\n",
    "\n",
    "            if target[i - 1] >= th and target[i + 1] >= th:\n",
    "                target[i] = 1\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=42)\n",
    "\n",
    "\n",
    "from transformers import DebertaV2Model,DebertaModel\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from transformers import RobertaModel\n",
    "import itertools\n",
    "\n",
    "from transformers import DebertaV2Model,DebertaModel\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import itertools    \n",
    "from tqdm import tqdm\n",
    "\n",
    "bestpointlist = [0.0,0.0,0.0,0.0,0.0]\n",
    "for current_fold in range(5):\n",
    "    print('************current_fold = %d************'%current_fold)\n",
    "    train_data = train[train['fold'] != current_fold]\n",
    "    valid_data = train[train['fold'] == current_fold]\n",
    "    train_text,valid_text = [],[]\n",
    "    train_input_ids,train_token_type_ids,train_attention_mask = [],[],[]\n",
    "    train_offset,train_label = [],[]\n",
    "    train_length = []\n",
    "    valid_input_ids,valid_token_type_ids,valid_attention_mask = [],[],[]\n",
    "    valid_offset,valid_label = [],[]\n",
    "    valid_length = []\n",
    "    train_origin_label,valid_origin_label = [],[]\n",
    "\n",
    "    for  index,data  in  tqdm(train_data.iterrows(),total=len(train_data)):\n",
    "        #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "        ids = data['id']\n",
    "        text = data['pn_history']\n",
    "        feature_text = data['feature_text']\n",
    "        inputs,length = prepare_input(ids,text,feature_text)\n",
    "        #train_text.append(text+feature_text)\n",
    "        train_text.append(text)\n",
    "        train_input_ids.append(inputs['input_ids'].tolist())\n",
    "        train_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "\n",
    "        annotation_length = data['annotation_length']\n",
    "\n",
    "        current_offset,current_label = create_label(text,annotation_length=data['annotation_length'],\\\n",
    "                                        location_list=data['location'])\n",
    "        true_label = change_location_to_offset(text,data['location'])\n",
    "        train_origin_label.append(true_label)\n",
    "        train_offset.append(current_offset)\n",
    "        train_label.append(current_label)\n",
    "        train_length.append(length)\n",
    "\n",
    "    for index,data in tqdm(valid_data.iterrows(),total=len(valid_data)):\n",
    "        ids = data['id']\n",
    "        text = data['pn_history']\n",
    "        feature_text = data['feature_text']\n",
    "        inputs,length = prepare_input(ids,text,feature_text)\n",
    "        #valid_text.append(text+feature_text)\n",
    "        valid_text.append(text)\n",
    "        valid_input_ids.append(inputs['input_ids'].tolist())\n",
    "        valid_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "        annotation_length = data['annotation_length']\n",
    "        current_offset,current_label = create_label(text,annotation_length=data['annotation_length'],\\\n",
    "                                     location_list=data['location'])\n",
    "        #true_label = change_location_to_offset(text,data['location'])\n",
    "        #发生bug的地方，true_label的标记错误\n",
    "        valid_offset.append(current_offset)\n",
    "        valid_label.append(data['location'])\n",
    "        valid_length.append(length)\n",
    "\n",
    "    train_dataset = TrainDataset(train_text,\\\n",
    "                                 train_input_ids,\\\n",
    "                                 train_offset,\\\n",
    "                                train_attention_mask,\\\n",
    "                                train_label)\n",
    "    valid_dataset = ValidDataset(valid_text,\\\n",
    "                                 valid_input_ids,\\\n",
    "                                 valid_offset,\\\n",
    "                                valid_attention_mask,\\\n",
    "                                valid_label)\n",
    "\n",
    "    batch_size = 4\n",
    "    train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset,batch_size=batch_size,shuffle=False,collate_fn = my_collate)\n",
    "    #bcewithlogitloss有sigmoid函数,batch_size必须要大\n",
    "    bestpoint = 0.0\n",
    "    gradient_accumulation_steps = 4\n",
    "    #梯度累积的步数，每训练两次增加一步\n",
    "\n",
    "    roberta = RobertaModel.from_pretrained(\"/home/xiaoguzai/模型/roberta-english-large\")\n",
    "    model = ClassificationModel(roberta)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=1e-5)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    for epoch in range(50):\n",
    "        \n",
    "        print('epoch = %d'%epoch)\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "    \n",
    "        losses = AverageMeter()\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "        step = 0\n",
    "        prebig = True\n",
    "        for batch_text,batch_ids,batch_offset,batch_attention_mask,batch_label in tqdm(train_loader):\n",
    "            batch_ids = batch_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            batch_label = batch_label.to(device)\n",
    "            loss = compute_multilabel_loss(model,batch_ids,\\\n",
    "                                batch_attention_mask,\\\n",
    "                                batch_label)\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss/gradient_accumulation_steps\n",
    "            losses.update(loss.item(),batch_size)\n",
    "            scaler.scale(loss).backward()\n",
    "            #loss.backward()\n",
    "            #每一次进行相应的梯度计算\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
    "            #防止梯度爆炸，超过1000的部分不予计算\n",
    "            if (step+1)%gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            step = step+1\n",
    "        scheduler.step()\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        pred_result = []\n",
    "        label_result = []\n",
    "        for batch_text,batch_ids,batch_offset,batch_attention_mask,batch_origin_label in tqdm(valid_loader):\n",
    "            batch_ids = batch_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                logit = model(input_ids=batch_ids,\\\n",
    "                              attention_mask=batch_attention_mask)\n",
    "                #logit = model(input_ids=batch_ids)\n",
    "                #logit = (4,512,1)\n",
    "                #!!!这点判断需要注意应该是以字符的形式进行判断!!!\n",
    "                #输入的id应该为text+symptom，但是判断正负的时候只应该判断text的内容\n",
    "                #torch.set_printoptions(threshold=np.inf)\n",
    "                logit = torch.sigmoid(logit)\n",
    "                #加上symptom得到的正常的logit\n",
    "                preds = get_char_probs(batch_text,batch_offset,\\\n",
    "                                       logit.cpu())\n",
    "                \n",
    "                for index in range(len(preds)):\n",
    "                    pred_data = preds[index]\n",
    "                    text_data = batch_text[index]\n",
    "                    pred_data = post_process_spaces(pred_data,text_data)\n",
    "                    #current_result.append(pred_data)\n",
    "                    preds[index] = pred_data\n",
    "                \n",
    "                results = get_results(preds,th=0.5)\n",
    "                preds = get_predictions(results)\n",
    "                truths = batch_origin_label\n",
    "                r\"\"\"\n",
    "                preds = \n",
    "                [[[696, 722]], [[668, 693]], [[203, 217]], [[70, 91]]]\n",
    "                truths = \n",
    "                [[[696, 724]], [[668, 693]], [[203, 217]], [[70, 91], [176, 183]]]\n",
    "                \"\"\"\n",
    "                \n",
    "                for data in preds:\n",
    "                    pred_result.append(data)\n",
    "                for data in truths:\n",
    "                    label_result.append(data)\n",
    "\n",
    "        point = get_score(pred_result,label_result)\n",
    "        print('point = ')\n",
    "        print(point)\n",
    "        if point > bestpoint:\n",
    "            bestpoint = point \n",
    "            prebig = 0\n",
    "            #torch.save(model,'roberta_Groupsplit_best_point='+str(bestpoint)+'_fold='+str(current_fold)+'.pth')\n",
    "            torch.save(model,'/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/模型文件/roberta/roberta_Groupsplit_best_point_fold='+str(current_fold)+'.pth')\n",
    "        else:\n",
    "            prebig = prebig+1\n",
    "\n",
    "        if prebig == 4:\n",
    "            break\n",
    "        bestpointlist[current_fold] = max(bestpointlist[current_fold],bestpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
