{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44b0012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb78945",
   "metadata": {},
   "source": [
    "## 加载入tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf14b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "# This must be done before importing transformers\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "transformers_path = Path(\"/home/xiaoguzai/.local/lib/python3.9/site-packages/transformers\")\n",
    "\n",
    "input_dir = Path(\"/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/代码\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "#print('convert_file = ')\n",
    "#print(convert_file)\n",
    "#print('conversion_path = ')\n",
    "#print(conversion_path)\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path \n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "    filepath = deberta_v2_path/filename\n",
    "    \n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)\n",
    "    \n",
    "from transformers import AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "import transformers\n",
    "tokenizer = DebertaV2TokenizerFast.from_pretrained('/home/xiaoguzai/模型/deberta-v3-large')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db304f",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42db6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 354\n",
    "#下面prepare_input的时候将text和feature_text合在一起很巧妙\n",
    "\n",
    "def prepare_input(text, feature_text):\n",
    "    #inputs1 = tokenizer.encode_plus(text,\\\n",
    "    #                               add_special_tokens=True,\\\n",
    "    #                               max_length = max_len,\\\n",
    "    #                               paddin)\n",
    "    inputs = {}\n",
    "    inputs1 = tokenizer.encode_plus(text,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   max_length = max_len,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs2 = tokenizer.encode_plus(feature_text,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   max_length = max_len,\\\n",
    "                                   return_offsets_mapping = False)  \n",
    "    \n",
    "    if len(inputs1['input_ids'])+len(inputs2['input_ids'])-2 > max_len:\n",
    "        exceed_length = len(inputs1['input_ids'])+len(inputs2['input_ids'])-max_len-2\n",
    "        inputs['input_ids'] = inputs1['input_ids'][:len(inputs1['input_ids'])-exceed_length-1]+inputs2['input_ids'][1:]\n",
    "        inputs['attention_mask'] = [1]*max_len\n",
    "    else:\n",
    "        inputs['input_ids'] = inputs1['input_ids'][:-1]+inputs2['input_ids'][1:-1]\n",
    "        inputs['input_ids'] = inputs['input_ids']+[tokenizer.sep_token_id]\n",
    "        #inputs['input_ids'] = inputs['input_ids']+[tokenizer.sep_token_id]+[tokenizer.pad_token_id]*(max_len-len(inputs['input_ids'])-1)\n",
    "        inputs['attention_mask'] = [1]*(len(inputs['input_ids']))\n",
    "        inputs['attention_mask'] = inputs['attention_mask']+[0]*(max_len-len(inputs['input_ids']))\n",
    "        inputs['input_ids'] = inputs['input_ids']+[tokenizer.pad_token_id]*(max_len-len(inputs['input_ids']))\n",
    "        inputs['token_type_ids'] = [0]*max_len\n",
    "        #inputs['attention_mask'] = [1]*max_len\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v,dtype=torch.long)\n",
    "    return inputs,len(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f7d5185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "tokenizer = DebertaV2TokenizerFast.from_pretrained('/home/xiaoguzai/模型/deberta-v3-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b690df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_location_to_offset(text,location_list):\n",
    "    results = np.zeros(len(text))\n",
    "    #以char为级别计算，应该对整个text计算len\n",
    "    for idx, offset_mapping in enumerate(location_list):\n",
    "        try:\n",
    "            start = (int)(offset_mapping[0])\n",
    "            end = (int)(offset_mapping[1])\n",
    "            results[start:end] = 1\n",
    "        except:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "def create_label(text):\n",
    "    encoded = tokenizer.encode_plus(text,\\\n",
    "                                add_special_tokens=True,\\\n",
    "                                max_length = max_len,\\\n",
    "                                padding = \"max_length\",\\\n",
    "                                return_offsets_mapping = True)\n",
    "    offset_mapping = encoded['offset_mapping']\n",
    "    return offset_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c38736a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "import itertools\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self,ids,text,input_ids,offset,token_type_ids,attention_mask):\n",
    "        self.input_ids = input_ids\n",
    "        self.tensors = [ids,\\\n",
    "                        text,\\\n",
    "                        torch.tensor(input_ids,dtype=torch.long),\n",
    "                        torch.tensor(offset),\\\n",
    "                        torch.tensor(token_type_ids,dtype=torch.long),\\\n",
    "                        torch.tensor(attention_mask,dtype=torch.long)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44da7d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(1024,1)\n",
    "        \n",
    "    def forward(self,input_ids,token_type_ids,attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids,\\\n",
    "                           token_type_ids=token_type_ids,\\\n",
    "                           attention_mask=attention_mask)\n",
    "        outputs = outputs[0]\n",
    "        #outputs = self.dropout(outputs)\n",
    "        outputs = self.fc1(outputs)\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f0b537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_char_probs(total_text,offsets,predictions):\n",
    "    results = [np.zeros(len(t)) for t in total_text]\n",
    "    #!!!results 长短不一!!!\n",
    "    #以char为级别计算，应该对整个text计算len\n",
    "    torch.set_printoptions(threshold=np.inf)\n",
    "    for i, (offset, prediction) in enumerate(zip(offsets, predictions)):\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(offset, prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            #results[i][start:end] = ((float)(pred[0].item(),)\n",
    "            results[i][start:end] = pred[0].item()\n",
    "    return results\n",
    "\n",
    "def get_results(test_text,char_probs, th=0.5):\n",
    "    results = []\n",
    "    #for char_prob in char_probs:\n",
    "    for index in range(len(char_probs)):\n",
    "        char_prob = char_probs[index]\n",
    "        char_text = test_text[index]\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        #result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = [[min(r),max(r)] for r in result]\n",
    "        \n",
    "        for index1 in range(len(result)):\n",
    "            if result[index1][0]-1 >= 0 and char_text[result[index1][0]-1] != ' ':\n",
    "                result[index1][0] = result[index1][0]-1\n",
    "                #preds[index][index1][0] = preds[index][index1][0]-1\n",
    "            #if preds[index][index1][1]+1 < len(current_text) and current_text[preds[index][index1][1]+1] != ' ':\n",
    "            #    preds[index][index1][1] = preds[index][index1][1]+1\n",
    "        \n",
    "        result = [str(r[0])+' '+str(r[1]) for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ba4d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/deberta-v3模型文件/deberta_capatalize_noid_best_point=0.887656_fold=0.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963c42b5",
   "metadata": {},
   "source": [
    "## 按照30%的比例增加数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3146bd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>feature_text</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>['696 724']</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>['668 693']</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>['chest pressure']</td>\n",
       "      <td>['203 217']</td>\n",
       "      <td>Chest-pressure</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>['intermittent episodes', 'episode']</td>\n",
       "      <td>['70 91', '176 183']</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>['felt as if he were going to pass out']</td>\n",
       "      <td>['222 258']</td>\n",
       "      <td>Lightheaded</td>\n",
       "      <td>HPI: 17yo M presents with palpitations. Patien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num  \\\n",
       "0  00016_000         0      16            0   \n",
       "1  00016_001         0      16            1   \n",
       "2  00016_002         0      16            2   \n",
       "3  00016_003         0      16            3   \n",
       "4  00016_004         0      16            4   \n",
       "\n",
       "                                 annotation              location  \\\n",
       "0          ['dad with recent heart attcak']           ['696 724']   \n",
       "1             ['mom with \"thyroid disease']           ['668 693']   \n",
       "2                        ['chest pressure']           ['203 217']   \n",
       "3      ['intermittent episodes', 'episode']  ['70 91', '176 183']   \n",
       "4  ['felt as if he were going to pass out']           ['222 258']   \n",
       "\n",
       "                                        feature_text  \\\n",
       "0  Family-history-of-MI-OR-Family-history-of-myoc...   \n",
       "1                 Family-history-of-thyroid-disorder   \n",
       "2                                     Chest-pressure   \n",
       "3                              Intermittent-symptoms   \n",
       "4                                        Lightheaded   \n",
       "\n",
       "                                          pn_history  \n",
       "0  HPI: 17yo M presents with palpitations. Patien...  \n",
       "1  HPI: 17yo M presents with palpitations. Patien...  \n",
       "2  HPI: 17yo M presents with palpitations. Patien...  \n",
       "3  HPI: 17yo M presents with palpitations. Patien...  \n",
       "4  HPI: 17yo M presents with palpitations. Patien...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/train.csv')\n",
    "features = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/features.csv')\n",
    "patient_notes = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/patient_notes.csv')\n",
    "train = train.merge(features, on=['feature_num','case_num'],how='left')\n",
    "train = train.merge(patient_notes, on=['pn_num','case_num'],how='left')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "222148b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>feature_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Chest-pressure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Lightheaded</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_num  case_num                                       feature_text\n",
       "0            0         0  Family-history-of-MI-OR-Family-history-of-myoc...\n",
       "1            1         0                 Family-history-of-thyroid-disorder\n",
       "2            2         0                                     Chest-pressure\n",
       "3            3         0                              Intermittent-symptoms\n",
       "4            4         0                                        Lightheaded"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d4dad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = train.pn_num.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7467fd",
   "metadata": {},
   "source": [
    "## 增加0标签部分内容\n",
    "id:00016~02436\n",
    "長度:0～1301\n",
    "(1301-0)/12 = 108\n",
    "108/3 = 36\n",
    "间隔id:2436/36=68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de5b4bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>feature_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Family-history-of-MI-OR-Family-history-of-myoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Family-history-of-thyroid-disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Chest-pressure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Intermittent-symptoms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Lightheaded</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_num  case_num                                       feature_text\n",
       "0            0         0  Family-history-of-MI-OR-Family-history-of-myoc...\n",
       "1            1         0                 Family-history-of-thyroid-disorder\n",
       "2            2         0                                     Chest-pressure\n",
       "3            3         0                              Intermittent-symptoms\n",
       "4            4         0                                        Lightheaded"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edf4782a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pn_num</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17-year-old male, has come to the student heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17 yo male with recurrent palpitations for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Dillon Cleveland is a 17 y.o. male patient wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a 17 yo m c/o palpitation started 3 mos ago; \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>17yo male with no pmh here for evaluation of p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pn_num  case_num                                         pn_history\n",
       "0       0         0  17-year-old male, has come to the student heal...\n",
       "1       1         0  17 yo male with recurrent palpitations for the...\n",
       "2       2         0  Dillon Cleveland is a 17 y.o. male patient wit...\n",
       "3       3         0  a 17 yo m c/o palpitation started 3 mos ago; \\...\n",
       "4       4         0  17yo male with no pmh here for evaluation of p..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "860a4133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========s = ========\n",
      "{'id': '00001_000', 'case_num': 0, 'pn_num': 1, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '00069_000', 'case_num': 0, 'pn_num': 69, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '00137_000', 'case_num': 0, 'pn_num': 137, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '00205_000', 'case_num': 0, 'pn_num': 205, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '00273_000', 'case_num': 0, 'pn_num': 273, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '00341_000', 'case_num': 0, 'pn_num': 341, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '00409_000', 'case_num': 0, 'pn_num': 409, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '00477_000', 'case_num': 0, 'pn_num': 477, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '00478_000', 'case_num': 0, 'pn_num': 478, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '00545_000', 'case_num': 0, 'pn_num': 545, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '00613_000', 'case_num': 0, 'pn_num': 613, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '00681_000', 'case_num': 0, 'pn_num': 681, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '00749_000', 'case_num': 0, 'pn_num': 749, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '00818_000', 'case_num': 0, 'pn_num': 818, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '00819_000', 'case_num': 0, 'pn_num': 819, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '00885_000', 'case_num': 0, 'pn_num': 885, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '00953_000', 'case_num': 0, 'pn_num': 953, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '01021_000', 'case_num': 0, 'pn_num': 1021, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '01089_000', 'case_num': 0, 'pn_num': 1089, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '01157_000', 'case_num': 0, 'pn_num': 1157, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '01225_000', 'case_num': 0, 'pn_num': 1225, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '01293_000', 'case_num': 0, 'pn_num': 1293, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '01361_000', 'case_num': 0, 'pn_num': 1361, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '01429_000', 'case_num': 0, 'pn_num': 1429, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '01497_000', 'case_num': 0, 'pn_num': 1497, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '01565_000', 'case_num': 0, 'pn_num': 1565, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '01633_000', 'case_num': 0, 'pn_num': 1633, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '01701_000', 'case_num': 0, 'pn_num': 1701, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '01769_000', 'case_num': 0, 'pn_num': 1769, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '01837_000', 'case_num': 0, 'pn_num': 1837, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '01905_000', 'case_num': 0, 'pn_num': 1905, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '01906_000', 'case_num': 0, 'pn_num': 1906, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '01973_000', 'case_num': 0, 'pn_num': 1973, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '02041_000', 'case_num': 0, 'pn_num': 2041, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '02109_000', 'case_num': 0, 'pn_num': 2109, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '02177_000', 'case_num': 0, 'pn_num': 2177, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '02245_000', 'case_num': 0, 'pn_num': 2245, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '02313_000', 'case_num': 0, 'pn_num': 2313, 'feature_num': 0}\n",
      "====================\n",
      "========s = ========\n",
      "{'id': '02381_000', 'case_num': 0, 'pn_num': 2381, 'feature_num': 0}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "#pseudo_labeling = pd.read_csv('Pseudo-Labelling.csv')\n",
    "import numpy as np\n",
    "pseudo_labeling = pd.DataFrame()\n",
    "#add 30 every time\n",
    "for index in range(36):\n",
    "    #总的次数\n",
    "    data1 = index*68\n",
    "    \n",
    "    #******判斷是否能預測出內容\n",
    "    while 1:\n",
    "        flag = False\n",
    "        final_result = []\n",
    "        data1 = data1+1\n",
    "        while data1 in values:\n",
    "            data1 = data1+1\n",
    "        #for index1 in range(13):\n",
    "        #每次增加的id\n",
    "        index1 = 0\n",
    "        new_data1 = str(data1)\n",
    "        r1 = new_data1.rjust(5,'0')\n",
    "        new_data2 = str(index1)\n",
    "        r2 = new_data2.rjust(3,'0')\n",
    "        s = {\"id\":r1+'_'+r2,\\\n",
    "             \"case_num\":0,\\\n",
    "             \"pn_num\":int(r1),\\\n",
    "             \"feature_num\":int(r2)}\n",
    "        #只判断0位置\n",
    "        print('========s = ========')\n",
    "        print(s)\n",
    "        print('====================')\n",
    "        current_data = pd.DataFrame(s,index=[0])\n",
    "        current_data = current_data.merge(features,on=['feature_num','case_num'],how='left')\n",
    "        current_data = current_data.merge(patient_notes,on=['pn_num','case_num'],how='left')\n",
    "        test_ids = []\n",
    "        test_text,test_input_ids,test_token_type_ids = [],[],[]\n",
    "        test_attention_mask,test_offset,test_label = [],[],[]\n",
    "        for  index,data  in  current_data.iterrows():\n",
    "            #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "            ids = data['id']\n",
    "            text = data['pn_history']\n",
    "            feature_text = data['feature_text']\n",
    "            if type(text) != str and math.isnan(text):\n",
    "                break\n",
    "            inputs,length = prepare_input(text,feature_text)\n",
    "            current_offset = create_label(text)\n",
    "            test_ids.append(ids)\n",
    "            test_text.append(text)\n",
    "            test_input_ids.append(inputs['input_ids'].tolist())\n",
    "            test_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "            test_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "            test_offset.append(current_offset)\n",
    "        \n",
    "        if len(test_text) == 0:\n",
    "            continue\n",
    "\n",
    "        test_dataset = TestDataset(test_ids,\\\n",
    "                   test_text,\\\n",
    "                   test_input_ids,\\\n",
    "                   test_offset,\\\n",
    "                   test_token_type_ids,\\\n",
    "                   test_attention_mask)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=4,shuffle=False)\n",
    "\n",
    "        current_result = []\n",
    "        for batch_ids,batch_text,batch_input_ids,batch_offset,batch_token_type_ids,batch_attention_mask in test_loader:\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "            batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                logit = model(input_ids=batch_input_ids,token_type_ids=batch_token_type_ids,\\\n",
    "                              attention_mask=batch_attention_mask)\n",
    "                logit = torch.sigmoid(logit)\n",
    "                preds = get_char_probs(batch_text,batch_offset,\\\n",
    "                                       logit.cpu())\n",
    "                for data in preds:\n",
    "                    current_result.append(data)\n",
    "                current_result = get_results(batch_text,current_result)\n",
    "        final_result.append(current_result)\n",
    "        #results = get_results(test_text,final_result)\n",
    "        for data in final_result:\n",
    "            if len(data) != 0:\n",
    "                flag = True\n",
    "        if flag == True:\n",
    "            for index1 in range(13):\n",
    "            #每次增加的id\n",
    "                new_data1 = str(data1)\n",
    "                r1 = new_data1.rjust(5,'0')\n",
    "                data2 = str(index1)\n",
    "                r2 = data2.rjust(3,'0')\n",
    "                s = {\"id\":r1+'_'+r2,\\\n",
    "                     \"case_num\":0,\\\n",
    "                     \"pn_num\":int(r1),\\\n",
    "                     \"feature_num\":int(r2)}\n",
    "                pseudo_labeling = pseudo_labeling.append(s,ignore_index=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa0aa700",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27369/2538804868.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'prin' is not defined"
     ]
    }
   ],
   "source": [
    "prin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf0b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8c9cb2",
   "metadata": {},
   "source": [
    "## 增加1标签部分内容\n",
    "id:10004~10988\n",
    "长度:1302～2601\n",
    "总的次数(2601-1302)/36=36\n",
    "间隔id:(10988-10004)/36=944/36=26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9982c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======s = ======\n",
      "{'id': '10031_000', 'case_num': 1, 'pn_num': 10031, 'feature_num': 0}\n",
      "================\n",
      "ids = \n",
      "10031_000\n",
      "text = \n",
      "HPI: 20 yo F c/o suddent RLQ abdominal pain, dull crampy pain, PS 5/10, took ibuprofen but it didn't help, worsened when walking around. she has been having diarrhea for a couple of days: watery, brown stool. He appetite decreased. No fever, no urinary/bowel movement problems, no falls. No abnormal vaginal discharge/bleeding. \r\n",
      "\r\n",
      "ROS: negative exxept above\r\n",
      "PMH: none\r\n",
      "PSH:none\r\n",
      "meds: ibuprofen\r\n",
      "allergies: NKDA\r\n",
      "menstruation: LMP last 2 weeks, regular every month, no change in flow\r\n",
      "SH: no smoking, no EtOH, no recreational drug use, last SI 9 months ago with her boyfriend; use condom everytime, no STD history\n",
      "feature_text = \n",
      "nan\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27369/3193697868.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature_text = '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mcurrent_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mtest_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27369/2943278435.py\u001b[0m in \u001b[0;36mprepare_input\u001b[0;34m(text, feature_text)\u001b[0m\n\u001b[1;32m     12\u001b[0m                                    \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                    return_offsets_mapping = False)\n\u001b[0;32m---> 14\u001b[0;31m     inputs2 = tokenizer.encode_plus(feature_text,\\\n\u001b[0m\u001b[1;32m     15\u001b[0m                                    \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                    \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2509\u001b[0m         )\n\u001b[1;32m   2510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2511\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   2512\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2513\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0mbatched_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         batched_output = self._batch_encode_plus(\n\u001b[0m\u001b[1;32m    483\u001b[0m             \u001b[0mbatched_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_split_into_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    407\u001b[0m         )\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "total_index = 36\n",
    "index = 1\n",
    "#for index in range(36):\n",
    "#for data1 in range(95217):\n",
    "data1 = 0\n",
    "for index in range(36):\n",
    "    data1 = 10030+index*26\n",
    "        \n",
    "    #******判斷是否能預測出內容\n",
    "    while 1:\n",
    "        flag = False\n",
    "        final_result = []\n",
    "        data1 = data1+1\n",
    "        while data1 in values:\n",
    "            data1 = data1+1\n",
    "        #for index1 in range(13):\n",
    "        #每次增加的id\n",
    "        index1 = 0\n",
    "        new_data1 = str(data1)\n",
    "        r1 = new_data1.rjust(5,'0')\n",
    "        new_data2 = str(index1)\n",
    "        r2 = new_data2.rjust(3,'0')\n",
    "        s = {\"id\":r1+'_'+r2,\\\n",
    "             \"case_num\":0,\\\n",
    "             \"pn_num\":int(r1),\\\n",
    "             \"feature_num\":int(r2)}\n",
    "        print('======s = ======')\n",
    "        print(s)\n",
    "        print('================')\n",
    "        #只判断0位置\n",
    "        current_data = pd.DataFrame(s,index=[0])\n",
    "        current_data = current_data.merge(features,on=['feature_num','case_num'],how='left')\n",
    "        current_data = current_data.merge(patient_notes,on=['pn_num','case_num'],how='left')\n",
    "        test_ids = []\n",
    "        test_text,test_input_ids,test_token_type_ids = [],[],[]\n",
    "        test_attention_mask,test_offset,test_label = [],[],[]\n",
    "        for  index,data  in  current_data.iterrows():\n",
    "            #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "            ids = data['id']\n",
    "            text = data['pn_history']\n",
    "            feature_text = data['feature_text']\n",
    "            if type(text) != str and math.isnan(text):\n",
    "                break\n",
    "            print('ids = ')\n",
    "            print(ids)\n",
    "            print('text = ')\n",
    "            print(text)\n",
    "            print('feature_text = ')\n",
    "            print(feature_text)\n",
    "            inputs,length = prepare_input(text,feature_text)\n",
    "            current_offset = create_label(text)\n",
    "            test_ids.append(ids)\n",
    "            test_text.append(text)\n",
    "            test_input_ids.append(inputs['input_ids'].tolist())\n",
    "            test_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "            test_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "            test_offset.append(current_offset)\n",
    "        \n",
    "        if len(test_text) == 0:\n",
    "            continue\n",
    "\n",
    "        test_dataset = TestDataset(test_ids,\\\n",
    "                   test_text,\\\n",
    "                   test_input_ids,\\\n",
    "                   test_offset,\\\n",
    "                   test_token_type_ids,\\\n",
    "                   test_attention_mask)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=4,shuffle=False)\n",
    "\n",
    "        current_result = []\n",
    "        for batch_ids,batch_text,batch_input_ids,batch_offset,batch_token_type_ids,batch_attention_mask in test_loader:\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "            batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                logit = model(input_ids=batch_input_ids,token_type_ids=batch_token_type_ids,\\\n",
    "                              attention_mask=batch_attention_mask)\n",
    "                logit = torch.sigmoid(logit)\n",
    "                preds = get_char_probs(batch_text,batch_offset,\\\n",
    "                                       logit.cpu())\n",
    "                for data in preds:\n",
    "                    current_result.append(data)\n",
    "                current_result = get_results(batch_text,current_result)\n",
    "        final_result.append(current_result)\n",
    "        #results = get_results(test_text,final_result)\n",
    "        for data in final_result:\n",
    "            if len(data) != 0:\n",
    "                flag = True\n",
    "        if flag == True:\n",
    "            for index1 in range(100,113):\n",
    "            #每次增加的id\n",
    "                new_data1 = str(data1)\n",
    "                r1 = new_data1.rjust(5,'0')\n",
    "                data2 = str(index1)\n",
    "                r2 = data2.rjust(3,'0')\n",
    "                s = {\"id\":r1+'_'+r2,\\\n",
    "                     \"case_num\":1,\\\n",
    "                     \"pn_num\":int(r1),\\\n",
    "                     \"feature_num\":int(r2)}\n",
    "                pseudo_labeling = pseudo_labeling.append(s,ignore_index=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763edee",
   "metadata": {},
   "source": [
    "## 增加2标签部分内容(20001～22100)\n",
    "id:20001~22123\n",
    "長度2602～4301\n",
    "总的次数:(4301-2602)/36=47\n",
    "间隔id:(22123-20001)/47=45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ae48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(47):\n",
    "    data1 = 20010+index*45\n",
    "    #******判斷是否能預測出內容\n",
    "    while 1:\n",
    "        flag = False\n",
    "        final_result = []\n",
    "        data1 = data1+1\n",
    "        while data1 in values:\n",
    "            data1 = data1+1\n",
    "        #for index1 in range(13):\n",
    "        #每次增加的id\n",
    "        index1 = 0\n",
    "        new_data1 = str(data1)\n",
    "        r1 = new_data1.rjust(5,'0')\n",
    "        new_data2 = str(index1)\n",
    "        r2 = new_data2.rjust(3,'0')\n",
    "        s = {\"id\":r1+'_'+r2,\\\n",
    "             \"case_num\":0,\\\n",
    "             \"pn_num\":int(r1),\\\n",
    "             \"feature_num\":int(r2)}\n",
    "        #只判断0位置\n",
    "        current_data = pd.DataFrame(s,index=[0])\n",
    "        current_data = current_data.merge(features,on=['feature_num','case_num'],how='left')\n",
    "        current_data = current_data.merge(patient_notes,on=['pn_num','case_num'],how='left')\n",
    "        test_ids = []\n",
    "        test_text,test_input_ids,test_token_type_ids = [],[],[]\n",
    "        test_attention_mask,test_offset,test_label = [],[],[]\n",
    "        for  index,data  in  current_data.iterrows():\n",
    "            #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "            ids = data['id']\n",
    "            text = data['pn_history']\n",
    "            feature_text = data['feature_text']\n",
    "            if type(text) != str and math.isnan(text):\n",
    "                break\n",
    "            inputs,length = prepare_input(ids,text,feature_text)\n",
    "            current_offset = create_label(text)\n",
    "            test_ids.append(ids)\n",
    "            test_text.append(text)\n",
    "            test_input_ids.append(inputs['input_ids'].tolist())\n",
    "            test_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "            test_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "            test_offset.append(current_offset)\n",
    "        \n",
    "        if len(test_text) == 0:\n",
    "            continue\n",
    "\n",
    "        test_dataset = TestDataset(test_ids,\\\n",
    "                   test_text,\\\n",
    "                   test_input_ids,\\\n",
    "                   test_offset,\\\n",
    "                   test_token_type_ids,\\\n",
    "                   test_attention_mask)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=4,shuffle=False)\n",
    "\n",
    "        current_result = []\n",
    "        for batch_ids,batch_text,batch_input_ids,batch_offset,batch_token_type_ids,batch_attention_mask in test_loader:\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "            batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                logit = model(input_ids=batch_input_ids,token_type_ids=batch_token_type_ids,\\\n",
    "                              attention_mask=batch_attention_mask)\n",
    "                logit = torch.sigmoid(logit)\n",
    "                preds = get_char_probs(batch_text,batch_offset,\\\n",
    "                                       logit.cpu())\n",
    "                for data in preds:\n",
    "                    current_result.append(data)\n",
    "                current_result = get_results(batch_text,current_result)\n",
    "        final_result.append(current_result)\n",
    "        #results = get_results(test_text,final_result)\n",
    "        for data in final_result:\n",
    "            if len(data) != 0:\n",
    "                flag = True\n",
    "        if flag == True:\n",
    "            for index1 in range(13):\n",
    "            #每次增加的id\n",
    "                data1 = str(data1)\n",
    "                r1 = data1.rjust(5,'0')\n",
    "                data2 = str(index1)\n",
    "                r2 = data2.rjust(3,'0')\n",
    "                s = {\"id\":r1+'_'+r2,\\\n",
    "                     \"case_num\":0,\\\n",
    "                     \"pn_num\":int(r1),\\\n",
    "                     \"feature_num\":int(r2)}\n",
    "                pseudo_labeling = pseudo_labeling.append(s,ignore_index=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865ef69",
   "metadata": {},
   "source": [
    "## 增加标签3部分内容\n",
    "id:30037~39921\n",
    "長度4302~5901\n",
    "总的次数(5901-4302)/36=44\n",
    "间隔id(39921-30037)/44 = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96180ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(30):\n",
    "    data1 = 30300+index*224\n",
    "    #******判斷是否能預測出內容\n",
    "    while 1:\n",
    "        flag = False\n",
    "        final_result = []\n",
    "        data1 = data1+1\n",
    "        while data1 in values:\n",
    "            data1 = data1+1\n",
    "        #for index1 in range(13):\n",
    "        #每次增加的id\n",
    "        index1 = 0\n",
    "        new_data1 = str(data1)\n",
    "        r1 = new_data1.rjust(5,'0')\n",
    "        new_data2 = str(index1)\n",
    "        r2 = new_data2.rjust(3,'0')\n",
    "        s = {\"id\":r1+'_'+r2,\\\n",
    "             \"case_num\":0,\\\n",
    "             \"pn_num\":int(r1),\\\n",
    "             \"feature_num\":int(r2)}\n",
    "        #只判断0位置\n",
    "        current_data = pd.DataFrame(s,index=[0])\n",
    "        current_data = current_data.merge(features,on=['feature_num','case_num'],how='left')\n",
    "        current_data = current_data.merge(patient_notes,on=['pn_num','case_num'],how='left')\n",
    "        test_ids = []\n",
    "        test_text,test_input_ids,test_token_type_ids = [],[],[]\n",
    "        test_attention_mask,test_offset,test_label = [],[],[]\n",
    "        for  index,data  in  current_data.iterrows():\n",
    "            #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "            ids = data['id']\n",
    "            text = data['pn_history']\n",
    "            feature_text = data['feature_text']\n",
    "            if type(text) != str and math.isnan(text):\n",
    "                break\n",
    "            inputs,length = prepare_input(ids,text,feature_text)\n",
    "            current_offset = create_label(text)\n",
    "            test_ids.append(ids)\n",
    "            test_text.append(text)\n",
    "            test_input_ids.append(inputs['input_ids'].tolist())\n",
    "            test_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "            test_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "            test_offset.append(current_offset)\n",
    "        \n",
    "        if len(test_text) == 0:\n",
    "            continue\n",
    "\n",
    "        test_dataset = TestDataset(test_ids,\\\n",
    "                   test_text,\\\n",
    "                   test_input_ids,\\\n",
    "                   test_offset,\\\n",
    "                   test_token_type_ids,\\\n",
    "                   test_attention_mask)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=4,shuffle=False)\n",
    "\n",
    "        current_result = []\n",
    "        for batch_ids,batch_text,batch_input_ids,batch_offset,batch_token_type_ids,batch_attention_mask in test_loader:\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "            batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                logit = model(input_ids=batch_input_ids,token_type_ids=batch_token_type_ids,\\\n",
    "                              attention_mask=batch_attention_mask)\n",
    "                logit = torch.sigmoid(logit)\n",
    "                preds = get_char_probs(batch_text,batch_offset,\\\n",
    "                                       logit.cpu())\n",
    "                for data in preds:\n",
    "                    current_result.append(data)\n",
    "                current_result = get_results(batch_text,current_result)\n",
    "        final_result.append(current_result)\n",
    "        #results = get_results(test_text,final_result)\n",
    "        for data in final_result:\n",
    "            if len(data) != 0:\n",
    "                flag = True\n",
    "        if flag == True:\n",
    "            for index1 in range(13):\n",
    "            #每次增加的id\n",
    "                data1 = str(data1)\n",
    "                r1 = data1.rjust(5,'0')\n",
    "                data2 = str(index1)\n",
    "                r2 = data2.rjust(3,'0')\n",
    "                s = {\"id\":r1+'_'+r2,\\\n",
    "                     \"case_num\":0,\\\n",
    "                     \"pn_num\":int(r1),\\\n",
    "                     \"feature_num\":int(r2)}\n",
    "                pseudo_labeling = pseudo_labeling.append(s,ignore_index=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b26f3",
   "metadata": {},
   "source": [
    "## 增加标签4部分内容\n",
    "id:40045~45947\n",
    "長度5902~6901\n",
    "总的次数:(6901-5902)/36=28\n",
    "间隔id:(45947-40045)/28=211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ca9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(28):\n",
    "    data1 = 40080+index*211\n",
    "    #******判斷是否能預測出內容\n",
    "    while 1:\n",
    "        flag = False\n",
    "        final_result = []\n",
    "        data1 = data1+1\n",
    "        while data1 in values:\n",
    "            data1 = data1+1\n",
    "        #for index1 in range(13):\n",
    "        #每次增加的id\n",
    "        index1 = 0\n",
    "        new_data1 = str(data1)\n",
    "        r1 = new_data1.rjust(5,'0')\n",
    "        new_data2 = str(index1)\n",
    "        r2 = new_data2.rjust(3,'0')\n",
    "        s = {\"id\":r1+'_'+r2,\\\n",
    "             \"case_num\":0,\\\n",
    "             \"pn_num\":int(r1),\\\n",
    "             \"feature_num\":int(r2)}\n",
    "        #只判断0位置\n",
    "        current_data = pd.DataFrame(s,index=[0])\n",
    "        current_data = current_data.merge(features,on=['feature_num','case_num'],how='left')\n",
    "        current_data = current_data.merge(patient_notes,on=['pn_num','case_num'],how='left')\n",
    "        test_ids = []\n",
    "        test_text,test_input_ids,test_token_type_ids = [],[],[]\n",
    "        test_attention_mask,test_offset,test_label = [],[],[]\n",
    "        for  index,data  in  current_data.iterrows():\n",
    "            #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "            ids = data['id']\n",
    "            text = data['pn_history']\n",
    "            feature_text = data['feature_text']\n",
    "            if type(text) != str and math.isnan(text):\n",
    "                break\n",
    "            inputs,length = prepare_input(ids,text,feature_text)\n",
    "            current_offset = create_label(text)\n",
    "            test_ids.append(ids)\n",
    "            test_text.append(text)\n",
    "            test_input_ids.append(inputs['input_ids'].tolist())\n",
    "            test_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "            test_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "            test_offset.append(current_offset)\n",
    "        \n",
    "        if len(test_text) == 0:\n",
    "            continue\n",
    "\n",
    "        test_dataset = TestDataset(test_ids,\\\n",
    "                   test_text,\\\n",
    "                   test_input_ids,\\\n",
    "                   test_offset,\\\n",
    "                   test_token_type_ids,\\\n",
    "                   test_attention_mask)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=4,shuffle=False)\n",
    "\n",
    "        current_result = []\n",
    "        for batch_ids,batch_text,batch_input_ids,batch_offset,batch_token_type_ids,batch_attention_mask in test_loader:\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "            batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                logit = model(input_ids=batch_input_ids,token_type_ids=batch_token_type_ids,\\\n",
    "                              attention_mask=batch_attention_mask)\n",
    "                logit = torch.sigmoid(logit)\n",
    "                preds = get_char_probs(batch_text,batch_offset,\\\n",
    "                                       logit.cpu())\n",
    "                for data in preds:\n",
    "                    current_result.append(data)\n",
    "                current_result = get_results(batch_text,current_result)\n",
    "        final_result.append(current_result)\n",
    "        #results = get_results(test_text,final_result)\n",
    "        for data in final_result:\n",
    "            if len(data) != 0:\n",
    "                flag = True\n",
    "        if flag == True:\n",
    "            for index1 in range(13):\n",
    "            #每次增加的id\n",
    "                data1 = str(data1)\n",
    "                r1 = data1.rjust(5,'0')\n",
    "                data2 = str(index1)\n",
    "                r2 = data2.rjust(3,'0')\n",
    "                s = {\"id\":r1+'_'+r2,\\\n",
    "                     \"case_num\":0,\\\n",
    "                     \"pn_num\":int(r1),\\\n",
    "                     \"feature_num\":int(r2)}\n",
    "                pseudo_labeling = pseudo_labeling.append(s,ignore_index=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5b2307",
   "metadata": {},
   "source": [
    "## 增加标签5部分内容\n",
    "id:50072~57026\n",
    "長度6902～8701\n",
    "总的次数:(8702-6902)/36=50\n",
    "间隔id:(57026-50072)/50 = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc97a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(50):\n",
    "    data1 = 50140+index*140\n",
    "    #******判斷是否能預測出內容\n",
    "    while 1:\n",
    "        flag = False\n",
    "        final_result = []\n",
    "        data1 = data1+1\n",
    "        while data1 in values:\n",
    "            data1 = data1+1\n",
    "        #for index1 in range(13):\n",
    "        #每次增加的id\n",
    "        index1 = 0\n",
    "        new_data1 = str(data1)\n",
    "        r1 = new_data1.rjust(5,'0')\n",
    "        new_data2 = str(index1)\n",
    "        r2 = new_data2.rjust(3,'0')\n",
    "        s = {\"id\":r1+'_'+r2,\\\n",
    "             \"case_num\":0,\\\n",
    "             \"pn_num\":int(r1),\\\n",
    "             \"feature_num\":int(r2)}\n",
    "        #只判断0位置\n",
    "        current_data = pd.DataFrame(s,index=[0])\n",
    "        current_data = current_data.merge(features,on=['feature_num','case_num'],how='left')\n",
    "        current_data = current_data.merge(patient_notes,on=['pn_num','case_num'],how='left')\n",
    "        test_ids = []\n",
    "        test_text,test_input_ids,test_token_type_ids = [],[],[]\n",
    "        test_attention_mask,test_offset,test_label = [],[],[]\n",
    "        for  index,data  in  current_data.iterrows():\n",
    "            #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "            ids = data['id']\n",
    "            text = data['pn_history']\n",
    "            feature_text = data['feature_text']\n",
    "            if type(text) != str and math.isnan(text):\n",
    "                break\n",
    "            inputs,length = prepare_input(ids,text,feature_text)\n",
    "            current_offset = create_label(text)\n",
    "            test_ids.append(ids)\n",
    "            test_text.append(text)\n",
    "            test_input_ids.append(inputs['input_ids'].tolist())\n",
    "            test_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "            test_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "            test_offset.append(current_offset)\n",
    "        \n",
    "        if len(test_text) == 0:\n",
    "            continue\n",
    "\n",
    "        test_dataset = TestDataset(test_ids,\\\n",
    "                   test_text,\\\n",
    "                   test_input_ids,\\\n",
    "                   test_offset,\\\n",
    "                   test_token_type_ids,\\\n",
    "                   test_attention_mask)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=4,shuffle=False)\n",
    "\n",
    "        current_result = []\n",
    "        for batch_ids,batch_text,batch_input_ids,batch_offset,batch_token_type_ids,batch_attention_mask in test_loader:\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "            batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                logit = model(input_ids=batch_input_ids,token_type_ids=batch_token_type_ids,\\\n",
    "                              attention_mask=batch_attention_mask)\n",
    "                logit = torch.sigmoid(logit)\n",
    "                preds = get_char_probs(batch_text,batch_offset,\\\n",
    "                                       logit.cpu())\n",
    "                for data in preds:\n",
    "                    current_result.append(data)\n",
    "                current_result = get_results(batch_text,current_result)\n",
    "        final_result.append(current_result)\n",
    "        #results = get_results(test_text,final_result)\n",
    "        for data in final_result:\n",
    "            if len(data) != 0:\n",
    "                flag = True\n",
    "        if flag == True:\n",
    "            for index1 in range(13):\n",
    "            #每次增加的id\n",
    "                data1 = str(data1)\n",
    "                r1 = data1.rjust(5,'0')\n",
    "                data2 = str(index1)\n",
    "                r2 = data2.rjust(3,'0')\n",
    "                s = {\"id\":r1+'_'+r2,\\\n",
    "                     \"case_num\":0,\\\n",
    "                     \"pn_num\":int(r1),\\\n",
    "                     \"feature_num\":int(r2)}\n",
    "                pseudo_labeling = pseudo_labeling.append(s,ignore_index=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb10876",
   "metadata": {},
   "source": [
    "## 增加标签6部分内容\n",
    "id:60004~61768\n",
    "长度:8702~9901\n",
    "总的次数:(9901-8702)/36=33\n",
    "间隔id:(61768-60004)/33=53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(33):\n",
    "    data1 = 60053+index*53\n",
    "    #******判斷是否能預測出內容\n",
    "    while 1:\n",
    "        flag = False\n",
    "        final_result = []\n",
    "        data1 = data1+1\n",
    "        while data1 in values:\n",
    "            data1 = data1+1\n",
    "        #for index1 in range(13):\n",
    "        #每次增加的id\n",
    "        index1 = 0\n",
    "        new_data1 = str(data1)\n",
    "        r1 = new_data1.rjust(5,'0')\n",
    "        new_data2 = str(index1)\n",
    "        r2 = new_data2.rjust(3,'0')\n",
    "        s = {\"id\":r1+'_'+r2,\\\n",
    "             \"case_num\":0,\\\n",
    "             \"pn_num\":int(r1),\\\n",
    "             \"feature_num\":int(r2)}\n",
    "        #只判断0位置\n",
    "        current_data = pd.DataFrame(s,index=[0])\n",
    "        current_data = current_data.merge(features,on=['feature_num','case_num'],how='left')\n",
    "        current_data = current_data.merge(patient_notes,on=['pn_num','case_num'],how='left')\n",
    "        test_ids = []\n",
    "        test_text,test_input_ids,test_token_type_ids = [],[],[]\n",
    "        test_attention_mask,test_offset,test_label = [],[],[]\n",
    "        for  index,data  in  current_data.iterrows():\n",
    "            #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "            ids = data['id']\n",
    "            text = data['pn_history']\n",
    "            feature_text = data['feature_text']\n",
    "            if type(text) != str and math.isnan(text):\n",
    "                break\n",
    "            inputs,length = prepare_input(ids,text,feature_text)\n",
    "            current_offset = create_label(text)\n",
    "            test_ids.append(ids)\n",
    "            test_text.append(text)\n",
    "            test_input_ids.append(inputs['input_ids'].tolist())\n",
    "            test_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "            test_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "            test_offset.append(current_offset)\n",
    "        \n",
    "        if len(test_text) == 0:\n",
    "            continue\n",
    "\n",
    "        test_dataset = TestDataset(test_ids,\\\n",
    "                   test_text,\\\n",
    "                   test_input_ids,\\\n",
    "                   test_offset,\\\n",
    "                   test_token_type_ids,\\\n",
    "                   test_attention_mask)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=4,shuffle=False)\n",
    "\n",
    "        current_result = []\n",
    "        for batch_ids,batch_text,batch_input_ids,batch_offset,batch_token_type_ids,batch_attention_mask in test_loader:\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "            batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                logit = model(input_ids=batch_input_ids,token_type_ids=batch_token_type_ids,\\\n",
    "                              attention_mask=batch_attention_mask)\n",
    "                logit = torch.sigmoid(logit)\n",
    "                preds = get_char_probs(batch_text,batch_offset,\\\n",
    "                                       logit.cpu())\n",
    "                for data in preds:\n",
    "                    current_result.append(data)\n",
    "                current_result = get_results(batch_text,current_result)\n",
    "        final_result.append(current_result)\n",
    "        #results = get_results(test_text,final_result)\n",
    "        for data in final_result:\n",
    "            if len(data) != 0:\n",
    "                flag = True\n",
    "        if flag == True:\n",
    "            for index1 in range(13):\n",
    "            #每次增加的id\n",
    "                data1 = str(data1)\n",
    "                r1 = data1.rjust(5,'0')\n",
    "                data2 = str(index1)\n",
    "                r2 = data2.rjust(3,'0')\n",
    "                s = {\"id\":r1+'_'+r2,\\\n",
    "                     \"case_num\":0,\\\n",
    "                     \"pn_num\":int(r1),\\\n",
    "                     \"feature_num\":int(r2)}\n",
    "                pseudo_labeling = pseudo_labeling.append(s,ignore_index=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11365eb2",
   "metadata": {},
   "source": [
    "## 增加标签7部分内容\n",
    "id:70087~74087\n",
    "长度9902~10801\n",
    "总的次数:(10801-9902)/36=25\n",
    "间隔id:(74087-70087)/25=160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81571da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(25):\n",
    "    data1 = 70160+index*160\n",
    "    #******判斷是否能預測出內容\n",
    "    while 1:\n",
    "        flag = False\n",
    "        final_result = []\n",
    "        data1 = data1+1\n",
    "        while data1 in values:\n",
    "            data1 = data1+1\n",
    "        #for index1 in range(13):\n",
    "        #每次增加的id\n",
    "        index1 = 0\n",
    "        new_data1 = str(data1)\n",
    "        r1 = new_data1.rjust(5,'0')\n",
    "        new_data2 = str(index1)\n",
    "        r2 = new_data2.rjust(3,'0')\n",
    "        s = {\"id\":r1+'_'+r2,\\\n",
    "             \"case_num\":0,\\\n",
    "             \"pn_num\":int(r1),\\\n",
    "             \"feature_num\":int(r2)}\n",
    "        #只判断0位置\n",
    "        current_data = pd.DataFrame(s,index=[0])\n",
    "        current_data = current_data.merge(features,on=['feature_num','case_num'],how='left')\n",
    "        current_data = current_data.merge(patient_notes,on=['pn_num','case_num'],how='left')\n",
    "        test_ids = []\n",
    "        test_text,test_input_ids,test_token_type_ids = [],[],[]\n",
    "        test_attention_mask,test_offset,test_label = [],[],[]\n",
    "        for  index,data  in  current_data.iterrows():\n",
    "            #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "            ids = data['id']\n",
    "            text = data['pn_history']\n",
    "            feature_text = data['feature_text']\n",
    "            if type(text) != str and math.isnan(text):\n",
    "                break\n",
    "            inputs,length = prepare_input(ids,text,feature_text)\n",
    "            current_offset = create_label(text)\n",
    "            test_ids.append(ids)\n",
    "            test_text.append(text)\n",
    "            test_input_ids.append(inputs['input_ids'].tolist())\n",
    "            test_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "            test_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "            test_offset.append(current_offset)\n",
    "        \n",
    "        if len(test_text) == 0:\n",
    "            continue\n",
    "\n",
    "        test_dataset = TestDataset(test_ids,\\\n",
    "                   test_text,\\\n",
    "                   test_input_ids,\\\n",
    "                   test_offset,\\\n",
    "                   test_token_type_ids,\\\n",
    "                   test_attention_mask)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=4,shuffle=False)\n",
    "\n",
    "        current_result = []\n",
    "        for batch_ids,batch_text,batch_input_ids,batch_offset,batch_token_type_ids,batch_attention_mask in test_loader:\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "            batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                logit = model(input_ids=batch_input_ids,token_type_ids=batch_token_type_ids,\\\n",
    "                              attention_mask=batch_attention_mask)\n",
    "                logit = torch.sigmoid(logit)\n",
    "                preds = get_char_probs(batch_text,batch_offset,\\\n",
    "                                       logit.cpu())\n",
    "                for data in preds:\n",
    "                    current_result.append(data)\n",
    "                current_result = get_results(batch_text,current_result)\n",
    "        final_result.append(current_result)\n",
    "        #results = get_results(test_text,final_result)\n",
    "        for data in final_result:\n",
    "            if len(data) != 0:\n",
    "                flag = True\n",
    "        if flag == True:\n",
    "            for index1 in range(13):\n",
    "            #每次增加的id\n",
    "                data1 = str(data1)\n",
    "                r1 = data1.rjust(5,'0')\n",
    "                data2 = str(index1)\n",
    "                r2 = data2.rjust(3,'0')\n",
    "                s = {\"id\":r1+'_'+r2,\\\n",
    "                     \"case_num\":0,\\\n",
    "                     \"pn_num\":int(r1),\\\n",
    "                     \"feature_num\":int(r2)}\n",
    "                pseudo_labeling = pseudo_labeling.append(s,ignore_index=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7a3990",
   "metadata": {},
   "source": [
    "## 增加标签8部分内容\n",
    "id:80039~84366\n",
    "长度:10802~12601\n",
    "总的次数:(12601-10802)/36=50\n",
    "间隔id:(84366-80039)/50=87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc42e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(50):\n",
    "    data1 = 80087+index*87\n",
    "    #******判斷是否能預測出內容\n",
    "    while 1:\n",
    "        flag = False\n",
    "        final_result = []\n",
    "        data1 = data1+1\n",
    "        while data1 in values:\n",
    "            data1 = data1+1\n",
    "        #for index1 in range(13):\n",
    "        #每次增加的id\n",
    "        index1 = 0\n",
    "        new_data1 = str(data1)\n",
    "        r1 = new_data1.rjust(5,'0')\n",
    "        new_data2 = str(index1)\n",
    "        r2 = new_data2.rjust(3,'0')\n",
    "        s = {\"id\":r1+'_'+r2,\\\n",
    "             \"case_num\":0,\\\n",
    "             \"pn_num\":int(r1),\\\n",
    "             \"feature_num\":int(r2)}\n",
    "        #只判断0位置\n",
    "        current_data = pd.DataFrame(s,index=[0])\n",
    "        current_data = current_data.merge(features,on=['feature_num','case_num'],how='left')\n",
    "        current_data = current_data.merge(patient_notes,on=['pn_num','case_num'],how='left')\n",
    "        test_ids = []\n",
    "        test_text,test_input_ids,test_token_type_ids = [],[],[]\n",
    "        test_attention_mask,test_offset,test_label = [],[],[]\n",
    "        for  index,data  in  current_data.iterrows():\n",
    "            #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "            ids = data['id']\n",
    "            text = data['pn_history']\n",
    "            feature_text = data['feature_text']\n",
    "            if type(text) != str and math.isnan(text):\n",
    "                break\n",
    "            inputs,length = prepare_input(ids,text,feature_text)\n",
    "            current_offset = create_label(text)\n",
    "            test_ids.append(ids)\n",
    "            test_text.append(text)\n",
    "            test_input_ids.append(inputs['input_ids'].tolist())\n",
    "            test_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "            test_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "            test_offset.append(current_offset)\n",
    "        \n",
    "        if len(test_text) == 0:\n",
    "            continue\n",
    "\n",
    "        test_dataset = TestDataset(test_ids,\\\n",
    "                   test_text,\\\n",
    "                   test_input_ids,\\\n",
    "                   test_offset,\\\n",
    "                   test_token_type_ids,\\\n",
    "                   test_attention_mask)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=4,shuffle=False)\n",
    "\n",
    "        current_result = []\n",
    "        for batch_ids,batch_text,batch_input_ids,batch_offset,batch_token_type_ids,batch_attention_mask in test_loader:\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "            batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                logit = model(input_ids=batch_input_ids,token_type_ids=batch_token_type_ids,\\\n",
    "                              attention_mask=batch_attention_mask)\n",
    "                logit = torch.sigmoid(logit)\n",
    "                preds = get_char_probs(batch_text,batch_offset,\\\n",
    "                                       logit.cpu())\n",
    "                for data in preds:\n",
    "                    current_result.append(data)\n",
    "                current_result = get_results(batch_text,current_result)\n",
    "        final_result.append(current_result)\n",
    "        #results = get_results(test_text,final_result)\n",
    "        for data in final_result:\n",
    "            if len(data) != 0:\n",
    "                flag = True\n",
    "        if flag == True:\n",
    "            for index1 in range(13):\n",
    "            #每次增加的id\n",
    "                data1 = str(data1)\n",
    "                r1 = data1.rjust(5,'0')\n",
    "                data2 = str(index1)\n",
    "                r2 = data2.rjust(3,'0')\n",
    "                s = {\"id\":r1+'_'+r2,\\\n",
    "                     \"case_num\":0,\\\n",
    "                     \"pn_num\":int(r1),\\\n",
    "                     \"feature_num\":int(r2)}\n",
    "                pseudo_labeling = pseudo_labeling.append(s,ignore_index=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997b1da4",
   "metadata": {},
   "source": [
    "## 增加标签9部分内容\n",
    "id:90127~95333\n",
    "长度:12602~14301\n",
    "总的次数(14301-12602)/36=47\n",
    "间隔id(95333-90127)/47=111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c038808",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(47):\n",
    "    data1 = 90111+index*111\n",
    "    #******判斷是否能預測出內容\n",
    "    while 1:\n",
    "        flag = False\n",
    "        final_result = []\n",
    "        data1 = data1+1\n",
    "        while data1 in values:\n",
    "            data1 = data1+1\n",
    "        #for index1 in range(13):\n",
    "        #每次增加的id\n",
    "        index1 = 0\n",
    "        new_data1 = str(data1)\n",
    "        r1 = new_data1.rjust(5,'0')\n",
    "        new_data2 = str(index1)\n",
    "        r2 = new_data2.rjust(3,'0')\n",
    "        s = {\"id\":r1+'_'+r2,\\\n",
    "             \"case_num\":0,\\\n",
    "             \"pn_num\":int(r1),\\\n",
    "             \"feature_num\":int(r2)}\n",
    "        #只判断0位置\n",
    "        current_data = pd.DataFrame(s,index=[0])\n",
    "        current_data = current_data.merge(features,on=['feature_num','case_num'],how='left')\n",
    "        current_data = current_data.merge(patient_notes,on=['pn_num','case_num'],how='left')\n",
    "        test_ids = []\n",
    "        test_text,test_input_ids,test_token_type_ids = [],[],[]\n",
    "        test_attention_mask,test_offset,test_label = [],[],[]\n",
    "        for  index,data  in  current_data.iterrows():\n",
    "            #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "            ids = data['id']\n",
    "            text = data['pn_history']\n",
    "            feature_text = data['feature_text']\n",
    "            if type(text) != str and math.isnan(text):\n",
    "                break\n",
    "            inputs,length = prepare_input(ids,text,feature_text)\n",
    "            current_offset = create_label(text)\n",
    "            test_ids.append(ids)\n",
    "            test_text.append(text)\n",
    "            test_input_ids.append(inputs['input_ids'].tolist())\n",
    "            test_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "            test_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "            test_offset.append(current_offset)\n",
    "        \n",
    "        if len(test_text) == 0:\n",
    "            continue\n",
    "\n",
    "        test_dataset = TestDataset(test_ids,\\\n",
    "                   test_text,\\\n",
    "                   test_input_ids,\\\n",
    "                   test_offset,\\\n",
    "                   test_token_type_ids,\\\n",
    "                   test_attention_mask)\n",
    "        test_loader = DataLoader(test_dataset,batch_size=4,shuffle=False)\n",
    "\n",
    "        current_result = []\n",
    "        for batch_ids,batch_text,batch_input_ids,batch_offset,batch_token_type_ids,batch_attention_mask in test_loader:\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "            batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                logit = model(input_ids=batch_input_ids,token_type_ids=batch_token_type_ids,\\\n",
    "                              attention_mask=batch_attention_mask)\n",
    "                logit = torch.sigmoid(logit)\n",
    "                preds = get_char_probs(batch_text,batch_offset,\\\n",
    "                                       logit.cpu())\n",
    "                for data in preds:\n",
    "                    current_result.append(data)\n",
    "                current_result = get_results(batch_text,current_result)\n",
    "        final_result.append(current_result)\n",
    "        #results = get_results(test_text,final_result)\n",
    "        for data in final_result:\n",
    "            if len(data) != 0:\n",
    "                flag = True\n",
    "        if flag == True:\n",
    "            for index1 in range(13):\n",
    "            #每次增加的id\n",
    "                data1 = str(data1)\n",
    "                r1 = data1.rjust(5,'0')\n",
    "                data2 = str(index1)\n",
    "                r2 = data2.rjust(3,'0')\n",
    "                s = {\"id\":r1+'_'+r2,\\\n",
    "                     \"case_num\":0,\\\n",
    "                     \"pn_num\":int(r1),\\\n",
    "                     \"feature_num\":int(r2)}\n",
    "                pseudo_labeling = pseudo_labeling.append(s,ignore_index=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_labeling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da70bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_labeling.to_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/pseudo_labeling.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
