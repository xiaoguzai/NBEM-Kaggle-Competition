{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d716bb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 42146/42146 [00:12<00:00, 3344.89it/s]\n",
      "100%|██████████████████████████████████████| 143/143 [00:00<00:00, 27107.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len = 354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|                                                 | 0/11440 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|███████████████████████████████████| 11440/11440 [00:11<00:00, 1016.11it/s]\n",
      "100%|█████████████████████████████████████| 2860/2860 [00:02<00:00, 1007.41it/s]\n",
      "Some weights of the model checkpoint at /home/xiaoguzai/模型/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  4%|█▌                                        | 27/715 [00:02<01:06, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id = \n",
      "['00970_009', '00970_010', '00970_011', '00970_012']\n",
      "...........\n",
      "label_result[index] = \n",
      "[[98, 117], [98, 103], [123, 129]]\n",
      "pred_result[index] = \n",
      "[[98, 116], [123, 129]]\n",
      "pred batch_text = \n",
      "heart is 'pounding#\n",
      "truth batch_text = \n",
      "heart is 'pounding'#\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████▋                                     | 79/715 [00:07<01:01, 10.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id = \n",
      "['10131_109', '10131_110', '10131_111', '10131_112']\n",
      "...........\n",
      "label_result[index] = \n",
      "[[49, 53], [55, 61]]\n",
      "pred_result[index] = \n",
      "[[54, 61]]\n",
      "pred batch_text = \n",
      "x10 hrs#\n",
      "batch_text = \n",
      "10 hrs\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█████▊                                   | 101/715 [00:09<00:59, 10.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id = \n",
      "['10460_102', '10460_103', '10460_104', '10460_105']\n",
      "...........\n",
      "label_result[index] = \n",
      "[[341, 349], [351, 365]]\n",
      "pred_result[index] = \n",
      "[[321, 350]]\n",
      "pred batch_text = \n",
      "loose brown stools (diarrhea)#\n",
      "truth batch_text = \n",
      "diarrhea#\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████▋                                 | 135/715 [00:13<00:56, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id = \n",
      "['20169_208', '20169_209', '20169_210', '20169_211']\n",
      "...........\n",
      "label_result[index] = \n",
      "[[25, 26]]\n",
      "pred_result[index] = \n",
      "[[24, 26]]\n",
      "pred batch_text = \n",
      "OF#\n",
      "batch_text = \n",
      "F\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████▏                              | 177/715 [00:16<00:50, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id = \n",
      "['21391_210', '21391_211', '21391_212', '21391_213']\n",
      "...........\n",
      "label_result[index] = \n",
      "[[189, 203], [199, 216]]\n",
      "pred_result[index] = \n",
      "[[189, 217]]\n",
      "pred batch_text = \n",
      "irregular flow (heavy/light)#\n",
      "truth batch_text = \n",
      "flow (heavy/light#\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████▍                         | 270/715 [00:26<00:43, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id = \n",
      "['34873_315', '35187_300', '35187_301', '35187_302']\n",
      "...........\n",
      "label_result[index] = \n",
      "[[17, 28], [111, 117]]\n",
      "pred_result[index] = \n",
      "[[17, 28], [111, 118]]\n",
      "pred batch_text = \n",
      "35 y.o.#\n",
      "truth batch_text = \n",
      "35 y.o#\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|█████████████████▎                       | 302/715 [00:29<00:40, 10.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id = \n",
      "['39654_315', '40085_400', '40085_401', '40085_402']\n",
      "...........\n",
      "label_result[index] = \n",
      "[[13, 20]]\n",
      "pred_result[index] = \n",
      "[[13, 19]]\n",
      "pred batch_text = \n",
      "35 y.o#\n",
      "truth batch_text = \n",
      "35 y.o.#\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|████████████████████▊                    | 362/715 [00:34<00:33, 10.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id = \n",
      "['50877_507', '50877_508', '50877_509', '50877_510']\n",
      "...........\n",
      "label_result[index] = \n",
      "[[205, 241]]\n",
      "pred_result[index] = \n",
      "[[205, 242]]\n",
      "pred batch_text = \n",
      "sensation that she \"was going to die\"#\n",
      "truth batch_text = \n",
      "sensation that she \"was going to die#\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|███████████████████████████▎             | 476/715 [00:45<00:22, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id = \n",
      "['60951_603', '60951_604', '60951_605', '60951_606']\n",
      "...........\n",
      "label_result[index] = \n",
      "[[205, 233], [317, 319], [332, 360]]\n",
      "pred_result[index] = \n",
      "[[205, 233], [317, 319], [332, 359]]\n",
      "pred batch_text = \n",
      "deep inhalation is a 8-9/10#\n",
      "truth batch_text = \n",
      "deep inhalation is a 8-9/10.#\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|█████████████████████████████████▏       | 578/715 [00:55<00:12, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_id = \n",
      "['81538_815', '81538_816', '81538_817', '81687_800']\n",
      "...........\n",
      "label_result[index] = \n",
      "[[13, 29]]\n",
      "pred_result[index] = \n",
      "[[13, 30]]\n",
      "pred batch_text = \n",
      "SLEEPING PROBLEMX#\n",
      "truth batch_text = \n",
      "SLEEPING PROBLEM#\n",
      "=====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 715/715 [01:08<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point = \n",
      "0.8818631806066759\n"
     ]
    }
   ],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "# This must be done before importing transformers\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "transformers_path = Path(\"/home/xiaoguzai/.local/lib/python3.9/site-packages/transformers\")\n",
    "\n",
    "input_dir = Path(\"/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/代码\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "#print('convert_file = ')\n",
    "#print(convert_file)\n",
    "#print('conversion_path = ')\n",
    "#print(conversion_path)\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path \n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "    filepath = deberta_v2_path/filename\n",
    "    \n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)\n",
    "    \n",
    "from transformers import AutoTokenizer,AutoModel,AutoConfig\n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "import transformers\n",
    "tokenizer = DebertaV2TokenizerFast.from_pretrained('/home/xiaoguzai/模型/deberta-v3-large')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "train = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/train.csv')\n",
    "\n",
    "train['annotation'] = train['annotation'].apply(ast.literal_eval)\n",
    "train['location'] = train['location'].apply(ast.literal_eval)\n",
    "features = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/features.csv')\n",
    "features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "\n",
    "patient_notes = pd.read_csv('/home/xiaoguzai/数据/NBME-Score Clinical Patient Notes/patient_notes.csv')\n",
    "train = train.merge(features, on=['feature_num','case_num'],how='left')\n",
    "train.head()\n",
    "\n",
    "train = train.merge(patient_notes, on=['pn_num','case_num'],how='left')\n",
    "train.head()\n",
    "#这里数据读取有bug，一个history被读取了好多次\n",
    "\n",
    "\n",
    "import ast\n",
    "# incorrect annotation\n",
    "train.loc[338, 'annotation'] = ast.literal_eval('[[\"father heart attack\"]]')\n",
    "train.loc[338, 'location'] = ast.literal_eval('[[\"764 783\"]]')\n",
    "\n",
    "train.loc[621, 'annotation'] = ast.literal_eval('[[\"for the last 2-3 months\"]]')\n",
    "train.loc[621, 'location'] = ast.literal_eval('[[\"77 100\"]]')\n",
    "\n",
    "train.loc[655, 'annotation'] = ast.literal_eval('[[\"no heat intolerance\"], [\"no cold intolerance\"]]')\n",
    "train.loc[655, 'location'] = ast.literal_eval('[[\"285 292;301 312\"], [\"285 287;296 312\"]]')\n",
    "\n",
    "train.loc[1262, 'annotation'] = ast.literal_eval('[[\"mother thyroid problem\"]]')\n",
    "train.loc[1262, 'location'] = ast.literal_eval('[[\"551 557;565 580\"]]')\n",
    "\n",
    "train.loc[1265, 'annotation'] = ast.literal_eval('[[\\'felt like he was going to \"pass out\"\\']]')\n",
    "train.loc[1265, 'location'] = ast.literal_eval('[[\"131 135;181 212\"]]')\n",
    "\n",
    "train.loc[1396, 'annotation'] = ast.literal_eval('[[\"stool , with no blood\"]]')\n",
    "train.loc[1396, 'location'] = ast.literal_eval('[[\"259 280\"]]')\n",
    "\n",
    "train.loc[1591, 'annotation'] = ast.literal_eval('[[\"diarrhoe non blooody\"]]')\n",
    "train.loc[1591, 'location'] = ast.literal_eval('[[\"176 184;201 212\"]]')\n",
    "\n",
    "train.loc[1615, 'annotation'] = ast.literal_eval('[[\"diarrhea for last 2-3 days\"]]')\n",
    "train.loc[1615, 'location'] = ast.literal_eval('[[\"249 257;271 288\"]]')\n",
    "\n",
    "train.loc[1664, 'annotation'] = ast.literal_eval('[[\"no vaginal discharge\"]]')\n",
    "train.loc[1664, 'location'] = ast.literal_eval('[[\"822 824;907 924\"]]')\n",
    "\n",
    "train.loc[1714, 'annotation'] = ast.literal_eval('[[\"started about 8-10 hours ago\"]]')\n",
    "train.loc[1714, 'location'] = ast.literal_eval('[[\"101 129\"]]')\n",
    "\n",
    "train.loc[1929, 'annotation'] = ast.literal_eval('[[\"no blood in the stool\"]]')\n",
    "train.loc[1929, 'location'] = ast.literal_eval('[[\"531 539;549 561\"]]')\n",
    "\n",
    "train.loc[2134, 'annotation'] = ast.literal_eval('[[\"last sexually active 9 months ago\"]]')\n",
    "train.loc[2134, 'location'] = ast.literal_eval('[[\"540 560;581 593\"]]')\n",
    "\n",
    "train.loc[2191, 'annotation'] = ast.literal_eval('[[\"right lower quadrant pain\"]]')\n",
    "train.loc[2191, 'location'] = ast.literal_eval('[[\"32 57\"]]')\n",
    "\n",
    "train.loc[2553, 'annotation'] = ast.literal_eval('[[\"diarrhoea no blood\"]]')\n",
    "train.loc[2553, 'location'] = ast.literal_eval('[[\"308 317;376 384\"]]')\n",
    "\n",
    "train.loc[3124, 'annotation'] = ast.literal_eval('[[\"sweating\"]]')\n",
    "train.loc[3124, 'location'] = ast.literal_eval('[[\"549 557\"]]')\n",
    "\n",
    "train.loc[3858, 'annotation'] = ast.literal_eval('[[\"previously as regular\"], [\"previously eveyr 28-29 days\"], [\"previously lasting 5 days\"], [\"previously regular flow\"]]')\n",
    "train.loc[3858, 'location'] = ast.literal_eval('[[\"102 123\"], [\"102 112;125 141\"], [\"102 112;143 157\"], [\"102 112;159 171\"]]')\n",
    "\n",
    "train.loc[4373, 'annotation'] = ast.literal_eval('[[\"for 2 months\"]]')\n",
    "train.loc[4373, 'location'] = ast.literal_eval('[[\"33 45\"]]')\n",
    "\n",
    "train.loc[4763, 'annotation'] = ast.literal_eval('[[\"35 year old\"]]')\n",
    "train.loc[4763, 'location'] = ast.literal_eval('[[\"5 16\"]]')\n",
    "\n",
    "train.loc[4782, 'annotation'] = ast.literal_eval('[[\"darker brown stools\"]]')\n",
    "train.loc[4782, 'location'] = ast.literal_eval('[[\"175 194\"]]')\n",
    "\n",
    "train.loc[4908, 'annotation'] = ast.literal_eval('[[\"uncle with peptic ulcer\"]]')\n",
    "train.loc[4908, 'location'] = ast.literal_eval('[[\"700 723\"]]')\n",
    "\n",
    "train.loc[6016, 'annotation'] = ast.literal_eval('[[\"difficulty falling asleep\"]]')\n",
    "train.loc[6016, 'location'] = ast.literal_eval('[[\"225 250\"]]')\n",
    "\n",
    "train.loc[6192, 'annotation'] = ast.literal_eval('[[\"helps to take care of aging mother and in-laws\"]]')\n",
    "train.loc[6192, 'location'] = ast.literal_eval('[[\"197 218;236 260\"]]')\n",
    "\n",
    "train.loc[6380, 'annotation'] = ast.literal_eval('[[\"No hair changes\"], [\"No skin changes\"], [\"No GI changes\"], [\"No palpitations\"], [\"No excessive sweating\"]]')\n",
    "train.loc[6380, 'location'] = ast.literal_eval('[[\"480 482;507 519\"], [\"480 482;499 503;512 519\"], [\"480 482;521 531\"], [\"480 482;533 545\"], [\"480 482;564 582\"]]')\n",
    "\n",
    "train.loc[6562, 'annotation'] = ast.literal_eval('[[\"stressed due to taking care of her mother\"], [\"stressed due to taking care of husbands parents\"]]')\n",
    "train.loc[6562, 'location'] = ast.literal_eval('[[\"290 320;327 337\"], [\"290 320;342 358\"]]')\n",
    "\n",
    "train.loc[6862, 'annotation'] = ast.literal_eval('[[\"stressor taking care of many sick family members\"]]')\n",
    "train.loc[6862, 'location'] = ast.literal_eval('[[\"288 296;324 363\"]]')\n",
    "\n",
    "train.loc[7022, 'annotation'] = ast.literal_eval('[[\"heart started racing and felt numbness for the 1st time in her finger tips\"]]')\n",
    "train.loc[7022, 'location'] = ast.literal_eval('[[\"108 182\"]]')\n",
    "\n",
    "train.loc[7422, 'annotation'] = ast.literal_eval('[[\"first started 5 yrs\"]]')\n",
    "train.loc[7422, 'location'] = ast.literal_eval('[[\"102 121\"]]')\n",
    "\n",
    "train.loc[8876, 'annotation'] = ast.literal_eval('[[\"No shortness of breath\"]]')\n",
    "train.loc[8876, 'location'] = ast.literal_eval('[[\"481 483;533 552\"]]')\n",
    "\n",
    "train.loc[9027, 'annotation'] = ast.literal_eval('[[\"recent URI\"], [\"nasal stuffines, rhinorrhea, for 3-4 days\"]]')\n",
    "train.loc[9027, 'location'] = ast.literal_eval('[[\"92 102\"], [\"123 164\"]]')\n",
    "\n",
    "train.loc[9938, 'annotation'] = ast.literal_eval('[[\"irregularity with her cycles\"], [\"heavier bleeding\"], [\"changes her pad every couple hours\"]]')\n",
    "train.loc[9938, 'location'] = ast.literal_eval('[[\"89 117\"], [\"122 138\"], [\"368 402\"]]')\n",
    "\n",
    "train.loc[9973, 'annotation'] = ast.literal_eval('[[\"gaining 10-15 lbs\"]]')\n",
    "train.loc[9973, 'location'] = ast.literal_eval('[[\"344 361\"]]')\n",
    "\n",
    "train.loc[10513, 'annotation'] = ast.literal_eval('[[\"weight gain\"], [\"gain of 10-16lbs\"]]')\n",
    "train.loc[10513, 'location'] = ast.literal_eval('[[\"600 611\"], [\"607 623\"]]')\n",
    "\n",
    "train.loc[11551, 'annotation'] = ast.literal_eval('[[\"seeing her son knows are not real\"]]')\n",
    "train.loc[11551, 'location'] = ast.literal_eval('[[\"386 400;443 461\"]]')\n",
    "\n",
    "train.loc[11677, 'annotation'] = ast.literal_eval('[[\"saw him once in the kitchen after he died\"]]')\n",
    "train.loc[11677, 'location'] = ast.literal_eval('[[\"160 201\"]]')\n",
    "\n",
    "train.loc[12124, 'annotation'] = ast.literal_eval('[[\"tried Ambien but it didnt work\"]]')\n",
    "train.loc[12124, 'location'] = ast.literal_eval('[[\"325 337;349 366\"]]')\n",
    "\n",
    "train.loc[12279, 'annotation'] = ast.literal_eval('[[\"heard what she described as a party later than evening these things did not actually happen\"]]')\n",
    "train.loc[12279, 'location'] = ast.literal_eval('[[\"405 459;488 524\"]]')\n",
    "\n",
    "train.loc[12289, 'annotation'] = ast.literal_eval('[[\"experienced seeing her son at the kitchen table these things did not actually happen\"]]')\n",
    "train.loc[12289, 'location'] = ast.literal_eval('[[\"353 400;488 524\"]]')\n",
    "\n",
    "train.loc[13238, 'annotation'] = ast.literal_eval('[[\"SCRACHY THROAT\"], [\"RUNNY NOSE\"]]')\n",
    "train.loc[13238, 'location'] = ast.literal_eval('[[\"293 307\"], [\"321 331\"]]')\n",
    "\n",
    "train.loc[13297, 'annotation'] = ast.literal_eval('[[\"without improvement when taking tylenol\"], [\"without improvement when taking ibuprofen\"]]')\n",
    "train.loc[13297, 'location'] = ast.literal_eval('[[\"182 221\"], [\"182 213;225 234\"]]')\n",
    "\n",
    "train.loc[13299, 'annotation'] = ast.literal_eval('[[\"yesterday\"], [\"yesterday\"]]')\n",
    "train.loc[13299, 'location'] = ast.literal_eval('[[\"79 88\"], [\"409 418\"]]')\n",
    "\n",
    "train.loc[13845, 'annotation'] = ast.literal_eval('[[\"headache global\"], [\"headache throughout her head\"]]')\n",
    "train.loc[13845, 'location'] = ast.literal_eval('[[\"86 94;230 236\"], [\"86 94;237 256\"]]')\n",
    "\n",
    "train.loc[14083, 'annotation'] = ast.literal_eval('[[\"headache generalized in her head\"]]')\n",
    "train.loc[14083, 'location'] = ast.literal_eval('[[\"56 64;156 179\"]]')\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=42)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "Fold = GroupKFold(n_splits=5)\n",
    "groups = train['pn_num'].values\n",
    "for n, (train_index,val_index) in enumerate(Fold.split(train,train['location'],groups)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "#按照groups也就是train['pn_num']进行划分\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "train['annotation_length'] = train['annotation'].apply(len)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "pn_history_lengths = []\n",
    "for text in tqdm(patient_notes['pn_history'].fillna(\"\").values,total=len(patient_notes)):\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    pn_history_lengths.append(length)\n",
    "    \n",
    "features_lengths = []\n",
    "for text in tqdm(features['feature_text'].fillna(\"\").values, total=len(features)):\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    features_lengths.append(length)\n",
    "max_len = max(pn_history_lengths)+max(features_lengths)+3\n",
    "print('max_len = %d'%max_len)\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,inputs,labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.tensors = [torch.tensor(self.inputs),\\\n",
    "                       torch.tensor(self.labels)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "\n",
    "max_len = 360\n",
    "#下面prepare_input的时候将text和feature_text合在一起很巧妙\n",
    "def prepare_input(ids, text, feature_text):\n",
    "    #inputs1 = tokenizer.encode_plus(text,\\\n",
    "    #                               add_special_tokens=True,\\\n",
    "    #                               max_length = max_len,\\\n",
    "    #                               paddin)\n",
    "    inputs = {}\n",
    "    inputs1 = tokenizer.encode_plus(text,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   max_length = max_len,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs2 = tokenizer.encode_plus(feature_text,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   max_length = max_len,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    ids1,ids2 = ids.split('_')\n",
    "    inputs3 = tokenizer.encode_plus(ids1,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs4 = tokenizer.encode_plus(ids2,\\\n",
    "                                   add_special_tokens=True,\\\n",
    "                                   return_offsets_mapping = False)\n",
    "    inputs2['input_ids'] = inputs2['input_ids']+inputs3['input_ids'][1:]+inputs4['input_ids'][1:]\n",
    "    inputs2['attention_mask'] = inputs2['attention_mask']+inputs3['attention_mask'][1:]+inputs4['attention_mask'][1:]\n",
    "    inputs2['token_type_ids'] = inputs2['token_type_ids']+inputs3['token_type_ids'][1:]+inputs4['token_type_ids'][1:]\n",
    "    \n",
    "    if len(inputs1['input_ids'])+len(inputs2['input_ids'])-2 > max_len:\n",
    "        exceed_length = len(inputs1['input_ids'])+len(inputs2['input_ids'])-max_len-2\n",
    "        inputs['input_ids'] = inputs1['input_ids'][:len(inputs1['input_ids'])-exceed_length-1]+inputs2['input_ids'][1:]\n",
    "        inputs['attention_mask'] = [1]*max_len\n",
    "    else:\n",
    "        inputs['input_ids'] = inputs1['input_ids'][:-1]+inputs2['input_ids'][1:-1]\n",
    "        inputs['input_ids'] = inputs['input_ids']+[tokenizer.sep_token_id]\n",
    "        #inputs['input_ids'] = inputs['input_ids']+[tokenizer.sep_token_id]+[tokenizer.pad_token_id]*(max_len-len(inputs['input_ids'])-1)\n",
    "        inputs['attention_mask'] = [1]*(len(inputs['input_ids']))\n",
    "        inputs['attention_mask'] = inputs['attention_mask']+[0]*(max_len-len(inputs['input_ids']))\n",
    "        inputs['input_ids'] = inputs['input_ids']+[tokenizer.pad_token_id]*(max_len-len(inputs['input_ids']))\n",
    "        inputs['token_type_ids'] = [0]*max_len\n",
    "        #inputs['attention_mask'] = [1]*max_len\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v,dtype=torch.long)\n",
    "    return inputs,len(inputs['input_ids'])\n",
    "\n",
    "#打标记的时候还是只放入text的内容，不考虑feature_text的文本内容\n",
    "def create_label(text, annotation_length, location_list):\n",
    "    encoded = tokenizer.encode_plus(text,\\\n",
    "                                add_special_tokens=True,\\\n",
    "                                max_length = max_len,\\\n",
    "                                padding = \"max_length\",\\\n",
    "                                return_offsets_mapping = True)\n",
    "    offset_mapping = encoded['offset_mapping']\n",
    "    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0]\n",
    "    label = np.zeros(len(offset_mapping))\n",
    "    label[ignore_idxes] = -1\n",
    "    if annotation_length != 0:\n",
    "        for location in location_list:\n",
    "            #location = 2 4,location = 8 10\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                #loc = ['2','4'],loc = ['8','10']\n",
    "                start_idx = -1\n",
    "                end_idx = -1\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                #start = 2,end = 4;start = 8,end = 10;\n",
    "                #!!!这里的start,end标记的为字符:Character spans indicating \n",
    "                #the location of each annotation within the note.\n",
    "                #注意前面的标注Character spans\n",
    "                for idx in range(len(offset_mapping)):\n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                        start_idx = idx - 1\n",
    "                        #print('111start_idx = %d 111'%start_idx)\n",
    "                        #字符比当前字符小的时候，指向前一位\n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                        end_idx = idx + 1\n",
    "                        #字符比当前字符大的时候，指向后一位\n",
    "                        #print('222start_idx = %d 222'%end_idx)\n",
    "                if start_idx == -1:\n",
    "                    start_idx = end_idx\n",
    "                    #print('333start_idx = %d 333'%start_idx)\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    #print('***start_idx = %d***'%start_idx)\n",
    "                    #print('***end_idx = %d***'%end_idx)\n",
    "                    label[start_idx:end_idx] = 1\n",
    "    return offset_mapping,label\n",
    "\n",
    "train.to_csv('current_train.csv')\n",
    "\n",
    "\n",
    "def change_location_to_offset(text,location_list):\n",
    "    results = np.zeros(len(text))\n",
    "    #以char为级别计算，应该对整个text计算len\n",
    "    for idx, offset_mapping in enumerate(location_list):\n",
    "        try:\n",
    "            start = (int)(offset_mapping[0])\n",
    "            end = (int)(offset_mapping[1])\n",
    "            results[start:end] = 1\n",
    "        except:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "    \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        self.model = model\n",
    "        #self.dropout = nn.Dropout(0.2)\n",
    "        #self.fc1 = nn.Linear(768,1)\n",
    "        self.fc1 = nn.Linear(1024,1)\n",
    "        #self.fc1 = nn.Linear(768,1)\n",
    "        \n",
    "    def forward(self,input_ids,token_type_ids,attention_mask):\n",
    "        outputs = self.model(input_ids=input_ids,\\\n",
    "                           token_type_ids=token_type_ids,\\\n",
    "                           attention_mask=attention_mask)\n",
    "        outputs = outputs[0]\n",
    "        #outputs = self.dropout(outputs)\n",
    "        outputs = self.fc1(outputs)\n",
    "        return outputs\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def my_collate(batch):\n",
    "    id_list = []\n",
    "    text_list,input_ids_list,offset_list = [],[],[]\n",
    "    token_type_ids_list,attention_mask_list,origin_label_list = [],[],[]\n",
    "    for data in batch:\n",
    "        id_list.append(data[0])\n",
    "        text_list.append(data[1])\n",
    "        input_ids_list.append(data[2].tolist())\n",
    "        offset_list.append(data[3])\n",
    "        token_type_ids_list.append(data[4].tolist())\n",
    "        attention_mask_list.append(data[5].tolist())\n",
    "        #current_data_list = get_predictions(data[5])\n",
    "        current_data_list = []\n",
    "        for data1 in data[6]:\n",
    "            if ' ' in data1:\n",
    "                for data2 in data1.split(';'):\n",
    "                    data3 = data2.split(' ')\n",
    "                    current_data_list.append([(int)(data3[0]),(int)(data3[1])])\n",
    "        origin_label_list.append(current_data_list)\n",
    "    input_ids_list = torch.tensor(input_ids_list)\n",
    "    token_type_ids_list = torch.tensor(token_type_ids_list)\n",
    "    attention_mask_list = torch.tensor(attention_mask_list)\n",
    "    return id_list,text_list,input_ids_list,offset_list,\\\n",
    "           token_type_ids_list,attention_mask_list,origin_label_list\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,text,input_ids,offset,token_type_ids,attention_mask,label):\n",
    "        self.input_ids = input_ids\n",
    "        self.tensors = [text,\\\n",
    "                        torch.tensor(input_ids,dtype=torch.long),\\\n",
    "                        torch.tensor(offset),\\\n",
    "                       torch.tensor(token_type_ids,dtype=torch.long),\\\n",
    "                       torch.tensor(attention_mask,dtype=torch.long),\\\n",
    "                       torch.tensor(label)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "\n",
    "class ValidDataset(Dataset):\n",
    "    def __init__(self,ids,text,input_ids,offset,token_type_ids,attention_mask,origin_label):\n",
    "        self.ids = ids\n",
    "        self.input_ids = input_ids\n",
    "        self.tensors = [ids,\\\n",
    "                        text,\\\n",
    "                        torch.tensor(input_ids,dtype=torch.long),\\\n",
    "                        torch.tensor(offset),\\\n",
    "                        torch.tensor(token_type_ids,dtype=torch.long),\\\n",
    "                        torch.tensor(attention_mask,dtype=torch.long),\\\n",
    "                        origin_label]\n",
    "        #这里origin_label放入的为['']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "    \n",
    "def compute_multilabel_loss(model,batch_token_ids,\\\n",
    "                            batch_token_type_ids,\\\n",
    "                            batch_attention_mask,\\\n",
    "                            batch_label):\n",
    "    with torch.cuda.amp.autocast(enabled=True):\n",
    "        logit = model(input_ids=batch_token_ids,\\\n",
    "                     attention_mask=batch_attention_mask,\\\n",
    "                     token_type_ids=batch_token_type_ids)\n",
    "    logit = logit.view(-1,1)\n",
    "    batch_label = batch_label.view(-1,1)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    loss = loss_fn(logit,batch_label)\n",
    "    torch.set_printoptions(threshold=np.inf)\n",
    "    loss = torch.masked_select(loss,batch_label!=-1)\n",
    "    loss = loss.mean()\n",
    "    #这里的loss不要勿写成logit\n",
    "    return loss\n",
    "\n",
    "def get_char_probs(total_text,offsets,predictions):\n",
    "    results = [np.zeros(len(t)) for t in total_text]\n",
    "    #!!!results 长短不一!!!\n",
    "    #以char为级别计算，应该对整个text计算len\n",
    "    torch.set_printoptions(threshold=np.inf)\n",
    "    for i, (offset, prediction) in enumerate(zip(offsets, predictions)):\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(offset, prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            #results[i][start:end] = ((float)(pred[0].item(),)\n",
    "            results[i][start:end] = pred[0].item()\n",
    "    return results\n",
    "\n",
    "from transformers import DebertaV2Model,DebertaModel\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import itertools\n",
    "deberta = DebertaV2Model.from_pretrained(\"/home/xiaoguzai/模型/deberta-v3-large\")\n",
    "#deberta = DebertaModel.from_pretrained(\"/home/xiaoguzai/模型/deberta\")\n",
    "model = ClassificationModel(deberta)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def get_results(test_text,char_probs, th=0.5):\n",
    "    results = []\n",
    "    #for char_prob in char_probs:\n",
    "    for index in range(len(char_probs)):\n",
    "        char_prob = char_probs[index]\n",
    "        char_text = test_text[index]\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        #result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = [[min(r),max(r)] for r in result]\n",
    "        \n",
    "        for index1 in range(len(result)):\n",
    "            if result[index1][0]-1 >= 0 and char_text[result[index1][0]-1] != ' ':\n",
    "                result[index1][0] = result[index1][0]-1\n",
    "                #preds[index][index1][0] = preds[index][index1][0]-1\n",
    "            #if preds[index][index1][1]+1 < len(current_text) and current_text[preds[index][index1][1]+1] != ' ':\n",
    "            #    preds[index][index1][1] = preds[index][index1][1]+1\n",
    "            if result[index1][1]-1 < len(char_text) and char_text[result[index1][1]-1] == ' ' \\\n",
    "               and result[index1][1]-2 >= 0 and char_text[result[index1][1]-2] != ' ':\n",
    "                print('original result = ')\n",
    "                print(result[index1])\n",
    "                result[index1][1] = result[index1][1]-1\n",
    "                print('now result = ')\n",
    "                print(result[index1])\n",
    "                print('=============')\n",
    "        \n",
    "        result = [str(r[0])+' '+str(r[1]) for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed=42)\n",
    "\n",
    "def post_process_spaces(target, text, th=0.5):\n",
    "    target = np.copy(target)\n",
    "\n",
    "    if len(text) > len(target):\n",
    "        padding = np.zeros(len(text) - len(target))\n",
    "        target = np.concatenate([target, padding])\n",
    "    else:\n",
    "        target = target[:len(text)]\n",
    "\n",
    "    if text[0] == \" \":\n",
    "        target[0] = 0\n",
    "    if text[-1] == \" \":\n",
    "        target[-1] = 0\n",
    "\n",
    "    for i in range(1, len(text) - 1):\n",
    "        if text[i] == \" \":\n",
    "            if target[i] >= th and target[i - 1] < th:  # space before\n",
    "                target[i] = 0\n",
    "\n",
    "            if target[i] >= th and target[i + 1] < th:  # space after\n",
    "                target[i] = 0\n",
    "\n",
    "            if target[i - 1] >= th and target[i + 1] >= th:\n",
    "                target[i] = 1\n",
    "\n",
    "    return target\n",
    "\n",
    "from transformers import DebertaV2Model,DebertaModel\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import itertools    \n",
    "from tqdm import tqdm\n",
    "\n",
    "bestpointlist = [0.0,0.0,0.0,0.0,0.0]\n",
    "for current_fold in range(1):\n",
    "    train_data = train[train['fold'] != current_fold]\n",
    "    valid_data = train[train['fold'] == current_fold]\n",
    "    train_text,valid_text = [],[]\n",
    "    train_id,valid_id = [],[]\n",
    "    train_input_ids,train_token_type_ids,train_attention_mask = [],[],[]\n",
    "    train_offset,train_label = [],[]\n",
    "    train_length = []\n",
    "    valid_input_ids,valid_token_type_ids,valid_attention_mask = [],[],[]\n",
    "    valid_offset,valid_label = [],[]\n",
    "    valid_length = []\n",
    "    train_origin_label,valid_origin_label = [],[]\n",
    "\n",
    "    for  index,data  in  tqdm(train_data.iterrows(),total=len(train_data)):\n",
    "        #!!!数据这里出现bug，读取的都是一组数据!!!\n",
    "        ids = data['id']\n",
    "        text = data['pn_history']\n",
    "        feature_text = data['feature_text']\n",
    "        #print('text = ')\n",
    "        #print(text)\n",
    "        inputs,length = prepare_input(ids,text,feature_text)\n",
    "        #train_text.append(text+feature_text)\n",
    "        train_id.append(ids)\n",
    "        train_text.append(text)\n",
    "        train_input_ids.append(inputs['input_ids'].tolist())\n",
    "        train_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "        train_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "\n",
    "        annotation_length = data['annotation_length']\n",
    "\n",
    "        current_offset,current_label = create_label(text,annotation_length=data['annotation_length'],\\\n",
    "                                        location_list=data['location'])\n",
    "        true_label = change_location_to_offset(text,data['location'])\n",
    "        train_origin_label.append(true_label)\n",
    "        train_offset.append(current_offset)\n",
    "        train_label.append(current_label)\n",
    "        train_length.append(length)\n",
    "\n",
    "    for index,data in tqdm(valid_data.iterrows(),total=len(valid_data)):\n",
    "        ids = data['id']\n",
    "        text = data['pn_history']\n",
    "        feature_text = data['feature_text']\n",
    "        inputs,length = prepare_input(ids,text,feature_text)\n",
    "        #valid_text.append(text+feature_text)\n",
    "        valid_id.append(ids)\n",
    "        valid_text.append(text)\n",
    "        valid_input_ids.append(inputs['input_ids'].tolist())\n",
    "        valid_token_type_ids.append(inputs['token_type_ids'].tolist())\n",
    "        valid_attention_mask.append(inputs['attention_mask'].tolist())\n",
    "        annotation_length = data['annotation_length']\n",
    "        current_offset,current_label = create_label(text,annotation_length=data['annotation_length'],\\\n",
    "                                     location_list=data['location'])\n",
    "        #true_label = change_location_to_offset(text,data['location'])\n",
    "        #发生bug的地方，true_label的标记错误\n",
    "        valid_offset.append(current_offset)\n",
    "        valid_label.append(data['location'])\n",
    "        valid_length.append(length)\n",
    "\n",
    "    train_dataset = TrainDataset(train_text,\\\n",
    "                                 train_input_ids,\\\n",
    "                                 train_offset,\\\n",
    "                                train_token_type_ids,\\\n",
    "                                train_attention_mask,\\\n",
    "                                train_label)\n",
    "    valid_dataset = ValidDataset(valid_id,\\\n",
    "                                 valid_text,\\\n",
    "                                 valid_input_ids,\\\n",
    "                                 valid_offset,\\\n",
    "                                valid_token_type_ids,\\\n",
    "                                valid_attention_mask,\\\n",
    "                                valid_label)\n",
    "\n",
    "    batch_size = 4\n",
    "    train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset,batch_size=batch_size,shuffle=False,collate_fn = my_collate)\n",
    "    #bcewithlogitloss有sigmoid函数,batch_size必须要大\n",
    "    bestpoint = 0.0\n",
    "    gradient_accumulation_steps = 4\n",
    "    #梯度累积的步数，每训练两次增加一步\n",
    "\n",
    "    deberta = DebertaV2Model.from_pretrained(\"/home/xiaoguzai/模型/deberta-v3-large\")\n",
    "    model = ClassificationModel(deberta)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr=1e-5)\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch > 5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2/(epoch+1)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "    model = torch.load('/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/deberta模型文件/deberta_fold=0.pth')\n",
    "    for epoch in range(1):\n",
    "        r\"\"\"\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        #model = torch.load(\"/home/xiaoguzai/程序/NBME-Score Clinical Patient Notes/best_point=0.8127543174426041.pth\")\n",
    "        losses = AverageMeter()\n",
    "        #scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "        step = 0\n",
    "        prebig = True\n",
    "        for batch_text,batch_ids,batch_offset,batch_token_type_ids,batch_attention_mask,batch_label in tqdm(train_loader):\n",
    "            batch_ids = batch_ids.to(device)\n",
    "            batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            batch_label = batch_label.to(device)\n",
    "            loss = compute_multilabel_loss(model,batch_ids,\\\n",
    "                                batch_token_type_ids,\\\n",
    "                                batch_attention_mask,\\\n",
    "                                batch_label)\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss/gradient_accumulation_steps\n",
    "            losses.update(loss.item(),batch_size)\n",
    "            scaler.scale(loss).backward()\n",
    "            #loss.backward()\n",
    "            #每一次进行相应的梯度计算\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(),1000)\n",
    "            #防止梯度爆炸，超过1000的部分不予计算\n",
    "            if (step+1)%gradient_accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            step = step+1\n",
    "        scheduler.step()\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        pred_result = []\n",
    "        label_result = []\n",
    "        for batch_id,batch_text,batch_ids,batch_offset,batch_token_type_ids,batch_attention_mask,batch_origin_label in tqdm(valid_loader):\n",
    "            #print('batch_id = ')\n",
    "            #print(batch_id)\n",
    "            #print('............')\n",
    "            #print('batch_label = ')\n",
    "            #print(batch_origin_label)\n",
    "            #print('..............')\n",
    "            batch_ids = batch_ids.to(device)\n",
    "            batch_token_type_ids = batch_token_type_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                logit = model(input_ids=batch_ids,token_type_ids=batch_token_type_ids,\\\n",
    "                              attention_mask=batch_attention_mask)\n",
    "                #logit = model(input_ids=batch_ids)\n",
    "                #logit = (4,512,1)\n",
    "                #!!!这点判断需要注意应该是以字符的形式进行判断!!!\n",
    "                #输入的id应该为text+symptom，但是判断正负的时候只应该判断text的内容\n",
    "                #torch.set_printoptions(threshold=np.inf)\n",
    "                logit = torch.sigmoid(logit)\n",
    "                #加上symptom得到的正常的logit\n",
    "                preds = get_char_probs(batch_text,batch_offset,\\\n",
    "                                       logit.cpu())\n",
    "                current_preds = preds\n",
    "                r\"\"\"\n",
    "                new_preds = []\n",
    "                for index in range(len(preds)):\n",
    "                    pred_data = preds[index]\n",
    "                    text_data = batch_text[index]\n",
    "                    pred_data = post_process_spaces(pred_data,text_data)\n",
    "                    new_preds.append(pred_data)\n",
    "                preds = new_preds\n",
    "                \"\"\"\n",
    "                results = get_results(batch_text,preds,th=0.4)\n",
    "                \n",
    "                preds = get_predictions(results)\n",
    "                truths = batch_origin_label\n",
    "                r\"\"\"\n",
    "                preds = \n",
    "                [[[696, 722]], [[668, 693]], [[203, 217]], [[70, 91]]]\n",
    "                truths = \n",
    "                [[[696, 724]], [[668, 693]], [[203, 217]], [[70, 91], [176, 183]]]\n",
    "                \"\"\"\n",
    "                for data in preds:\n",
    "                    pred_result.append(data)\n",
    "                for data in truths:\n",
    "                    label_result.append(data)\n",
    "                    \n",
    "                if preds != truths:\n",
    "                    flag = False\n",
    "                    for index in range(len(preds)):\n",
    "                        if truths[index] != preds[index]:\n",
    "                            r\"\"\"\n",
    "                            print('batch_id = ')\n",
    "                            print(batch_id)\n",
    "                            print('...........')\n",
    "                            print('label_result[index] = ')\n",
    "                            print(truths[index])\n",
    "                            print('pred_result[index] = ')\n",
    "                            print(preds[index])\n",
    "                            \n",
    "                            print('pred batch_text = ')\n",
    "                            print(batch_text[index][preds[index][index2][0]:preds[index][index2][1]]+'#')\n",
    "                            print('batch_text = ')\n",
    "                            print(batch_text[index][truths[index][index3][0]:truths[index][index3][1]])\n",
    "                            print('=====================')\n",
    "                            print('current_preds = ')\n",
    "                            print(current_preds[index][preds[index][index2][0]:preds[index][index2][1]])\n",
    "                            print('=====================')\n",
    "                            \n",
    "                            print('预测结果概率')\n",
    "                            for index2 in range(len(preds[index])):\n",
    "                                print(current_preds[index][preds[index][index2][0]:preds[index][index2][1]])\n",
    "                                \n",
    "                            print('真实结果概率')\n",
    "                            for index3 in range(len(truths[index])):\n",
    "                                print(current_preds[index][truths[index][index3][0]:truths[index][index3][1]])\n",
    "                            \"\"\"\n",
    "                            for index2 in range(len(preds[index])):\n",
    "                                for index3 in range(len(truths[index])):\n",
    "                                    if abs(truths[index][index3][0]-preds[index][index2][0]) == 1:\n",
    "                                        print('batch_id = ')\n",
    "                                        print(batch_id)\n",
    "                                        print('...........')\n",
    "                                        print('label_result[index] = ')\n",
    "                                        print(truths[index])\n",
    "                                        print('pred_result[index] = ')\n",
    "                                        print(preds[index])\n",
    "                                        print('pred batch_text = ')\n",
    "                                        print(batch_text[index][preds[index][index2][0]:preds[index][index2][1]]+'#')\n",
    "                                        print('batch_text = ')\n",
    "                                        print(batch_text[index][truths[index][index3][0]:truths[index][index3][1]])\n",
    "                                        print('=====================')\n",
    "                                        #flag = True\n",
    "                                    elif abs(truths[index][index3][1]-preds[index][index2][1]) == 1:\n",
    "                                        print('batch_id = ')\n",
    "                                        print(batch_id)\n",
    "                                        print('...........')\n",
    "                                        print('label_result[index] = ')\n",
    "                                        print(truths[index])\n",
    "                                        print('pred_result[index] = ')\n",
    "                                        print(preds[index])\n",
    "                                        print('pred batch_text = ')\n",
    "                                        print(batch_text[index][preds[index][index2][0]:preds[index][index2][1]]+'#')\n",
    "                                        print('truth batch_text = ')\n",
    "                                        print(batch_text[index][truths[index][index3][0]:truths[index][index3][1]]+'#')\n",
    "                                        print('=====================')\n",
    "                                \n",
    "\n",
    "        point = get_score(pred_result,label_result)\n",
    "        print('point = ')\n",
    "        print(point)\n",
    "        if point > bestpoint:\n",
    "            bestpoint = point \n",
    "            prebig = True\n",
    "            torch.save(model,'deberta_nocapitalize_groupkfold_best_point='+str(bestpoint)+'_fold='+str(current_fold)+'.pth')\n",
    "        else:\n",
    "            prebig = False\n",
    "\n",
    "        if prebig and point < bestpoint:\n",
    "            break\n",
    "        bestpointlist[current_fold] = max(bestpointlist[current_fold],bestpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a5aba4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5818baa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00082_000',\n",
       " '00082_001',\n",
       " '00082_002',\n",
       " '00082_003',\n",
       " '00082_004',\n",
       " '00082_005',\n",
       " '00082_006',\n",
       " '00082_007',\n",
       " '00082_008',\n",
       " '00082_009',\n",
       " '00082_010',\n",
       " '00082_011',\n",
       " '00082_012',\n",
       " '00224_000',\n",
       " '00224_001',\n",
       " '00224_002',\n",
       " '00224_003',\n",
       " '00224_004',\n",
       " '00224_005',\n",
       " '00224_006',\n",
       " '00224_007',\n",
       " '00224_008',\n",
       " '00224_009',\n",
       " '00224_010',\n",
       " '00224_011',\n",
       " '00224_012',\n",
       " '00352_000',\n",
       " '00352_001',\n",
       " '00352_002',\n",
       " '00352_003',\n",
       " '00352_004',\n",
       " '00352_005',\n",
       " '00352_006',\n",
       " '00352_007',\n",
       " '00352_008',\n",
       " '00352_009',\n",
       " '00352_010',\n",
       " '00352_011',\n",
       " '00352_012',\n",
       " '00489_000',\n",
       " '00489_001',\n",
       " '00489_002',\n",
       " '00489_003',\n",
       " '00489_004',\n",
       " '00489_005',\n",
       " '00489_006',\n",
       " '00489_007',\n",
       " '00489_008',\n",
       " '00489_009',\n",
       " '00489_010',\n",
       " '00489_011',\n",
       " '00489_012',\n",
       " '00699_000',\n",
       " '00699_001',\n",
       " '00699_002',\n",
       " '00699_003',\n",
       " '00699_004',\n",
       " '00699_005',\n",
       " '00699_006',\n",
       " '00699_007',\n",
       " '00699_008',\n",
       " '00699_009',\n",
       " '00699_010',\n",
       " '00699_011',\n",
       " '00699_012',\n",
       " '00777_000',\n",
       " '00777_001',\n",
       " '00777_002',\n",
       " '00777_003',\n",
       " '00777_004',\n",
       " '00777_005',\n",
       " '00777_006',\n",
       " '00777_007',\n",
       " '00777_008',\n",
       " '00777_009',\n",
       " '00777_010',\n",
       " '00777_011',\n",
       " '00777_012',\n",
       " '00817_000',\n",
       " '00817_001',\n",
       " '00817_002',\n",
       " '00817_003',\n",
       " '00817_004',\n",
       " '00817_005',\n",
       " '00817_006',\n",
       " '00817_007',\n",
       " '00817_008',\n",
       " '00817_009',\n",
       " '00817_010',\n",
       " '00817_011',\n",
       " '00817_012',\n",
       " '00970_000',\n",
       " '00970_001',\n",
       " '00970_002',\n",
       " '00970_003',\n",
       " '00970_004',\n",
       " '00970_005',\n",
       " '00970_006',\n",
       " '00970_007',\n",
       " '00970_008',\n",
       " '00970_009',\n",
       " '00970_010',\n",
       " '00970_011',\n",
       " '00970_012',\n",
       " '01110_000',\n",
       " '01110_001',\n",
       " '01110_002',\n",
       " '01110_003',\n",
       " '01110_004',\n",
       " '01110_005',\n",
       " '01110_006',\n",
       " '01110_007',\n",
       " '01110_008',\n",
       " '01110_009',\n",
       " '01110_010',\n",
       " '01110_011',\n",
       " '01110_012',\n",
       " '01151_000',\n",
       " '01151_001',\n",
       " '01151_002',\n",
       " '01151_003',\n",
       " '01151_004',\n",
       " '01151_005',\n",
       " '01151_006',\n",
       " '01151_007',\n",
       " '01151_008',\n",
       " '01151_009',\n",
       " '01151_010',\n",
       " '01151_011',\n",
       " '01151_012',\n",
       " '01209_000',\n",
       " '01209_001',\n",
       " '01209_002',\n",
       " '01209_003',\n",
       " '01209_004',\n",
       " '01209_005',\n",
       " '01209_006',\n",
       " '01209_007',\n",
       " '01209_008',\n",
       " '01209_009',\n",
       " '01209_010',\n",
       " '01209_011',\n",
       " '01209_012',\n",
       " '01270_000',\n",
       " '01270_001',\n",
       " '01270_002',\n",
       " '01270_003',\n",
       " '01270_004',\n",
       " '01270_005',\n",
       " '01270_006',\n",
       " '01270_007',\n",
       " '01270_008',\n",
       " '01270_009',\n",
       " '01270_010',\n",
       " '01270_011',\n",
       " '01270_012',\n",
       " '01281_000',\n",
       " '01281_001',\n",
       " '01281_002',\n",
       " '01281_003',\n",
       " '01281_004',\n",
       " '01281_005',\n",
       " '01281_006',\n",
       " '01281_007',\n",
       " '01281_008',\n",
       " '01281_009',\n",
       " '01281_010',\n",
       " '01281_011',\n",
       " '01281_012',\n",
       " '01581_000',\n",
       " '01581_001',\n",
       " '01581_002',\n",
       " '01581_003',\n",
       " '01581_004',\n",
       " '01581_005',\n",
       " '01581_006',\n",
       " '01581_007',\n",
       " '01581_008',\n",
       " '01581_009',\n",
       " '01581_010',\n",
       " '01581_011',\n",
       " '01581_012',\n",
       " '01699_000',\n",
       " '01699_001',\n",
       " '01699_002',\n",
       " '01699_003',\n",
       " '01699_004',\n",
       " '01699_005',\n",
       " '01699_006',\n",
       " '01699_007',\n",
       " '01699_008',\n",
       " '01699_009',\n",
       " '01699_010',\n",
       " '01699_011',\n",
       " '01699_012',\n",
       " '01835_000',\n",
       " '01835_001',\n",
       " '01835_002',\n",
       " '01835_003',\n",
       " '01835_004',\n",
       " '01835_005',\n",
       " '01835_006',\n",
       " '01835_007',\n",
       " '01835_008',\n",
       " '01835_009',\n",
       " '01835_010',\n",
       " '01835_011',\n",
       " '01835_012',\n",
       " '01948_000',\n",
       " '01948_001',\n",
       " '01948_002',\n",
       " '01948_003',\n",
       " '01948_004',\n",
       " '01948_005',\n",
       " '01948_006',\n",
       " '01948_007',\n",
       " '01948_008',\n",
       " '01948_009',\n",
       " '01948_010',\n",
       " '01948_011',\n",
       " '01948_012',\n",
       " '02198_000',\n",
       " '02198_001',\n",
       " '02198_002',\n",
       " '02198_003',\n",
       " '02198_004',\n",
       " '02198_005',\n",
       " '02198_006',\n",
       " '02198_007',\n",
       " '02198_008',\n",
       " '02198_009',\n",
       " '02198_010',\n",
       " '02198_011',\n",
       " '02198_012',\n",
       " '02290_000',\n",
       " '02290_001',\n",
       " '02290_002',\n",
       " '02290_003',\n",
       " '02290_004',\n",
       " '02290_005',\n",
       " '02290_006',\n",
       " '02290_007',\n",
       " '02290_008',\n",
       " '02290_009',\n",
       " '02290_010',\n",
       " '02290_011',\n",
       " '02290_012',\n",
       " '02430_000',\n",
       " '02430_001',\n",
       " '02430_002',\n",
       " '02430_003',\n",
       " '02430_004',\n",
       " '02430_005',\n",
       " '02430_006',\n",
       " '02430_007',\n",
       " '02430_008',\n",
       " '02430_009',\n",
       " '02430_010',\n",
       " '02430_011',\n",
       " '02430_012',\n",
       " '10004_100',\n",
       " '10004_101',\n",
       " '10004_102',\n",
       " '10004_103',\n",
       " '10004_104',\n",
       " '10004_105',\n",
       " '10004_106',\n",
       " '10004_107',\n",
       " '10004_108',\n",
       " '10004_109',\n",
       " '10004_110',\n",
       " '10004_111',\n",
       " '10004_112',\n",
       " '10019_100',\n",
       " '10019_101',\n",
       " '10019_102',\n",
       " '10019_103',\n",
       " '10019_104',\n",
       " '10019_105',\n",
       " '10019_106',\n",
       " '10019_107',\n",
       " '10019_108',\n",
       " '10019_109',\n",
       " '10019_110',\n",
       " '10019_111',\n",
       " '10019_112',\n",
       " '10067_100',\n",
       " '10067_101',\n",
       " '10067_102',\n",
       " '10067_103',\n",
       " '10067_104',\n",
       " '10067_105',\n",
       " '10067_106',\n",
       " '10067_107',\n",
       " '10067_108',\n",
       " '10067_109',\n",
       " '10067_110',\n",
       " '10067_111',\n",
       " '10067_112',\n",
       " '10131_100',\n",
       " '10131_101',\n",
       " '10131_102',\n",
       " '10131_103',\n",
       " '10131_104',\n",
       " '10131_105',\n",
       " '10131_106',\n",
       " '10131_107',\n",
       " '10131_108',\n",
       " '10131_109',\n",
       " '10131_110',\n",
       " '10131_111',\n",
       " '10131_112',\n",
       " '10181_100',\n",
       " '10181_101',\n",
       " '10181_102',\n",
       " '10181_103',\n",
       " '10181_104',\n",
       " '10181_105',\n",
       " '10181_106',\n",
       " '10181_107',\n",
       " '10181_108',\n",
       " '10181_109',\n",
       " '10181_110',\n",
       " '10181_111',\n",
       " '10181_112',\n",
       " '10228_100',\n",
       " '10228_101',\n",
       " '10228_102',\n",
       " '10228_103',\n",
       " '10228_104',\n",
       " '10228_105',\n",
       " '10228_106',\n",
       " '10228_107',\n",
       " '10228_108',\n",
       " '10228_109',\n",
       " '10228_110',\n",
       " '10228_111',\n",
       " '10228_112',\n",
       " '10316_100',\n",
       " '10316_101',\n",
       " '10316_102',\n",
       " '10316_103',\n",
       " '10316_104',\n",
       " '10316_105',\n",
       " '10316_106',\n",
       " '10316_107',\n",
       " '10316_108',\n",
       " '10316_109',\n",
       " '10316_110',\n",
       " '10316_111',\n",
       " '10316_112',\n",
       " '10354_100',\n",
       " '10354_101',\n",
       " '10354_102',\n",
       " '10354_103',\n",
       " '10354_104',\n",
       " '10354_105',\n",
       " '10354_106',\n",
       " '10354_107',\n",
       " '10354_108',\n",
       " '10354_109',\n",
       " '10354_110',\n",
       " '10354_111',\n",
       " '10354_112',\n",
       " '10384_100',\n",
       " '10384_101',\n",
       " '10384_102',\n",
       " '10384_103',\n",
       " '10384_104',\n",
       " '10384_105',\n",
       " '10384_106',\n",
       " '10384_107',\n",
       " '10384_108',\n",
       " '10384_109',\n",
       " '10384_110',\n",
       " '10384_111',\n",
       " '10384_112',\n",
       " '10454_100',\n",
       " '10454_101',\n",
       " '10454_102',\n",
       " '10454_103',\n",
       " '10454_104',\n",
       " '10454_105',\n",
       " '10454_106',\n",
       " '10454_107',\n",
       " '10454_108',\n",
       " '10454_109',\n",
       " '10454_110',\n",
       " '10454_111',\n",
       " '10454_112',\n",
       " '10460_100',\n",
       " '10460_101',\n",
       " '10460_102',\n",
       " '10460_103',\n",
       " '10460_104',\n",
       " '10460_105',\n",
       " '10460_106',\n",
       " '10460_107',\n",
       " '10460_108',\n",
       " '10460_109',\n",
       " '10460_110',\n",
       " '10460_111',\n",
       " '10460_112',\n",
       " '10534_100',\n",
       " '10534_101',\n",
       " '10534_102',\n",
       " '10534_103',\n",
       " '10534_104',\n",
       " '10534_105',\n",
       " '10534_106',\n",
       " '10534_107',\n",
       " '10534_108',\n",
       " '10534_109',\n",
       " '10534_110',\n",
       " '10534_111',\n",
       " '10534_112',\n",
       " '10555_100',\n",
       " '10555_101',\n",
       " '10555_102',\n",
       " '10555_103',\n",
       " '10555_104',\n",
       " '10555_105',\n",
       " '10555_106',\n",
       " '10555_107',\n",
       " '10555_108',\n",
       " '10555_109',\n",
       " '10555_110',\n",
       " '10555_111',\n",
       " '10555_112',\n",
       " '10620_100',\n",
       " '10620_101',\n",
       " '10620_102',\n",
       " '10620_103',\n",
       " '10620_104',\n",
       " '10620_105',\n",
       " '10620_106',\n",
       " '10620_107',\n",
       " '10620_108',\n",
       " '10620_109',\n",
       " '10620_110',\n",
       " '10620_111',\n",
       " '10620_112',\n",
       " '10659_100',\n",
       " '10659_101',\n",
       " '10659_102',\n",
       " '10659_103',\n",
       " '10659_104',\n",
       " '10659_105',\n",
       " '10659_106',\n",
       " '10659_107',\n",
       " '10659_108',\n",
       " '10659_109',\n",
       " '10659_110',\n",
       " '10659_111',\n",
       " '10659_112',\n",
       " '10819_100',\n",
       " '10819_101',\n",
       " '10819_102',\n",
       " '10819_103',\n",
       " '10819_104',\n",
       " '10819_105',\n",
       " '10819_106',\n",
       " '10819_107',\n",
       " '10819_108',\n",
       " '10819_109',\n",
       " '10819_110',\n",
       " '10819_111',\n",
       " '10819_112',\n",
       " '10850_100',\n",
       " '10850_101',\n",
       " '10850_102',\n",
       " '10850_103',\n",
       " '10850_104',\n",
       " '10850_105',\n",
       " '10850_106',\n",
       " '10850_107',\n",
       " '10850_108',\n",
       " '10850_109',\n",
       " '10850_110',\n",
       " '10850_111',\n",
       " '10850_112',\n",
       " '10869_100',\n",
       " '10869_101',\n",
       " '10869_102',\n",
       " '10869_103',\n",
       " '10869_104',\n",
       " '10869_105',\n",
       " '10869_106',\n",
       " '10869_107',\n",
       " '10869_108',\n",
       " '10869_109',\n",
       " '10869_110',\n",
       " '10869_111',\n",
       " '10869_112',\n",
       " '10944_100',\n",
       " '10944_101',\n",
       " '10944_102',\n",
       " '10944_103',\n",
       " '10944_104',\n",
       " '10944_105',\n",
       " '10944_106',\n",
       " '10944_107',\n",
       " '10944_108',\n",
       " '10944_109',\n",
       " '10944_110',\n",
       " '10944_111',\n",
       " '10944_112',\n",
       " '10972_100',\n",
       " '10972_101',\n",
       " '10972_102',\n",
       " '10972_103',\n",
       " '10972_104',\n",
       " '10972_105',\n",
       " '10972_106',\n",
       " '10972_107',\n",
       " '10972_108',\n",
       " '10972_109',\n",
       " '10972_110',\n",
       " '10972_111',\n",
       " '10972_112',\n",
       " '20169_200',\n",
       " '20169_201',\n",
       " '20169_202',\n",
       " '20169_203',\n",
       " '20169_204',\n",
       " '20169_205',\n",
       " '20169_206',\n",
       " '20169_207',\n",
       " '20169_208',\n",
       " '20169_209',\n",
       " '20169_210',\n",
       " '20169_211',\n",
       " '20169_212',\n",
       " '20169_213',\n",
       " '20169_214',\n",
       " '20169_215',\n",
       " '20169_216',\n",
       " '20260_200',\n",
       " '20260_201',\n",
       " '20260_202',\n",
       " '20260_203',\n",
       " '20260_204',\n",
       " '20260_205',\n",
       " '20260_206',\n",
       " '20260_207',\n",
       " '20260_208',\n",
       " '20260_209',\n",
       " '20260_210',\n",
       " '20260_211',\n",
       " '20260_212',\n",
       " '20260_213',\n",
       " '20260_214',\n",
       " '20260_215',\n",
       " '20260_216',\n",
       " '20299_200',\n",
       " '20299_201',\n",
       " '20299_202',\n",
       " '20299_203',\n",
       " '20299_204',\n",
       " '20299_205',\n",
       " '20299_206',\n",
       " '20299_207',\n",
       " '20299_208',\n",
       " '20299_209',\n",
       " '20299_210',\n",
       " '20299_211',\n",
       " '20299_212',\n",
       " '20299_213',\n",
       " '20299_214',\n",
       " '20299_215',\n",
       " '20299_216',\n",
       " '20383_200',\n",
       " '20383_201',\n",
       " '20383_202',\n",
       " '20383_203',\n",
       " '20383_204',\n",
       " '20383_205',\n",
       " '20383_206',\n",
       " '20383_207',\n",
       " '20383_208',\n",
       " '20383_209',\n",
       " '20383_210',\n",
       " '20383_211',\n",
       " '20383_212',\n",
       " '20383_213',\n",
       " '20383_214',\n",
       " '20383_215',\n",
       " '20383_216',\n",
       " '20540_200',\n",
       " '20540_201',\n",
       " '20540_202',\n",
       " '20540_203',\n",
       " '20540_204',\n",
       " '20540_205',\n",
       " '20540_206',\n",
       " '20540_207',\n",
       " '20540_208',\n",
       " '20540_209',\n",
       " '20540_210',\n",
       " '20540_211',\n",
       " '20540_212',\n",
       " '20540_213',\n",
       " '20540_214',\n",
       " '20540_215',\n",
       " '20540_216',\n",
       " '20643_200',\n",
       " '20643_201',\n",
       " '20643_202',\n",
       " '20643_203',\n",
       " '20643_204',\n",
       " '20643_205',\n",
       " '20643_206',\n",
       " '20643_207',\n",
       " '20643_208',\n",
       " '20643_209',\n",
       " '20643_210',\n",
       " '20643_211',\n",
       " '20643_212',\n",
       " '20643_213',\n",
       " '20643_214',\n",
       " '20643_215',\n",
       " '20643_216',\n",
       " '20772_200',\n",
       " '20772_201',\n",
       " '20772_202',\n",
       " '20772_203',\n",
       " '20772_204',\n",
       " '20772_205',\n",
       " '20772_206',\n",
       " '20772_207',\n",
       " '20772_208',\n",
       " '20772_209',\n",
       " '20772_210',\n",
       " '20772_211',\n",
       " '20772_212',\n",
       " '20772_213',\n",
       " '20772_214',\n",
       " '20772_215',\n",
       " '20772_216',\n",
       " '21030_200',\n",
       " '21030_201',\n",
       " '21030_202',\n",
       " '21030_203',\n",
       " '21030_204',\n",
       " '21030_205',\n",
       " '21030_206',\n",
       " '21030_207',\n",
       " '21030_208',\n",
       " '21030_209',\n",
       " '21030_210',\n",
       " '21030_211',\n",
       " '21030_212',\n",
       " '21030_213',\n",
       " '21030_214',\n",
       " '21030_215',\n",
       " '21030_216',\n",
       " '21114_200',\n",
       " '21114_201',\n",
       " '21114_202',\n",
       " '21114_203',\n",
       " '21114_204',\n",
       " '21114_205',\n",
       " '21114_206',\n",
       " '21114_207',\n",
       " '21114_208',\n",
       " '21114_209',\n",
       " '21114_210',\n",
       " '21114_211',\n",
       " '21114_212',\n",
       " '21114_213',\n",
       " '21114_214',\n",
       " '21114_215',\n",
       " '21114_216',\n",
       " '21268_200',\n",
       " '21268_201',\n",
       " '21268_202',\n",
       " '21268_203',\n",
       " '21268_204',\n",
       " '21268_205',\n",
       " '21268_206',\n",
       " '21268_207',\n",
       " '21268_208',\n",
       " '21268_209',\n",
       " '21268_210',\n",
       " '21268_211',\n",
       " '21268_212',\n",
       " '21268_213',\n",
       " '21268_214',\n",
       " '21268_215',\n",
       " '21268_216',\n",
       " '21391_200',\n",
       " '21391_201',\n",
       " '21391_202',\n",
       " '21391_203',\n",
       " '21391_204',\n",
       " '21391_205',\n",
       " '21391_206',\n",
       " '21391_207',\n",
       " '21391_208',\n",
       " '21391_209',\n",
       " '21391_210',\n",
       " '21391_211',\n",
       " '21391_212',\n",
       " '21391_213',\n",
       " '21391_214',\n",
       " '21391_215',\n",
       " '21391_216',\n",
       " '21540_200',\n",
       " '21540_201',\n",
       " '21540_202',\n",
       " '21540_203',\n",
       " '21540_204',\n",
       " '21540_205',\n",
       " '21540_206',\n",
       " '21540_207',\n",
       " '21540_208',\n",
       " '21540_209',\n",
       " '21540_210',\n",
       " '21540_211',\n",
       " '21540_212',\n",
       " '21540_213',\n",
       " '21540_214',\n",
       " '21540_215',\n",
       " '21540_216',\n",
       " '21614_200',\n",
       " '21614_201',\n",
       " '21614_202',\n",
       " '21614_203',\n",
       " '21614_204',\n",
       " '21614_205',\n",
       " '21614_206',\n",
       " '21614_207',\n",
       " '21614_208',\n",
       " '21614_209',\n",
       " '21614_210',\n",
       " '21614_211',\n",
       " '21614_212',\n",
       " '21614_213',\n",
       " '21614_214',\n",
       " '21614_215',\n",
       " '21614_216',\n",
       " '21627_200',\n",
       " '21627_201',\n",
       " '21627_202',\n",
       " '21627_203',\n",
       " '21627_204',\n",
       " '21627_205',\n",
       " '21627_206',\n",
       " '21627_207',\n",
       " '21627_208',\n",
       " '21627_209',\n",
       " '21627_210',\n",
       " '21627_211',\n",
       " '21627_212',\n",
       " '21627_213',\n",
       " '21627_214',\n",
       " '21627_215',\n",
       " '21627_216',\n",
       " '21681_200',\n",
       " '21681_201',\n",
       " '21681_202',\n",
       " '21681_203',\n",
       " '21681_204',\n",
       " '21681_205',\n",
       " '21681_206',\n",
       " '21681_207',\n",
       " '21681_208',\n",
       " '21681_209',\n",
       " '21681_210',\n",
       " '21681_211',\n",
       " '21681_212',\n",
       " '21681_213',\n",
       " '21681_214',\n",
       " '21681_215',\n",
       " '21681_216',\n",
       " '21706_200',\n",
       " '21706_201',\n",
       " '21706_202',\n",
       " '21706_203',\n",
       " '21706_204',\n",
       " '21706_205',\n",
       " '21706_206',\n",
       " '21706_207',\n",
       " '21706_208',\n",
       " '21706_209',\n",
       " '21706_210',\n",
       " '21706_211',\n",
       " '21706_212',\n",
       " '21706_213',\n",
       " '21706_214',\n",
       " '21706_215',\n",
       " '21706_216',\n",
       " '21808_200',\n",
       " '21808_201',\n",
       " '21808_202',\n",
       " '21808_203',\n",
       " '21808_204',\n",
       " '21808_205',\n",
       " '21808_206',\n",
       " '21808_207',\n",
       " '21808_208',\n",
       " '21808_209',\n",
       " '21808_210',\n",
       " '21808_211',\n",
       " '21808_212',\n",
       " '21808_213',\n",
       " '21808_214',\n",
       " '21808_215',\n",
       " '21808_216',\n",
       " '21905_200',\n",
       " '21905_201',\n",
       " '21905_202',\n",
       " '21905_203',\n",
       " '21905_204',\n",
       " '21905_205',\n",
       " '21905_206',\n",
       " '21905_207',\n",
       " '21905_208',\n",
       " '21905_209',\n",
       " '21905_210',\n",
       " '21905_211',\n",
       " '21905_212',\n",
       " '21905_213',\n",
       " '21905_214',\n",
       " '21905_215',\n",
       " '21905_216',\n",
       " '21997_200',\n",
       " '21997_201',\n",
       " '21997_202',\n",
       " '21997_203',\n",
       " '21997_204',\n",
       " '21997_205',\n",
       " '21997_206',\n",
       " '21997_207',\n",
       " '21997_208',\n",
       " '21997_209',\n",
       " '21997_210',\n",
       " '21997_211',\n",
       " '21997_212',\n",
       " '21997_213',\n",
       " '21997_214',\n",
       " '21997_215',\n",
       " '21997_216',\n",
       " '22120_200',\n",
       " '22120_201',\n",
       " '22120_202',\n",
       " '22120_203',\n",
       " '22120_204',\n",
       " '22120_205',\n",
       " '22120_206',\n",
       " '22120_207',\n",
       " '22120_208',\n",
       " '22120_209',\n",
       " '22120_210',\n",
       " '22120_211',\n",
       " '22120_212',\n",
       " '22120_213',\n",
       " '22120_214',\n",
       " '22120_215',\n",
       " '22120_216',\n",
       " '22123_200',\n",
       " '22123_201',\n",
       " '22123_202',\n",
       " '22123_203',\n",
       " '22123_204',\n",
       " '22123_205',\n",
       " '22123_206',\n",
       " '22123_207',\n",
       " '22123_208',\n",
       " '22123_209',\n",
       " '22123_210',\n",
       " '22123_211',\n",
       " '22123_212',\n",
       " '22123_213',\n",
       " '22123_214',\n",
       " '22123_215',\n",
       " '22123_216',\n",
       " '30168_300',\n",
       " '30168_301',\n",
       " '30168_302',\n",
       " '30168_303',\n",
       " '30168_304',\n",
       " '30168_305',\n",
       " '30168_306',\n",
       " '30168_307',\n",
       " '30168_308',\n",
       " '30168_309',\n",
       " '30168_310',\n",
       " '30168_311',\n",
       " '30168_312',\n",
       " '30168_313',\n",
       " '30168_314',\n",
       " '30168_315',\n",
       " '30260_300',\n",
       " '30260_301',\n",
       " '30260_302',\n",
       " '30260_303',\n",
       " '30260_304',\n",
       " '30260_305',\n",
       " '30260_306',\n",
       " '30260_307',\n",
       " '30260_308',\n",
       " '30260_309',\n",
       " '30260_310',\n",
       " '30260_311',\n",
       " '30260_312',\n",
       " '30260_313',\n",
       " '30260_314',\n",
       " '30260_315',\n",
       " '30549_300',\n",
       " '30549_301',\n",
       " '30549_302',\n",
       " '30549_303',\n",
       " '30549_304',\n",
       " '30549_305',\n",
       " '30549_306',\n",
       " '30549_307',\n",
       " '30549_308',\n",
       " '30549_309',\n",
       " '30549_310',\n",
       " '30549_311',\n",
       " '30549_312',\n",
       " '30549_313',\n",
       " '30549_314',\n",
       " '30549_315',\n",
       " '30961_300',\n",
       " '30961_301',\n",
       " '30961_302',\n",
       " '30961_303',\n",
       " '30961_304',\n",
       " '30961_305',\n",
       " '30961_306',\n",
       " '30961_307',\n",
       " '30961_308',\n",
       " '30961_309',\n",
       " '30961_310',\n",
       " '30961_311',\n",
       " '30961_312',\n",
       " '30961_313',\n",
       " '30961_314',\n",
       " '30961_315',\n",
       " '31408_300',\n",
       " '31408_301',\n",
       " '31408_302',\n",
       " '31408_303',\n",
       " '31408_304',\n",
       " '31408_305',\n",
       " '31408_306',\n",
       " '31408_307',\n",
       " '31408_308',\n",
       " '31408_309',\n",
       " '31408_310',\n",
       " '31408_311',\n",
       " '31408_312',\n",
       " '31408_313',\n",
       " '31408_314',\n",
       " '31408_315',\n",
       " '31652_300',\n",
       " '31652_301',\n",
       " '31652_302',\n",
       " '31652_303',\n",
       " '31652_304',\n",
       " '31652_305',\n",
       " '31652_306',\n",
       " '31652_307',\n",
       " '31652_308',\n",
       " '31652_309',\n",
       " '31652_310',\n",
       " '31652_311',\n",
       " '31652_312',\n",
       " '31652_313',\n",
       " '31652_314',\n",
       " '31652_315',\n",
       " '32614_300',\n",
       " '32614_301',\n",
       " '32614_302',\n",
       " '32614_303',\n",
       " '32614_304',\n",
       " '32614_305',\n",
       " '32614_306',\n",
       " '32614_307',\n",
       " '32614_308',\n",
       " '32614_309',\n",
       " '32614_310',\n",
       " '32614_311',\n",
       " '32614_312',\n",
       " '32614_313',\n",
       " '32614_314',\n",
       " '32614_315',\n",
       " '32996_300',\n",
       " '32996_301',\n",
       " '32996_302',\n",
       " '32996_303',\n",
       " '32996_304',\n",
       " '32996_305',\n",
       " '32996_306',\n",
       " '32996_307',\n",
       " '32996_308',\n",
       " '32996_309',\n",
       " '32996_310',\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6fc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec22e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
